---
title: Gateway connectivity
description: Install an Istio mesh across multiple Kubernetes clusters using Istio Gateway to reach remote pods.
weight: 2
keywords: [kubernetes,multicluster,federation,gateway]
---

Instructions for installing an Istio mesh across multiple clusters when pods
in each cluster can only connect to remote gateway IPs.

Instead of using a central Istio control plane to manage the mesh,
in this configuration each cluster has an **identical** Istio control plane
installation, each managing its own endpoints.
All of the clusters are under a shared administrative control for the purposes of
policy enforcement and security.

A single Istio service mesh across the clusters is achieved by replicating
shared services and namespaces and using a common root CA in all of the clusters.
Cross-cluster communication occurs over Istio Gateways of the respective clusters.

{{< image width="80%" link="./multicluster-with-gateways.svg" caption="Istio mesh spanning multiple Kubernetes clusters using Istio Gateway to reach remote pods" >}}

## Prerequisites

* Two or more Kubernetes clusters with **1.10 or newer**.

* Authority to deploy the [Istio control plane using Helm](/docs/setup/kubernetes/helm-install/)
on **each** Kubernetes cluster.

* The IP address of the `istio-ingressgateway` service in each cluster must
  be accessible from every other cluster.

* A **Root CA**. Cross cluster communication requires mTLS connection
  between services. To enable mTLS communication across clusters, each
  cluster's Citadel will be configured with intermediate CA credentials
  generated by a shared root CA. For illustration purposes, we will use a
  sample root CA certificate available as part of Istio install
  under the `samples/certs` directory.

## Deploy Istio control plane in each cluster

1. Generate intermediate CA certs for each cluster's Citadel from your
organization's root CA. The shared root CA enables mTLS communication
across different clusters. For illustration purposes, we will use
the sample root certificates as the intermediate certificate.

1. In every cluster, create a Kubernetes secret for your generated CA certs
   using a command similar to the following:

    {{< text bash >}}
    $ kubectl create namespace istio-system
    $ kubectl create secret generic cacerts -n istio-system \
        --from-file=samples/certs/ca-cert.pem \
        --from-file=samples/certs/ca-key.pem \
        --from-file=samples/certs/root-cert.pem \
        --from-file=samples/certs/cert-chain.pem
    {{< /text >}}

1. Install the Istio control plane in every cluster using the following commands:

    {{< text bash >}}
    $ helm template install/kubernetes/helm/istio --name istio --namespace istio-system \
        -f install/kubernetes/helm/istio/values-istio-multicluster-gateways.yaml > $HOME/istio.yaml
    $ kubectl apply -f $HOME/istio.yaml
    {{< /text >}}

For further details and customization options, refer to the [Installation
with Helm](/docs/setup/kubernetes/helm-install/) instructions.

## Configure DNS

Providing a DNS resolution for services in remote clusters will allow
existing applications to function unmodified, as applications typically
expect to resolve services by their DNS names and access the resulting
IP. Istio itself does not use the DNS for routing requests between
services. Services local to a cluster share a common DNS suffix
(e.g., `svc.cluster.local`). Kubernetes DNS provides DNS resolution for these
services.

To provide a similar setup for services from remote clusters, we will name
services from remote clusters in the format
`<name>.<namespace>.global`. Istio also ships with a CoreDNS server that
will provide DNS resolution for these services. In order to utilize this
DNS, Kubernetes' DNS needs to be configured to point to CoreDNS as the DNS
server for the `.global` DNS domain. Create the following ConfigMap (or
update an existing one):

{{< text bash >}}
$ kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-dns
  namespace: kube-system
data:
  stubDomains: |
    {"global": ["$(kubectl get svc -n istio-system istiocoredns -o jsonpath={.spec.clusterIP})"]}
EOF
{{< /text >}}

## Adding services from other clusters

Each service in the remote cluster that needs to be accessed from a given
cluster requires a `ServiceEntry` configuration. The host used in the
service entry should be of the form `<name>.<namespace>.global` where name
and namespace correspond to the remote service's name and namespace
respectively. In order to provide DNS resolution for services under the
`*.global` domain, you need to assign these services an IP address. We
suggest assigning an IP address from the 127.255.0.0/16 subnet. These IPs
are non-routable outside of a pod. Application traffic for these IPs will
be captured by the sidecar and routed to the appropriate remote service

> Each service (in the .global DNS domain) must have a unique IP within the cluster.

For example, the diagram above depicts two services `foo.ns1` in `cluster1`
and `bar.ns2` in `cluster2`. In order to access `bar.ns2` from `cluster1`,
add the following service entry to `cluster1`:

{{< text yaml >}}
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: bar-ns2
spec:
  hosts:
  # must be of form name.namespace.global
  - bar.ns2.global
  # Treat remote cluster services as part of the service mesh
  # as all clusters in the service mesh share the same root of trust.
  location: MESH_INTERNAL
  ports:
  - name: http1
    number: 8080
    protocol: http
  - name: tcp2
    number: 9999
    protocol: tcp
  resolution: DNS
  addresses:
  # the IP address to which bar.ns2.global will resolve to
  # must be unique for each remote service, within a given cluster.
  # This address need not be routable. Traffic for this IP will be captured
  # by the sidecar and routed appropriately.
  - 127.255.0.2
  endpoints:
  # This is the routable address of the ingress gateway in cluster2 that
  # sits in front of bar.ns2 service. Traffic from the sidecar will be routed
  # to this address.
  - address: <IPofCluster2IngressGateway>
    ports:
      http1: 15443 # Do not change this port value
      tcp2: 15443 # Do not change this port value
{{< /text >}}

If you wish to route all egress traffic from `cluster1` via a dedicated
egress gateway, use the following service entry for `bar.ns2`

{{< text yaml >}}
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: bar-ns2
spec:
  hosts:
  # must be of form name.namespace.global
  - bar.ns2.global
  location: MESH_INTERNAL
  ports:
  - name: http1
    number: 8080
    protocol: http
  - name: tcp2
    number: 9999
    protocol: tcp
  resolution: DNS
  addresses:
  - 127.255.0.2
  endpoints:
  - address: <IPofCluster2IngressGateway>
    network: external
    ports:
      http1: 15443 # Do not change this port value
      tcp2: 15443 # Do not change this port value
  - address: istio-egressgateway.istio-system.svc.cluster.local
    ports:
      http1: 15443
      tcp2: 15443
{{< /text >}}

Verify the setup by trying to access `bar.ns2.global` or `bar.ns2` from any
pod on `cluster1`. Both DNS names should resolve to 127.255.0.2, the
address used in the service entry configuration.

The configurations above will result in all traffic in `cluster1` for
`bar.ns2.global` on *any port* to be routed to the endpoint
`<IPofCluster2IngressGateway>:15443` over an mTLS connection.

The gateway for port 15443 is a special SNI-aware Envoy that has been
preconfigured and installed as part of the Istio installation step
described in the prerequisite section.  Traffic entering port 15443 will be
load balanced among pods of the appropriate internal service of the target
cluster (in this case, `bar.ns2`).

> Do not create a Gateway configuration for port 15443.

## Version-aware routing to remote services

If the remote service being added has multiple versions, add one or more
labels to the service entry endpoint, and follow the steps outlined in the
[request routing](/docs/tasks/traffic-management/request-routing/) section
to create appropriate virtual services and destination rules. For example,

{{< text yaml >}}
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: bar-ns2
spec:
  hosts:
  # must be of form name.namespace.global
  - bar.ns2.global
  location: MESH_INTERNAL
  ports:
  - name: http1
    number: 8080
    protocol: http
  - name: tcp2
    number: 9999
    protocol: tcp
  resolution: DNS
  addresses:
  # the IP address to which bar.ns2.global will resolve to
  # must be unique for each service.
  - 127.255.0.2
  endpoints:
  - address: <IPofCluster2IngressGateway>
    labels:
      version: beta
      some: thing
      foo: bar
    ports:
      http1: 15443 # Do not change this port value
      tcp2: 15443 # Do not change this port value
{{< /text >}}

Use destination rules to create subsets for `bar.ns2` service with
appropriate label selectors. The set of steps to follow are identical to
those used for a local service.

## Summary

Using Istio gateways, a common root CA, and service entries, you configured
a single Istio service mesh across multiple Kubernetes clusters.  Although
the above procedure involved a certain amount of manual work, the entire
process could be automated by creating service entries for each service in
the system, with a unique IP allocated from the 127.255.0.0/16 subnet. Once
configured this way, traffic can be transparently routed to remote clusters
without any application involvement.
