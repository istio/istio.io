<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Istio Blog</title><description>Connect, secure, control, and observe services.</description><link>/v1.1</link><image><url>/v1.1/favicons/android-192x192.png</url><link>/v1.1</link></image><category>Service mesh</category><item><title>延长 Istio 自签发根证书的有效期</title><description>&lt;p>Istio 的自签发证书只有一年的有效期。如果你选择使用 Istio 的自签发证书，就需要在它们过期之前订好计划进行根证书的更迭。根证书过期可能会导致集群范围内的意外中断。这一问题的影响范围涵盖了 1.0.7、1.1.7 及之前的所有版本。&lt;/p>
&lt;p>阅读&lt;a href="/v1.1/zh/help/ops/security/root-transition/">《延长自签发证书的有效期》&lt;/a>一文，其中包含了获取证书有效期以及完成证书轮换的方法。&lt;/p>
&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">我们认为每年更换根证书和密钥是一个安全方面的最佳实践，我们会在后续内容中介绍根证书和密钥的轮换方法。&lt;/div>
&lt;/aside>
&lt;/div></description><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/root-transition/</link><author>Oliver Liu</author><guid isPermaLink="true">/v1.1/zh/blog/2019/root-transition/</guid><category>security</category><category>PKI</category><category>certificate</category><category>Citadel</category></item><item><title>发布 Istio 1.0.8</title><description>&lt;p>在此宣布 Istio 1.0.8 已经可用，请参照下列内容获取更新信息。&lt;/p>
&lt;div class="call-to-action">
&lt;a class="btn" href="https://github.com/istio/istio/releases/tag/1.0.8">1.0.8 下载&lt;/a>
&lt;a class="btn" href="https://archive.istio.io/v1.0">1.0.8 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.0.7...1.0.8">1.0.8 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Fri, 07 Jun 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.0.8/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.0.8/</guid></item><item><title>发布 Istio 1.1.8</title><description>&lt;p>Istio 1.1.8 现已发布。请参照下列内容获取更新信息。&lt;/p>
&lt;div class="call-to-action">
&lt;a class="btn" href="https://github.com/istio/istio/releases/tag/1.1.8">1.1.8 下载&lt;/a>
&lt;a class="btn" href="https://istio.io/zh/docs">1.1.8 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.1.7...1.1.8">1.1.8 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Thu, 06 Jun 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.1.8/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.1.8/</guid></item><item><title>安全更新 - CVE-2019-12243</title><description>&lt;p>在对 &lt;a href="/v1.1/zh/about/notes/1.1.7">Istio 1.1.7&lt;/a> 的发行声明进行评审时，我们意识到这一版本中修复的 &lt;a href="https://github.com/istio/istio/issues/13868">Issue 13868&lt;/a>，实际上是一个安全漏洞。&lt;/p>
&lt;p>起初我们认为这个 Bug 影响的是 &lt;a href="/v1.1/zh/about/feature-stages/#%E5%AE%89%E5%85%A8%E5%92%8C%E7%AD%96%E7%95%A5%E5%AE%9E%E6%96%BD">TCP 认证&lt;/a>，这一功能仍然处于 Alpha 阶段，因此无需启动安全顾问流程，然而接下来我们意识到 &lt;a href="/v1.1/docs/reference/config/policy-and-telemetry/adapters/denier/">Deny Checker&lt;/a> 和 &lt;a href="/v1.1/docs/reference/config/policy-and-telemetry/adapters/list/">List Checker&lt;/a> 这两个稳定版功能也受到了影响。我们对流程进行了复查，确认将应该进行&lt;a href="/v1.1/zh/about/security-vulnerabilities/">披露&lt;/a>的安全漏洞当成了普通 Bug。&lt;/p>
&lt;p>我们对 Bug 的代码变更进行了跟踪，发现是从 Istio 1.1 中引入的，其影响一直持续到了 1.1.6 版本。&lt;/p>
&lt;p>这一漏洞现已命名为 &lt;a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-12243">CVE 2019-12243&lt;/a>。&lt;/p>
&lt;h2 id="-istio-">受影响的 Istio 版本&lt;/h2>
&lt;p>下列版本都受到这一问题的影响：&lt;/p>
&lt;ul>
&lt;li>1.1、1.1.1、1.1.2、1.1.3、1.1.4、1.1.5、1.1.6。&lt;/li>
&lt;/ul>
&lt;h2 id="heading">影响级别&lt;/h2>
&lt;p>总体 CVSS 分数：8.9 &lt;a href="https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator?vector=AV:A/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:N/E:H/RL:O/RC:C">AV:A/AC:L/PR:N/UI:N/S:C/C:H/I:H/A:N/E:H/RL:O/RC:C&lt;/a>&lt;/p>
&lt;h2 id="heading-1">漏洞影响和检测&lt;/h2>
&lt;p>从 Istio 1.1 开始，缺省的 Istio 安装配置中，策略功能会被禁用。可以用下列命令来检查策略功能的状态：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl -n istio-system get cm istio -o jsonpath=&amp;#34;{@.data.mesh}&amp;#34; | grep disablePolicyChecks
disablePolicyChecks: true
&lt;/code>&lt;/pre>
&lt;p>如果 &lt;code>disablePolicyChecks&lt;/code> 设置为 &lt;code>true&lt;/code>，就不会受到该漏洞的影响。&lt;/p>
&lt;p>如果下列几个条件全都符合，就会因为该漏洞造成安全威胁：&lt;/p>
&lt;ul>
&lt;li>运行的是受影响的 Istio 版本。&lt;/li>
&lt;li>&lt;code>disablePolicyChecks&lt;/code> 设置为 false（可以用前面提到的方法进行检查）。&lt;/li>
&lt;li>你的工作负载不是 HTTP、HTTP/2 或者 gRPC 协议。&lt;/li>
&lt;li>使用 Mixer 适配器（例如 Deny Checker、List Checker）为 TCP 服务提供认证。&lt;/li>
&lt;/ul>
&lt;h2 id="heading-2">对策&lt;/h2>
&lt;ul>
&lt;li>Istio 1.0.x 的用户不受影响。&lt;/li>
&lt;li>Istio 1.1.x 用户：至少要更新到 &lt;a href="/v1.1/zh/about/notes/1.1.7">Istio 1.1.7&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="heading-3">致谢&lt;/h2>
&lt;p>Istio 团队在此感谢报告这一问题的 &lt;code>Haim Helman&lt;/code>。&lt;/p></description><pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/cve-2019-12243/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/cve-2019-12243/</guid></item><item><title>Istio 1.0 将在 2019 年 6 月 19 日停止支持</title><description>&lt;p>根据 Istio 的&lt;a href="/v1.1/zh/about/release-cadence/">支持策略&lt;/a>，1.0 这样的 LTS 版本会在下一个 LTS 版本发布的三个月之后停止支持。&lt;a href="/v1.1/zh/about/notes/1.1/">Istio 1.1 在 3 月 19 日已经发布&lt;/a>，因此 1.0 版本的支持将在 2019 年 6 月 19 日终止。&lt;/p>
&lt;p>自此开始，我们会停止向 1.0 版本发布新的安全问题以及严重 Bug 的修复，因此我们建议用户升级到 Istio 的最新版本（1.1.9）。如果不预先进行升级，那么新的安全修复发生时，可能会迫使用户在一个较短的时间窗口里进行一次紧急升级。&lt;/p>
&lt;p>我们关心用户和用户的集群，所以请对自己好一点，尽快升级。&lt;/p></description><pubDate>Thu, 23 May 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.0-eol/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.0-eol/</guid></item><item><title>发布 Istio 1.1.7</title><description>&lt;p>Istio 1.1.7 现已发布。请参照下列内容获取更新信息。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.1.7"
data-updateadvice='Before you download 1.1.7, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.1.8'
data-updatehref="/v1.1/zh/about/notes/1.1.8">
1.1.7 下载
&lt;/button>
&lt;a class="btn" href="https://istio.io/zh/docs">1.1.7 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.1.6...1.1.7">1.1.7 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Fri, 17 May 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.1.7/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.1.7/</guid></item><item><title>发布 Istio 1.1.6</title><description>&lt;p>Istio 1.1.6 现已发布。请参照下列内容获取更新信息。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.1.6"
data-updateadvice='Before you download 1.1.6, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.1.8'
data-updatehref="/v1.1/zh/about/notes/1.1.8">
1.1.6 下载
&lt;/button>
&lt;a class="btn" href="https://istio.io/zh/docs">1.1.6 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.1.5...1.1.6">1.1.6 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.1.6/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.1.6/</guid></item><item><title>发布 Istio 1.1.5</title><description>&lt;p>Istio 1.1.5 现已发布。请参照下列内容获取更新信息。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.1.5"
data-updateadvice='Before you download 1.1.5, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.1.8'
data-updatehref="/v1.1/zh/about/notes/1.1.8">
1.1.5 下载
&lt;/button>
&lt;a class="btn" href="https://istio.io/zh/docs">1.1.5 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.1.4...1.1.5">1.1.5 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.1.5/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.1.5/</guid></item><item><title>发布 Istio 1.1.4</title><description>&lt;p>Istio 1.1.4 现已发布。请参照下列内容获取更新信息。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.1.4"
data-updateadvice='Before you download 1.1.4, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.1.8'
data-updatehref="/v1.1/zh/about/notes/1.1.8">
1.1.4 下载
&lt;/button>
&lt;a class="btn" href="https://istio.io/zh/docs">1.1.4 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.1.3...1.1.4">1.1.4 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.1.4/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.1.4/</guid></item><item><title>安全更新：发布 Istio 1.1.3</title><description>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.1.3"
data-updateadvice='Before you download 1.1.3, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.1.8'
data-updatehref="/v1.1/zh/about/notes/1.1.8">
1.1.3 下载
&lt;/button>
&lt;a class="btn" href="https://istio.io/zh/docs">1.1.3 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.1.2...1.1.3">1.1.3 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.1.3/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.1.3/</guid></item><item><title>安全更新：发布 Istio 1.1.2</title><description>&lt;p>第一时间发布 Istio 1.1.2，其中包含了重要的安全更新。请参看下方详情链接。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.1.2"
data-updateadvice='Before you download 1.1.2, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.1.8'
data-updatehref="/v1.1/zh/about/notes/1.1.8">
1.1.2 下载
&lt;/button>
&lt;a class="btn" href="https://istio.io/zh/docs">1.1.2 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.1.1...1.1.2">1.1.2 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.1.2/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.1.2/</guid></item><item><title>安全更新：发布 Istio 1.0.7</title><description>&lt;p>第一时间发布 1.0.7，其中包含了重要的安全更新。请参看下方详情链接。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.0.7"
data-updateadvice='Before you download 1.0.7, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.0.8'
data-updatehref="/v1.1/zh/about/notes/1.0.8">
1.0.7 下载
&lt;/button>
&lt;a class="btn" href="https://archive.istio.io/v1.0">1.0.7 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.0.6...1.0.7">1.0.7 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Fri, 05 Apr 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.0.7/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.0.7/</guid></item><item><title>发布 Istio 1.1.1</title><description>&lt;p>很高兴地在此宣布，Istio 1.1.1 现已发布。请参照下列内容获取更新信息。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.1.1"
data-updateadvice='Before you download 1.1.1, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.1.8'
data-updatehref="/v1.1/zh/about/notes/1.1.8">
1.1.1 下载
&lt;/button>
&lt;a class="btn" href="https://istio.io/zh/docs">1.1.1 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.1.0...1.1.1">1.1.1 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.1.1/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.1.1/</guid></item><item><title>宣布 Istio 1.1 发布</title><description>&lt;p>我们很高兴地宣布，Istio 1.1 发布！&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.1.0"
data-updateadvice='Before you download 1.1, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.1.8'
data-updatehref="/v1.1/zh/about/notes/1.1.8">
1.1 下载
&lt;/button>
&lt;a class="btn" href="https://istio.io/zh/docs">1.1 文档&lt;/a>
&lt;a class="btn" href="/v1.1/zh/about/notes/1.1/">1.1 发布说明&lt;/a>
&lt;/div>
&lt;p>自从去年 7 月份 1.0 发布以来，为了帮助人们将 Istio 投入生产我们做了很多工作。意料之中，我们发布了很多补丁（到目前为止已经发布了 6 个补丁！），但我们也在努力为产品添加新功能。&lt;/p>
&lt;p>1.1 版本的主题是 “Enterprise Ready”（企业级就绪）。我们很高兴看到越来越多的公司在生产中使用 Istio，但是随着一些大公司加入进来，Istio 也遇到了一些瓶颈。&lt;/p>
&lt;p>我们关注的主要领域包括性能和可扩展性。随着人们将 Istio 逐步投入生产，使用更大的集群以更高的容量运行更多服务，可能会遇到了一些扩展和性能问题。Sidecar 占用了太多资源增加了太多的延迟。控制平面（尤其是 Pilot）过度耗费资源。&lt;/p>
&lt;p>我们投入了很多精力在使数据平面和控制平面更有效率上。在 1.1 的性能测试中，我们观察到 sidecar 处理 1000 rps 通常需要 0.5 个 vCPU。单个 Pilot 实例能够处理 1000 个服务（以及 2000 个 pod），需要消耗 1.5 个 vCPU 和 2GB 内存。Sidecar 在第 50 百分位增加 5 毫秒，在第 99 百分位增加 10 毫秒（执行策略将增加延迟）。&lt;/p>
&lt;p>我们也完成了命名空间隔离的工作。您可以使用 Kubernetes 命名空间来强制控制边界以确保团队之间不会相互干扰。&lt;/p>
&lt;p>我们还改进了多集群功能和可用性。我们听取了社区的意见，改进了流量控制和策略的默认设置。我们引入了一个名为 Galley 的新组件。Galley 验证 YAML 配置，减少了配置错误的可能性。Galley 还用在多集群设置中——从每个 Kubernetes 集群中收集服务发现信息。我们还支持了其他多集群拓扑，包括单控制平面和多个同步控制平面，而无需扁平网络支持。&lt;/p>
&lt;p>更多信息和详情请查看&lt;a href="/v1.1/about/notes/1.1/">发布说明&lt;/a>。&lt;/p>
&lt;p>该项目还有更多进展。众所周知 Istio 有许多可移动部件，它们承担了太多工作。为了解决这个问题，我们最近成立了 &lt;a href="https://github.com/istio/community/blob/master/WORKING-GROUPS.md#working-group-meetings">Usability Working Group（可用性工作组）&lt;/a>（可随时加入）。&lt;a href="https://github.com/istio/community#community-meeting">社区会议&lt;/a>（周四上午 11 点）和工作组里也发生了很多事情。您可以使用 GitHub 凭据登录 &lt;a href="https://discuss.istio.io">discuss.istio.io&lt;/a> 参与讨论！&lt;/p>
&lt;p>感谢在过去几个月里为 Istio 作出贡献的所有人——修补 1.0，为 1.1 增加功能以及最近在 1.1 上进行的大量测试。特别感谢那些与我们合作安装和升级到早期版本，帮助我们在发布之前发现问题的公司和用户。&lt;/p>
&lt;p>最后，去浏览最新文档，安装 1.1 版本吧！Happy meshing！&lt;/p></description><pubDate>Tue, 19 Mar 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.1/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.1/</guid></item><item><title>面向性能而架构的 Istio 1.1</title><description>&lt;p>构建一个超大规模的基于微服务的云环境一直令人非常兴奋，但却难于管理。自从 2014 年出现 Kubernetes (容器编排引擎)，随后在 2017 年出现 Istio (容器服务管理)，这两个开源项目让开发者无需在管理上耗费太多时间即可扩展基于容器的应用程序。&lt;/p>
&lt;p>现在，Istio 1.1 新的增强功能带来了改进的应用性能和服务管理效率。相比于 Istio 1.0，使用我们的示例商业航班预订程序的模拟显示出了如下改进。&lt;/p>
&lt;p>我们看到大量的应用程序性能提升:&lt;/p>
&lt;ul>
&lt;li>应用程序平均延迟降低 30％&lt;/li>
&lt;li>在大型网格中服务启动时间快 40％&lt;/li>
&lt;/ul>
&lt;p>同样还有服务管理效率的显著提升:&lt;/p>
&lt;ul>
&lt;li>在大型网格中，Pilot 的 CPU 使用率降低了 90％&lt;/li>
&lt;li>在大型网格中，Pilot 的内存使用率降低了 50％&lt;/li>
&lt;/ul>
&lt;p>使用 Istio 1.1，企业会对一致性和可控的应用程序扩展能力更加自信 —— 即使在超大规模的云环境中也无所畏惧。&lt;/p>
&lt;p>祝贺那些来自世界各地的为此次版本发布做出贡献的 Istio 专家。我们对这些结果无比高兴。&lt;/p>
&lt;h2 id="istio-11-">Istio 1.1 性能增强&lt;/h2>
&lt;p>作为 Istio Performance and Scalability （性能和可伸缩）工作组的成员，我们进行了广泛的性能评估。我们与其他 Istio 贡献者合作，为 Istio 1.1 引入了许多旨在提高性能的新特性。1.1 中一些显著的性能增强包括:&lt;/p>
&lt;ul>
&lt;li>Envoy 生成统计数据的默认集合显著减少&lt;/li>
&lt;li>为 Mixer 工作负载添加了减载特性&lt;/li>
&lt;li>改进了 Envoy 和 Mixer 之间的协议&lt;/li>
&lt;li>隔离命名空间以减少操作开销&lt;/li>
&lt;li>可配置的并发工作线程，可以提高整体吞吐量&lt;/li>
&lt;li>为限制遥测数据的可配置过滤器&lt;/li>
&lt;li>解除同步瓶颈&lt;/li>
&lt;/ul>
&lt;h2 id="heading">持续的代码质量和性能验证&lt;/h2>
&lt;p>回归巡检促进了 Istio 性能和质量的不断提高，在幕后帮助 Istio 开发者识别并修正代码错误。每天的构建都会经过以客户为中心的性能基准 &lt;a href="https://github.com/blueperf/">BluePerf&lt;/a> 的性能检测。测试结果会展示在 &lt;a href="https://ibmcloud-perf.istio.io/regpatrol/">Istio 社区门户网站&lt;/a>。评估了各种应用配置以帮助洞悉 Istio 组件的性能。&lt;/p>
&lt;p>另一个用于评估 Istio 构建性能的工具是 &lt;a href="https://fortio.org/">Fortio&lt;/a>，它提供了一个综合的端到端的压力测试基准。&lt;/p>
&lt;h2 id="heading-1">概要&lt;/h2>
&lt;p>Istio 1.1 旨在提高性能和可扩展性。Istio Performance and Scalability （性能和可扩展性）工作组实现了自 1.0 以来显著的性能改进。
Istio 1.1 提供的新特性和功能优化，提高了服务网格对企业工作负载的支撑能力。Istio 1.1 性能和调优指南记录了性能模拟，提供了调整和容量规划指导，并包含了调优客户用例的最佳实践。&lt;/p>
&lt;h2 id="heading-2">有用的链接&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?time_continue=349&amp;amp;v=G4F5aRFEXnU">Istio 服务网格性能 (34:30)&lt;/a>, 作者：Surya Duggirala, Laurent Demailly 和 Fawad Khaliq 于 Kubecon Europe 2018&lt;/li>
&lt;li>&lt;a href="https://discuss.istio.io/c/performance-and-scalability">Istio 性能和可扩展性讨论专题&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="heading-3">免责声明&lt;/h2>
&lt;p>这里展示的性能数据是在一个可控的隔离环境中产生的。在其他环境中获得的实际结果可能存在较大差异。无法保证在其他地方获得相同或类似的结果。&lt;/p></description><pubDate>Mon, 18 Mar 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/istio1.1_perf/</link><author>Surya V Duggirala (IBM), Mandar Jog (Google), Jose Nativio (IBM)</author><guid isPermaLink="true">/v1.1/zh/blog/2019/istio1.1_perf/</guid><category>performance</category><category>scalability</category><category>scale</category><category>benchmarks</category></item><item><title>Istio 1.0.6 发布</title><description>&lt;p>在此高兴地宣布：Istio 1.0.6 已经发布。请查看&lt;a href="/v1.1/zh/about/notes/1.0.6/">发行说明&lt;/a> 来了解和下载新版本。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.0.6"
data-updateadvice='Before you download 1.0.6, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.0.8'
data-updatehref="/v1.1/zh/about/notes/1.0.8">
1.0.6 下载
&lt;/button>
&lt;a class="btn" href="https://archive.istio.io/v1.0">1.0.6 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.0.5...1.0.6">1.0.6 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Tue, 12 Feb 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-1.0.6/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-1.0.6/</guid></item><item><title>多集群服务网格中的分版本路由</title><description>&lt;p>如果花一点时间对 Istio 进行了解，你可能会注意到，大量的功能都可以在单一的 Kubernetes 集群中，用简单的&lt;a href="/v1.1/zh/docs/tasks">任务&lt;/a>和&lt;a href="/v1.1/zh/docs/examples/">示例&lt;/a>所表达的方式来运行。但是真实世界中的云计算和基于微服务的应用往往不是这么简单的，会需要在不止一个地点分布运行，用户难免会产生怀疑，生产环境中是否还能这样运行？&lt;/p>
&lt;p>幸运的是，Istio 提供了多种服务网格的配置方式，应用能够用近乎透明的方式加入一个跨越多个集群运行的服务网格之中，也就是&lt;a href="/v1.1/zh/docs/concepts/multicluster-deployments/#%E5%A4%9A%E9%9B%86%E7%BE%A4%E6%9C%8D%E5%8A%A1%E7%BD%91%E6%A0%BC">多集群服务网格&lt;/a>。最简单的设置多集群网格的方式，就是使用&lt;a href="/v1.1/zh/docs/concepts/multicluster-deployments/#%E5%A4%9A%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E6%8B%93%E6%89%91">多控制平面拓扑&lt;/a>，这种方式不需要特别的网络依赖。在这种条件下，每个 Kubernetes 集群都有自己的控制平面，但是每个控制平面都是同步的，并接受统一的管理。&lt;/p>
&lt;p>本文中，我们会在多控制平面拓扑形式的多集群网格中尝试一下 Istio 的&lt;a href="/v1.1/zh/docs/concepts/traffic-management/">流量管理&lt;/a>功能。我们会展示如何配置 Istio 路由规则，在多集群服务网格中部署 &lt;a href="https://github.com/istio/istio/tree/release-1.1/samples/bookinfo">Bookinfo 示例&lt;/a>，&lt;code>reviews&lt;/code> 服务的 &lt;code>v1&lt;/code> 版本运行在一个集群上，而 &lt;code>v2&lt;/code> 和 &lt;code>v3&lt;/code> 运行在另一个集群上，并完成远程服务调用。&lt;/p>
&lt;h2 id="setup-clusters">集群部署&lt;/h2>
&lt;p>首先需要部署两个 Kubernetes 集群，并各自运行一个做了轻度定制的 Istio。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>依照&lt;a href="/v1.1/zh/docs/setup/kubernetes/install/multicluster/gateways/">使用 Gateway 连接多个集群&lt;/a>中提到的步骤设置一个多集群环境。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>kubectl&lt;/code> 命令可以使用 &lt;code>--context&lt;/code> 参数访问两个集群。
使用下面的命令列出所有 &lt;code>context&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl config get-contexts
CURRENT NAME CLUSTER AUTHINFO NAMESPACE
* cluster1 cluster1 user@foo.com default
cluster2 cluster2 user@foo.com default
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>将配置文件中的 &lt;code>context&lt;/code> 名称赋值给两个环境变量：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export CTX_CLUSTER1=&amp;lt;cluster1 context name&amp;gt;
$ export CTX_CLUSTER2=&amp;lt;cluster2 context name&amp;gt;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;h2 id="-cluster1--bookinfo--v1-">在 &lt;code>cluster1&lt;/code> 中部署 &lt;code>bookinfo&lt;/code> 的 &lt;code>v1&lt;/code> 版本&lt;/h2>
&lt;p>在 &lt;code>cluster1&lt;/code> 中运行 &lt;code>productpage&lt;/code> 和 &lt;code>details&lt;/code> 服务，以及 &lt;code>reviews&lt;/code> 服务的 &lt;code>v1&lt;/code> 版本。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl label --context=$CTX_CLUSTER1 namespace default istio-injection=enabled
$ kubectl apply --context=$CTX_CLUSTER1 -f - &amp;lt;&amp;lt;EOF
apiVersion: v1
kind: Service
metadata:
name: productpage
labels:
app: productpage
spec:
ports:
- port: 9080
name: http
selector:
app: productpage
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: productpage-v1
spec:
replicas: 1
template:
metadata:
labels:
app: productpage
version: v1
spec:
containers:
- name: productpage
image: istio/examples-bookinfo-productpage-v1:1.10.0
imagePullPolicy: IfNotPresent
ports:
- containerPort: 9080
---
apiVersion: v1
kind: Service
metadata:
name: details
labels:
app: details
spec:
ports:
- port: 9080
name: http
selector:
app: details
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: details-v1
spec:
replicas: 1
template:
metadata:
labels:
app: details
version: v1
spec:
containers:
- name: details
image: istio/examples-bookinfo-details-v1:1.10.0
imagePullPolicy: IfNotPresent
ports:
- containerPort: 9080
---
apiVersion: v1
kind: Service
metadata:
name: reviews
labels:
app: reviews
spec:
ports:
- port: 9080
name: http
selector:
app: reviews
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: reviews-v1
spec:
replicas: 1
template:
metadata:
labels:
app: reviews
version: v1
spec:
containers:
- name: reviews
image: istio/examples-bookinfo-reviews-v1:1.10.0
imagePullPolicy: IfNotPresent
ports:
- containerPort: 9080
EOF
&lt;/code>&lt;/pre>
&lt;h2 id="-cluster2--bookinfo--v2--v3">在 &lt;code>cluster2&lt;/code> 中部署 &lt;code>bookinfo&lt;/code> 的 &lt;code>v2&lt;/code> 和 &lt;code>v3&lt;/code>&lt;/h2>
&lt;p>在 &lt;code>cluster2&lt;/code> 中运行 &lt;code>ratings&lt;/code> 服务以及 &lt;code>reviews&lt;/code> 服务的 &lt;code>v2&lt;/code> 和 &lt;code>v3&lt;/code> 版本：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl label --context=$CTX_CLUSTER2 namespace default istio-injection=enabled
$ kubectl apply --context=$CTX_CLUSTER2 -f - &amp;lt;&amp;lt;EOF
apiVersion: v1
kind: Service
metadata:
name: ratings
labels:
app: ratings
spec:
ports:
- port: 9080
name: http
selector:
app: ratings
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: ratings-v1
spec:
replicas: 1
template:
metadata:
labels:
app: ratings
version: v1
spec:
containers:
- name: ratings
image: istio/examples-bookinfo-ratings-v1:1.10.0
imagePullPolicy: IfNotPresent
ports:
- containerPort: 9080
---
apiVersion: v1
kind: Service
metadata:
name: reviews
labels:
app: reviews
spec:
ports:
- port: 9080
name: http
selector:
app: reviews
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: reviews-v2
spec:
replicas: 1
template:
metadata:
labels:
app: reviews
version: v2
spec:
containers:
- name: reviews
image: istio/examples-bookinfo-reviews-v2:1.10.0
imagePullPolicy: IfNotPresent
ports:
- containerPort: 9080
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: reviews-v3
spec:
replicas: 1
template:
metadata:
labels:
app: reviews
version: v3
spec:
containers:
- name: reviews
image: istio/examples-bookinfo-reviews-v3:1.10.0
imagePullPolicy: IfNotPresent
ports:
- containerPort: 9080
EOF
&lt;/code>&lt;/pre>
&lt;h2 id="-bookinfo-">访问 &lt;code>bookinfo&lt;/code> 应用&lt;/h2>
&lt;p>和平常一样，我们需要使用一个 Istio gateway 来访问 &lt;code>bookinfo&lt;/code> 应用。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>在 &lt;code>cluster1&lt;/code> 中创建 &lt;code>bookinfo&lt;/code> 的网关：&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/bookinfo-gateway.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply --context=$CTX_CLUSTER1 -f @samples/bookinfo/networking/bookinfo-gateway.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;li>
&lt;p>遵循 &lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E7%A1%AE%E5%AE%9A-ingress-%E7%9A%84-ip-%E5%92%8C%E7%AB%AF%E5%8F%A3">Bookinfo 示例应用&lt;/a>中的步骤，确定 Ingress 的 IP 和端口，用浏览器打开 &lt;code>http://$GATEWAY_URL/productpage&lt;/code>。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这里会看到 &lt;code>productpage&lt;/code>，其中包含了 &lt;code>reviews&lt;/code> 的内容，但是没有出现 &lt;code>ratings&lt;/code>，这是因为只有 &lt;code>reviews&lt;/code> 服务的 &lt;code>v1&lt;/code> 版本运行在 &lt;code>cluster1&lt;/code> 上，我们还没有配置到 &lt;code>cluster2&lt;/code> 的访问。&lt;/p>
&lt;h2 id="-cluster1--reviews--serviceentry--destinationrule">在 &lt;code>cluster1&lt;/code> 上为远端的 &lt;code>reviews&lt;/code> 服务创建 &lt;code>ServiceEntry&lt;/code> 以及 &lt;code>DestinationRule&lt;/code>&lt;/h2>
&lt;p>根据&lt;a href="/v1.1/zh/docs/setup/kubernetes/install/multicluster/gateways/#%E9%85%8D%E7%BD%AE-dns">配置指南&lt;/a>中的介绍，远程服务可以用一个 &lt;code>.global&lt;/code> 的 DNS 名称进行访问。在我们的案例中，就是 &lt;code>reviews.default.global&lt;/code>，所以我们需要为这个主机创建 &lt;code>ServiceEntry&lt;/code> 和 &lt;code>DestinationRule&lt;/code>。&lt;code>ServiceEntry&lt;/code> 会使用 &lt;code>cluster2&lt;/code> 网关作为端点地址来访问服务。可以使用网关的 DNS 名称或者公共 IP：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export CLUSTER2_GW_ADDR=$(kubectl get --context=$CTX_CLUSTER2 svc --selector=app=istio-ingressgateway \
-n istio-system -o jsonpath=&amp;#34;{.items[0].status.loadBalancer.ingress[0].ip}&amp;#34;)
&lt;/code>&lt;/pre>
&lt;p>用下面的命令来创建 &lt;code>ServiceEntry&lt;/code> 和 &lt;code>DestinationRule&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply --context=$CTX_CLUSTER1 -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: reviews-default
spec:
hosts:
- reviews.default.global
location: MESH_INTERNAL
ports:
- name: http1
number: 9080
protocol: http
resolution: DNS
addresses:
- 127.255.0.3
endpoints:
- address: ${CLUSTER2_GW_ADDR}
labels:
cluster: cluster2
ports:
http1: 15443 # 不要修改端口值
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: reviews-global
spec:
host: reviews.default.global
trafficPolicy:
tls:
mode: ISTIO_MUTUAL
subsets:
- name: v2
labels:
cluster: cluster2
- name: v3
labels:
cluster: cluster2
EOF
&lt;/code>&lt;/pre>
&lt;p>&lt;code>ServiceEntry&lt;/code> 的地址 &lt;code>127.255.0.3&lt;/code> 可以是任意的未分配 IP。在 &lt;code>127.0.0.0/8&lt;/code> 的范围里面进行选择是个不错的主意。阅读&lt;a href="/v1.1/zh/docs/examples/multicluster/gateways/#configure-the-example-services">通过网关进行连接的多集群&lt;/a>一文，能够获得更多相关信息。&lt;/p>
&lt;p>注意 &lt;code>DestinationRule&lt;/code> 中的 &lt;code>subset&lt;/code> 的标签，&lt;code>cluster: cluster2&lt;/code> 对应的是 &lt;code>cluster2&lt;/code> 网关。一旦流量到达目标集群，就会由本地目的 &lt;code>DestinationRule&lt;/code> 来鉴别实际的 Pod 标签（&lt;code>version: v1&lt;/code> 或者 &lt;code>version: v2&lt;/code>）&lt;/p>
&lt;h2 id="-reviews--destinationrule">在所有集群上为本地 &lt;code>reviews&lt;/code> 服务创建 &lt;code>DestinationRule&lt;/code>&lt;/h2>
&lt;p>技术上来说，我们只需要为每个集群定义本地的 &lt;code>subset&lt;/code> 即可（&lt;code>cluster1&lt;/code> 中的 &lt;code>v1&lt;/code>，&lt;code>cluster2&lt;/code> 中的 &lt;code>v2&lt;/code> 和 &lt;code>v3&lt;/code>），但是定义一个用不到的并未部署的版本也没什么大碍，为了清晰一点，我们会在两个集群上都创建全部三个 &lt;code>subset&lt;/code>。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply --context=$CTX_CLUSTER1 -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: reviews
spec:
host: reviews.default.svc.cluster.local
trafficPolicy:
tls:
mode: ISTIO_MUTUAL
subsets:
- name: v1
labels:
version: v1
- name: v2
labels:
version: v2
- name: v3
labels:
version: v3
EOF
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply --context=$CTX_CLUSTER2 -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: reviews
spec:
host: reviews.default.svc.cluster.local
trafficPolicy:
tls:
mode: ISTIO_MUTUAL
subsets:
- name: v1
labels:
version: v1
- name: v2
labels:
version: v2
- name: v3
labels:
version: v3
EOF
&lt;/code>&lt;/pre>
&lt;h2 id="-virtualservice--reviews-">创建 &lt;code>VirtualService&lt;/code> 来路由 &lt;code>reviews&lt;/code> 服务的流量&lt;/h2>
&lt;p>目前所有调用 &lt;code>reviews&lt;/code> 服务的流量都会进入本地的 &lt;code>reviews&lt;/code> Pod，也就是 &lt;code>v1&lt;/code>，如果查看一下远吗，会发现 &lt;code>productpage&lt;/code> 的实现只是简单的对 &lt;code>http://reviews:9080&lt;/code> （也就是 &lt;code>reviews.default.svc.cluster.local&lt;/code>）发起了请求，也就是本地版本。对应的远程服务名称为 &lt;code>reviews.default.global&lt;/code>，所以需要用路由规则来把请求转发到远端集群。&lt;/p>
&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">注意如果所有版本的 &lt;code>reviews&lt;/code> 服务都在远端，也就是说本地没有 &lt;code>reviews&lt;/code> 服务，那么 DNS 就会把 &lt;code>reviews&lt;/code> 直接解析到 &lt;code>reviews.default.global&lt;/code>，在本文的环境里，无需定义任何路由规则就可以发起对远端集群的请求。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;p>创建下列的 &lt;code>VirtualService&lt;/code>，把 &lt;code>jason&lt;/code> 的流量转发给运行在 &lt;code>cluster2&lt;/code> 上的 &lt;code>v2&lt;/code> 和 &lt;code>v3&lt;/code> 版本的 &lt;code>reviews&lt;/code>，两个版本各负责一半流量。其他用户的流量还是会发给 &lt;code>v1&lt;/code> 版本的 &lt;code>reviews&lt;/code>。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply --context=$CTX_CLUSTER1 -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: reviews
spec:
hosts:
- reviews.default.svc.cluster.local
http:
- match:
- headers:
end-user:
exact: jason
route:
- destination:
host: reviews.default.global
subset: v2
weight: 50
- destination:
host: reviews.default.global
subset: v3
weight: 50
- route:
- destination:
host: reviews.default.svc.cluster.local
subset: v1
EOF
&lt;/code>&lt;/pre>
&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">这种平均分配的规则并不实际，只是一种用于演示远端服务多版本之间流量分配的方便手段。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;p>回到浏览器，用 &lt;code>jason&lt;/code> 的身份登录。刷新页面几次，会看到星形图标在红黑两色之间切换（&lt;code>v2&lt;/code> 和 &lt;code>v3&lt;/code>）。如果登出，就只会看到没有 &lt;code>ratings&lt;/code> 的 &lt;code>reviews&lt;/code> 服务了。&lt;/p>
&lt;h2 id="heading">总结&lt;/h2>
&lt;p>本文中，我们看到在&lt;a href="/v1.1/zh/docs/concepts/multicluster-deployments/#%E5%A4%9A%E6%8E%A7%E5%88%B6%E5%B9%B3%E9%9D%A2%E6%8B%93%E6%89%91">多控制平面拓扑&lt;/a>的多集群网格中，如何使用 Istio 路由规则进行跨集群的流量分配。
这里我们手工配置了 &lt;code>.global&lt;/code> 的 &lt;code>ServiceEntry&lt;/code> 以及 &lt;code>DestinationRule&lt;/code>，用于进行对远端集群中 &lt;code>reviews&lt;/code> 服务的访问。实际上如果我们想要的话，可以让任何服务都在远端或本地运行，当然需要为远端服务配置 &lt;code>.global&lt;/code> 的相关资源。幸运的是，这个过程可以自动化，并且可能在 Istio 的未来版本中实现。&lt;/p></description><pubDate>Thu, 07 Feb 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/multicluster-version-routing/</link><author>Frank Budinsky (IBM)</author><guid isPermaLink="true">/v1.1/zh/blog/2019/multicluster-version-routing/</guid><category>traffic-management</category><category>multicluster</category></item><item><title>博客策略更新</title><description>&lt;p>欢迎来到 Istio 博客。&lt;/p>
&lt;p>为了让你更方便的在我们的网站上发表博客，我们&lt;a href="/v1.1/zh/about/contribute/creating-and-editing-pages/#choosing-a-page-type">更新了内容类型指南&lt;/a>。&lt;/p>
&lt;p>这次更新的目标是对内容进行更方便的分享和搜索。&lt;/p>
&lt;p>我们希望更简单的共享 Istio 方面的最新信息，Istio 博客是一个好的出发点。&lt;/p>
&lt;p>如果你认为你的内容符合下面四个分类中的一个，我们非常欢迎将其提交到 Istio 博客之中。&lt;/p>
&lt;ul>
&lt;li>文章中详细介绍了你在使用和配置 Istio 中的经验。如果分享了一种新的观点或体验，那就更好了。&lt;/li>
&lt;li>文章中着重介绍或宣布了 Istio 的功能。&lt;/li>
&lt;li>文章中对 Istio 的相关事件进行了发布或回顾。&lt;/li>
&lt;li>文章中详细讲解了使用 Istio 完成一个任务或者满足特定用例需求的相关内容。&lt;/li>
&lt;/ul>
&lt;p>仅需&lt;a href="/v1.1/zh/about/contribute/github/#%E5%A6%82%E4%BD%95%E8%B4%A1%E7%8C%AE">提交一个 PR&lt;/a>，如果需要，可以&lt;a href="/v1.1/zh/about/contribute/github/#review">申请评审&lt;/a>。&lt;/p>
&lt;p>我们期待能够很快就看到你的 Istio 体验！&lt;/p></description><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/sail-the-blog/</link><author>Rigs Caballero, Google</author><guid isPermaLink="true">/v1.1/zh/blog/2019/sail-the-blog/</guid><category>community</category><category>blog</category><category>contribution</category><category>guide</category><category>guideline</category><category>event</category></item><item><title>Istio Sidecar 注入过程解密</title><description>&lt;p>Istio 服务网格架构的概述，通常都是从对数据面和控制面的叙述开始的。&lt;/p>
&lt;p>&lt;a href="/v1.1/zh/docs/concepts/what-is-istio/#%E6%9E%B6%E6%9E%84">来自 Istio 的文档：&lt;/a>&lt;/p>
&lt;div>
&lt;aside class="callout quote">
&lt;div class="type">
&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-quote"/>&lt;/svg>
&lt;/div>
&lt;div class="content">&lt;p>Istio 服务网格逻辑上分为数据平面和控制平面。&lt;/p>
&lt;p>数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。这些代理和 Mixer（一个通用的策略和遥测中心）合作，对所有微服务之间的之间所有的网络通信进行控制。&lt;/p>
&lt;p>控制平面负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。&lt;/p>
&lt;/div>
&lt;/aside>
&lt;/div>
&lt;figure style="width:40%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:80%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/data-plane-setup/./arch-2.svg" title="Istio 架构">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/data-plane-setup/./arch-2.svg" alt="基于 Istio 的应用的总体架构。" />
&lt;/a>
&lt;/div>
&lt;figcaption>Istio 架构&lt;/figcaption>
&lt;/figure>
&lt;p>Sidecar 注入到应用的过程，可以是自动的，也可以是手动的，了解这一过程是很重要的。应用的流量会被重定向进入或流出 Sidecar，开发人员无需关心。应用接入 Istio 服务网格之后，开发者可以开始使用网格功能并从中受益。然而数据平面是如何工作的，以及需要怎样的条件才能完成这种无缝工作？本文中我们会深入到 Sidecar 注入模型中，来更清晰的了解 Sidecar 的注入过程。&lt;/p>
&lt;h2 id="sidecar-">Sidecar 注入&lt;/h2>
&lt;p>简单来说，注入 Sidecar 就是把附加的容器配置插入 Pod 模板的过程。Istio 服务网格所需的附加容器是：&lt;/p>
&lt;p>&lt;code>istio-init&lt;/code>&lt;/p>
&lt;p>这个&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">初始化容器&lt;/a>用于设置 &lt;code>iptables&lt;/code> 规则，让出入流量都转由 Sidecar 进行处理。和应用容器相比，初始化容器有几点不同：&lt;/p>
&lt;ul>
&lt;li>它在应用容器之前启动，并且会运行到结束。&lt;/li>
&lt;li>如果有多个初始化容器，只有在前一个初始化容器成功结束之后才会启动下一个。&lt;/li>
&lt;/ul>
&lt;p>不看看出，这种容器是非常适合执行启动或者初始化任务的，并且也无需和实际的应用容器集成到一起。&lt;code>istio-init&lt;/code> 只是用来设置 &lt;code>iptables&lt;/code> 的规则。&lt;/p>
&lt;p>&lt;code>istio-proxy&lt;/code>&lt;/p>
&lt;p>这个容器是真正的 Sidecar（基于 Envoy）。&lt;/p>
&lt;h3 id="heading">手工注入&lt;/h3>
&lt;p>要进行手工注入，只要用 &lt;code>istioctl&lt;/code> 把前面提到的两个容器定义加入到 Pod 的模板之中即可。不论是手工注入还是自动注入，都是从 &lt;code>istio-sidecar-injector&lt;/code> 以及 &lt;code>istio&lt;/code> 两个 Configmap 对象中获取配置的。&lt;/p>
&lt;p>我们先来看看 &lt;code>istio-sidecar-injector&lt;/code> Configmap，了解一下其中的内容。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' data-outputis='yaml' >$ kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath=&amp;#39;{.data.config}&amp;#39;
部分输出内容：
policy: enabled
template: |-
initContainers:
- name: istio-init
image: docker.io/istio/proxy_init:1.0.2
args:
- &amp;#34;-p&amp;#34;
- [[ .MeshConfig.ProxyListenPort ]]
- &amp;#34;-u&amp;#34;
- 1337
.....
imagePullPolicy: IfNotPresent
securityContext:
capabilities:
add:
- NET_ADMIN
restartPolicy: Always
containers:
- name: istio-proxy
image: [[ if (isset .ObjectMeta.Annotations &amp;#34;sidecar.istio.io/proxyImage&amp;#34;) -]]
&amp;#34;[[ index .ObjectMeta.Annotations &amp;#34;sidecar.istio.io/proxyImage&amp;#34; ]]&amp;#34;
[[ else -]]
docker.io/istio/proxyv2:1.0.2
[[ end -]]
args:
- proxy
- sidecar
.....
env:
.....
- name: ISTIO_META_INTERCEPTION_MODE
value: [[ or (index .ObjectMeta.Annotations &amp;#34;sidecar.istio.io/interceptionMode&amp;#34;) .ProxyConfig.InterceptionMode.String ]]
imagePullPolicy: IfNotPresent
securityContext:
readOnlyRootFilesystem: true
[[ if eq (or (index .ObjectMeta.Annotations &amp;#34;sidecar.istio.io/interceptionMode&amp;#34;) .ProxyConfig.InterceptionMode.String) &amp;#34;TPROXY&amp;#34; -]]
capabilities:
add:
- NET_ADMIN
restartPolicy: Always
.....
&lt;/code>&lt;/pre>
&lt;p>如你所见，这个 Configmap 中包含了前面提到的两个容器的配置内容：&lt;code>istio-init&lt;/code> 初始化容器以及 &lt;code>istio-proxy&lt;/code> 代理容器。配置中包含了容器镜像名称以及一些参数，例如拦截模式，权限要求等。&lt;/p>
&lt;p>从安全角度看来，要注意 &lt;code>istio-init&lt;/code> 需要 &lt;code>NET_ADMIN&lt;/code> 权限，以便在 Pod 的命名空间中修改 &lt;code>iptables&lt;/code>；如果 &lt;code>istio-proxy&lt;/code> 设置为 &lt;code>TPROXY&lt;/code> 模式，也会需要这一权限。这一权限是限制在 Pod 的命名空间之内的，这应该没问题。然而我注意到最近的 open-shift 版本好像在这方面有点问题，需要进一步确认。这方面的选项会在文末继续讨论。&lt;/p>
&lt;p>要修改当前的 Pod 模板来进行注入，可以：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ istioctl kube-inject -f demo-red.yaml | kubectl apply -f -
&lt;/code>&lt;/pre>
&lt;p>或者&lt;/p>
&lt;p>想要使用修改过的 Configmap 或者本地 Configmap：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>从 Configmap 中创建 &lt;code>inject-config.yaml&lt;/code> and &lt;code>mesh-config.yaml&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath=&amp;#39;{.data.config}&amp;#39; &amp;gt; inject-config.yaml
$ kubectl -n istio-system get configmap istio -o=jsonpath=&amp;#39;{.data.mesh}&amp;#39; &amp;gt; mesh-config.yaml
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>修改现存的 Pod 模板，这里假设文件名为 &lt;code>demo-red.yaml&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ istioctl kube-inject --injectConfigFile inject-config.yaml --meshConfigFile mesh-config.yaml --filename demo-red.yaml --output demo-red-injected.yaml
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>提交 &lt;code>demo-red-injected.yaml&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f demo-red-injected.yaml
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;p>上面的几个步骤中，我们使用 &lt;code>sidecar-injector&lt;/code> 以及 &lt;code>istio&lt;/code> 两个 Configmap 的内容修改了 Pod 模板并使用 &lt;code>kubectl&lt;/code> 命令进行了提交。如果查看一下注入后的 YAML 文件，会看到前面讨论过的 Istio 的专属容器，提交到集群上之后，会看到两个容器在运行：一个是实际的应用容器，另一个则是 &lt;code>istio-proxy&lt;/code> Sidecar。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods | grep demo-red
demo-red-pod-8b5df99cc-pgnl7 2/2 Running 0 3d
&lt;/code>&lt;/pre>
&lt;p>这里没有 3 个 Pod，这是因为 &lt;code>istio-init&lt;/code> 容器是一个初始化容器，它完成任务之后就会退出——他的任务就是设置 Pod 内的 &lt;code>iptables&lt;/code> 规则。要确认退出的初始化容器，可以看看 &lt;code>kubectl describe&lt;/code> 的输出：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' data-outputis='yaml' >$ kubectl describe pod demo-red-pod-8b5df99cc-pgnl7
SNIPPET from the output:
Name: demo-red-pod-8b5df99cc-pgnl7
Namespace: default
.....
Labels: app=demo-red
pod-template-hash=8b5df99cc
version=version-red
Annotations: sidecar.istio.io/status={&amp;#34;version&amp;#34;:&amp;#34;3c0b8d11844e85232bc77ad85365487638ee3134c91edda28def191c086dc23e&amp;#34;,&amp;#34;initContainers&amp;#34;:[&amp;#34;istio-init&amp;#34;],&amp;#34;containers&amp;#34;:[&amp;#34;istio-proxy&amp;#34;],&amp;#34;volumes&amp;#34;:[&amp;#34;istio-envoy&amp;#34;,&amp;#34;istio-certs...
Status: Running
IP: 10.32.0.6
Controlled By: ReplicaSet/demo-red-pod-8b5df99cc
Init Containers:
istio-init:
Container ID: docker://bef731eae1eb3b6c9d926cacb497bb39a7d9796db49cd14a63014fc1a177d95b
Image: docker.io/istio/proxy_init:1.0.2
Image ID: docker-pullable://docker.io/istio/proxy_init@sha256:e16a0746f46cd45a9f63c27b9e09daff5432e33a2d80c8cc0956d7d63e2f9185
.....
State: Terminated
Reason: Completed
.....
Ready: True
Containers:
demo-red:
Container ID: docker://8cd9957955ff7e534376eb6f28b56462099af6dfb8b9bc37aaf06e516175495e
Image: chugtum/blue-green-image:v3
Image ID: docker-pullable://docker.io/chugtum/blue-green-image@sha256:274756dbc215a6b2bd089c10de24fcece296f4c940067ac1a9b4aea67cf815db
State: Running
Started: Sun, 09 Dec 2018 18:12:31 -0800
Ready: True
istio-proxy:
Container ID: docker://ca5d690be8cd6557419cc19ec4e76163c14aed2336eaad7ebf17dd46ca188b4a
Image: docker.io/istio/proxyv2:1.0.2
Image ID: docker-pullable://docker.io/istio/proxyv2@sha256:54e206530ba6ca9b3820254454e01b7592e9f986d27a5640b6c03704b3b68332
Args:
proxy
sidecar
.....
State: Running
Started: Sun, 09 Dec 2018 18:12:31 -0800
Ready: True
.....
&lt;/code>&lt;/pre>
&lt;p>从上文的输出可以看出，&lt;code>istio-init&lt;/code> 容器的 &lt;code>State&lt;/code> 字段值为 &lt;code>Terminated&lt;/code>，&lt;code>Reason&lt;/code> 是 &lt;code>Completed&lt;/code>。正在运行的两个 Pod 是应用容器 &lt;code>demo-red&lt;/code> 以及 &lt;code>istio-proxy&lt;/code>。&lt;/p>
&lt;h3 id="heading-1">自动注入&lt;/h3>
&lt;p>多数时候，用户不想在每次部署应用的时候都用 &lt;code>istioctl&lt;/code> 命令进行手工注入，这时候就可以使用 Istio 的自动注入功能来应对了。只要给用于部署应用的命名空间打个 &lt;code>istio-injection=enabled&lt;/code> 标签就可以了。&lt;/p>
&lt;p>打上标签之后，这个命名空间中新建的任何 Pod 都会被 Istio 注入 Sidecar。下面的例子里，&lt;code>istio-dev&lt;/code> 命名空间中部署的 Pod 被自动注入了 Sidecar：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get namespaces --show-labels
NAME STATUS AGE LABELS
default Active 40d &amp;lt;none&amp;gt;
istio-dev Active 19d istio-injection=enabled
istio-system Active 24d &amp;lt;none&amp;gt;
kube-public Active 40d &amp;lt;none&amp;gt;
kube-system Active 40d &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>这是怎么完成的呢？要回答这一问题，首先要了解一下 Kubernetes 的准入控制器。&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">来自 Kubernetes 文档&lt;/a>&lt;/p>
&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">准入控制器是一段代码，会拦截 Kubernetes API Server 收到的请求，拦截发生在认证和鉴权完成之后，对象进行持久化之前。可以定义两种类型的 Admission webhook：Validating 和 Mutating。Validating 类型的 Webhook 可以根据自定义的准入策略决定是否拒绝请求；Mutating 类型的 Webhook 可以根据自定义配置来对请求进行编辑。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;p>Istio 的自动注入过程中会依赖 Mutating webhook。我们看看 &lt;code>istio-sidecar-injector&lt;/code> 中的配置详情：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' data-outputis='yaml' >$ kubectl get mutatingwebhookconfiguration istio-sidecar-injector -o yaml
输出内容节选:
apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingWebhookConfiguration
metadata:
annotations:
kubectl.kubernetes.io/last-applied-configuration: |
{&amp;#34;apiVersion&amp;#34;:&amp;#34;admissionregistration.k8s.io/v1beta1&amp;#34;,&amp;#34;kind&amp;#34;:&amp;#34;MutatingWebhookConfiguration&amp;#34;,&amp;#34;metadata&amp;#34;:{&amp;#34;annotations&amp;#34;:{},&amp;#34;labels&amp;#34;:{&amp;#34;app&amp;#34;:&amp;#34;istio-sidecar-injector&amp;#34;,&amp;#34;chart&amp;#34;:&amp;#34;sidecarInjectorWebhook-1.0.1&amp;#34;,&amp;#34;heritage&amp;#34;:&amp;#34;Tiller&amp;#34;,&amp;#34;release&amp;#34;:&amp;#34;istio-remote&amp;#34;},&amp;#34;name&amp;#34;:&amp;#34;istio-sidecar-injector&amp;#34;,&amp;#34;namespace&amp;#34;:&amp;#34;&amp;#34;},&amp;#34;webhooks&amp;#34;:[{&amp;#34;clientConfig&amp;#34;:{&amp;#34;caBundle&amp;#34;:&amp;#34;&amp;#34;,&amp;#34;service&amp;#34;:{&amp;#34;name&amp;#34;:&amp;#34;istio-sidecar-injector&amp;#34;,&amp;#34;namespace&amp;#34;:&amp;#34;istio-system&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/inject&amp;#34;}},&amp;#34;failurePolicy&amp;#34;:&amp;#34;Fail&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;sidecar-injector.istio.io&amp;#34;,&amp;#34;namespaceSelector&amp;#34;:{&amp;#34;matchLabels&amp;#34;:{&amp;#34;istio-injection&amp;#34;:&amp;#34;enabled&amp;#34;}},&amp;#34;rules&amp;#34;:[{&amp;#34;apiGroups&amp;#34;:[&amp;#34;&amp;#34;],&amp;#34;apiVersions&amp;#34;:[&amp;#34;v1&amp;#34;],&amp;#34;operations&amp;#34;:[&amp;#34;CREATE&amp;#34;],&amp;#34;resources&amp;#34;:[&amp;#34;pods&amp;#34;]}]}]}
creationTimestamp: 2018-12-10T08:40:15Z
generation: 2
labels:
app: istio-sidecar-injector
chart: sidecarInjectorWebhook-1.0.1
heritage: Tiller
release: istio-remote
name: istio-sidecar-injector
.....
webhooks:
- clientConfig:
service:
name: istio-sidecar-injector
namespace: istio-system
path: /inject
name: sidecar-injector.istio.io
namespaceSelector:
matchLabels:
istio-injection: enabled
rules:
- apiGroups:
- &amp;#34;&amp;#34;
apiVersions:
- v1
operations:
- CREATE
resources:
- pods
&lt;/code>&lt;/pre>
&lt;p>这里可以看到一个 &lt;code>namespaceSelector&lt;/code>，其中的定义表明它的工作是针对带有 &lt;code>istio-injection: enabled&lt;/code> 标签的命名空间的。这种情况下，你还会看到 Pod 创建期间进行注入时的一些其它工作和相关资源的内容。当 &lt;code>apiserver&lt;/code> 收到一个符合规则的请求时，&lt;code>apiserver&lt;/code> 会给 Webhook 服务发送一个准入审核的请求，Webhook 服务的定义包含在 &lt;code>clientConfig&lt;/code> 配置中的 &lt;code>name: istio-sidecar-injector&lt;/code> 字段里。我们会看到，这个服务正在 &lt;code>istio-system&lt;/code> 命名空间里运行。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get svc --namespace=istio-system | grep sidecar-injector
istio-sidecar-injector ClusterIP 10.102.70.184 &amp;lt;none&amp;gt; 443/TCP 24d
&lt;/code>&lt;/pre>
&lt;p>这个配置总体上来说，完成了我们手工注入所需完成的工作。只不过是它是在 Pod 创建的过程中自动完成的，所以你也不会看到 Deployment 对象发生了任何变化。可以用 &lt;code>kubectl describe&lt;/code> 命令来查看 Sidecar 和初始化容器。如果想要修改注入逻辑，例如 Istio 自动注入的生效范围，可以编辑 &lt;code>MutatingWebhookConfiguration&lt;/code>，然后重启 Sidecar injector Pod。&lt;/p>
&lt;p>Sidecar 的自动注入过程除了根据 &lt;code>namespaceSelector&lt;/code> 选择命名空间之外，还受到缺省注入策略以及 Pod 自身注解的影响。&lt;/p>
&lt;p>再看看 &lt;code>istio-sidecar-injector&lt;/code> ConfigMap 中的缺省策略定义。可以看到，缺省是启用的。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' data-outputis='yaml' >$ kubectl -n istio-system get configmap istio-sidecar-injector -o=jsonpath=&amp;#39;{.data.config}&amp;#39;
SNIPPET from the output:
policy: enabled
template: |-
initContainers:
- name: istio-init
image: &amp;#34;gcr.io/istio-release/proxy_init:1.0.2&amp;#34;
args:
- &amp;#34;-p&amp;#34;
- [[ .MeshConfig.ProxyListenPort ]]
&lt;/code>&lt;/pre>
&lt;p>还可以在 Pod 模板中使用注解 &lt;code>sidecar.istio.io/inject&lt;/code> 覆盖缺省策略。下面的例子中的 &lt;code>Deployment&lt;/code> 用这种方式禁用了自动注入：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: ignored
spec:
template:
metadata:
annotations:
sidecar.istio.io/inject: &amp;#34;false&amp;#34;
spec:
containers:
- name: ignored
image: tutum/curl
command: [&amp;#34;/bin/sleep&amp;#34;,&amp;#34;infinity&amp;#34;]
&lt;/code>&lt;/pre>
&lt;p>这个例子展示了很多可能性，可以从命名空间标签、ConfigMap 或者 Pod 中分别进行控制：&lt;/p>
&lt;ul>
&lt;li>Webhooks 定义中的 &lt;code>namespaceSelector&lt;/code>（&lt;code>istio-injection: enabled&lt;/code>）&lt;/li>
&lt;li>缺省策略（在 ConfigMap &lt;code>istio-sidecar-injector&lt;/code> 中定义）&lt;/li>
&lt;li>Pod 注解（&lt;code>sidecar.istio.io/inject&lt;/code>）&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="/v1.1/zh/help/ops/setup/injection/">注入状态表&lt;/a>中，根据上面三个因素的不同，可以看到有不同的注入结果。&lt;/p>
&lt;h2 id="-sidecar-">从应用容器到 Sidecar 代理的通信&lt;/h2>
&lt;p>现在我们知道了 Sidecar 容器和初始化容器被注入到应用中的过程了，Sidecar 代理是如何截获进出容器的流量呢？我们前面提到过，这是通过对 Pod 命名空间中 &lt;code>iptable&lt;/code> 规则的设置来完成的，这个设置过程由 &lt;code>istio-init&lt;/code> 容器来控制。现在可以看看命名空间中到底更新了些什么。&lt;/p>
&lt;p>进入前面我们部署的应用 Pod 的命名空间，看看配置完成的 iptables。我会使用 &lt;code>nsenter&lt;/code>。也可以用特权模式进入容器获得同样的信息。如果无法访问节点，可以用 &lt;code>exec&lt;/code> 进入 Sidecar 来执行 &lt;code>iptables&lt;/code> 指令。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ docker inspect b8de099d3510 --format &amp;#39;{{ .State.Pid }}&amp;#39;
4125
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ nsenter -t 4215 -n iptables -t nat -S
-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P OUTPUT ACCEPT
-P POSTROUTING ACCEPT
-N ISTIO_INBOUND
-N ISTIO_IN_REDIRECT
-N ISTIO_OUTPUT
-N ISTIO_REDIRECT
-A PREROUTING -p tcp -j ISTIO_INBOUND
-A OUTPUT -p tcp -j ISTIO_OUTPUT
-A ISTIO_INBOUND -p tcp -m tcp --dport 80 -j ISTIO_IN_REDIRECT
-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15001
-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -j ISTIO_REDIRECT
-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN
-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN
-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN
-A ISTIO_OUTPUT -j ISTIO_REDIRECT
-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001
&lt;/code>&lt;/pre>
&lt;p>上面展示的内容中，可以清楚的看到，所有从 80 端口（&lt;code>red-demo&lt;/code> 应用监听的端口）进入的流量，被 &lt;code>REDIRECTED&lt;/code> 到了端口 &lt;code>15001&lt;/code>，这是 &lt;code>istio-proxy&lt;/code>（Envoy 代理）监听的端口。外发流量也是如此。&lt;/p>
&lt;p>本文进入尾声。希望能够让读者了解到 Istio 将 Sidecar 注入到 Pod 中的过程，以及 Istio 将流量路由到代理服务器的过程。&lt;/p>
&lt;div>
&lt;aside class="callout idea">
&lt;div class="type">
&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-idea"/>&lt;/svg>
&lt;/div>
&lt;div class="content">更新：在 &lt;code>istio-init&lt;/code> 阶段，现在有了一个新的使用 CNI 的方式，这种方式无需初始化容器的帮助，也不需要对应的权限。&lt;a href="https://github.com/istio/cni">&lt;code>istio-cni&lt;/code>&lt;/a> 插件会设置 Pod 的网络来满足这一要求，可以代替 &lt;code>istio-init&lt;/code> 来完成任务。&lt;/div>
&lt;/aside>
&lt;/div></description><pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/data-plane-setup/</link><author>Manish Chugtu</author><guid isPermaLink="true">/v1.1/zh/blog/2019/data-plane-setup/</guid><category>kubernetes</category><category>sidecar-injection</category><category>traffic-management</category></item><item><title>Egress gateway 性能测试</title><description>&lt;p>为了从网格中访问外部服务（本例中使用的是 MongoDB），需要加入 Egress gateway，本次测试的主要目的就是调查这一行为对性能和资源使用造成的影响。在博客&lt;a href="/v1.1/zh/blog/2018/egress-mongo/">使用外部 MongoDB 服务&lt;/a>中介绍了为外部 MongoDB 配置 Egress gateway 的具体步骤。&lt;/p>
&lt;p>本次测试中使用的应用是 Acmeair 的 Java 版，这个应用会模拟一个航空订票系统。在 Istio 的每日构建中会使用该应用来进行性能的回归测试，但是在回归测试过程中，这些应用会使用自己的 Sidecar 来访问外部的 MongoDB，而不是 Egress gateway。&lt;/p>
&lt;p>下图描述了目前的 Istio 回归测试过程中，Acmeair 应用的运行方式：&lt;/p>
&lt;figure style="width:70%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:62.69230769230769%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/egress-performance/./acmeair_regpatrol3.png" title="在 Istio 性能回归测试环境中的 Acmeair 基准测试">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/egress-performance/./acmeair_regpatrol3.png" alt="在 Istio 性能回归测试环境中的 Acmeair 基准测试" />
&lt;/a>
&lt;/div>
&lt;figcaption>在 Istio 性能回归测试环境中的 Acmeair 基准测试&lt;/figcaption>
&lt;/figure>
&lt;p>还有一个差别就是，这一应用和外部数据库使用的是明文的 MongoDB 协议。本文中的第一个变化就是将应用到外部 MongoDB 之间的连接升级为 TLS 模式，以体现更贴近实际情况的场景。&lt;/p>
&lt;p>下面会讲到一些从网格中访问外部数据库的具体案例。&lt;/p>
&lt;h2 id="egress-">Egress 流量案例&lt;/h2>
&lt;h3 id="-1-sidecar">案例 1：绕过 Sidecar&lt;/h3>
&lt;p>在这个案例中，Sidecar 对应用和外部数据库之间的通信不做拦截。这一配置是通过初始化容器中的 &lt;code>-x&lt;/code> 参数来完成的，将其内容设置为 MongoDB 的 CIDR 即可。这种做法导致 Sidecar 忽略流入/流出指定 IP 地址的流量。举例来说：&lt;/p>
&lt;pre>&lt;code> - -x
- &amp;quot;169.47.232.211/32&amp;quot;
&lt;/code>&lt;/pre>
&lt;figure style="width:70%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:76.45536869340232%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/egress-performance/./case1_sidecar_bypass3.png" title="绕过 Sidecar 和外部 MongoDB 进行通信">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/egress-performance/./case1_sidecar_bypass3.png" alt="绕过 Sidecar 和外部 MongoDB 进行通信" />
&lt;/a>
&lt;/div>
&lt;figcaption>绕过 Sidecar 和外部 MongoDB 进行通信&lt;/figcaption>
&lt;/figure>
&lt;h3 id="-2-service-entry-sidecar-">案例 2：使用 Service Entry，通过 Sidecar 完成访问&lt;/h3>
&lt;p>在 Sidecar 已经注入到应用 Pod 之后，这种方式是缺省（访问外部服务）的方式。所有的流量都被 Sidecar 拦截，然后根据配置好的规则路由到目的地，这里所说的目的地也包含了外部服务。下面为 MongoDB 配置一个 &lt;code>ServiceEntry&lt;/code>。&lt;/p>
&lt;figure style="width:70%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:74.41253263707573%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/egress-performance/./case2_sidecar_passthru3.png" title="Sidecar 拦截对外部 MongoDB 的流量">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/egress-performance/./case2_sidecar_passthru3.png" alt="Sidecar 拦截对外部 MongoDB 的流量" />
&lt;/a>
&lt;/div>
&lt;figcaption>Sidecar 拦截对外部 MongoDB 的流量&lt;/figcaption>
&lt;/figure>
&lt;h3 id="-3-egress-gateway">案例 3: Egress gateway&lt;/h3>
&lt;p>配置 Egress gateway 以及配套的 Destination rule 和 Virtual service，用于访问 MongoDB。所有进出外部数据库的流量都从 Egress gateway（Envoy）通过。&lt;/p>
&lt;figure style="width:70%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:62.309368191721134%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/egress-performance/./case3_egressgw3.png" title="使用 Egress gateway 访问 MongoDB">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/egress-performance/./case3_egressgw3.png" alt="使用 Egress gateway 访问 MongoDB" />
&lt;/a>
&lt;/div>
&lt;figcaption>使用 Egress gateway 访问 MongoDB&lt;/figcaption>
&lt;/figure>
&lt;h3 id="-4-sidecar--egress-gateway--tls">案例 4：在 Sidecar 和 Egress gateway 之间的双向 TLS&lt;/h3>
&lt;p>这种方式中，在 Sidecar 和 Gateway 之中多出了一个安全层，所以会影响性能。&lt;/p>
&lt;figure style="width:70%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:63.968957871396896%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/egress-performance/./case4_egressgw_mtls3.png" title="在 Sidecar 和 Egress gateway 之间启用双向 TLS">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/egress-performance/./case4_egressgw_mtls3.png" alt="在 Sidecar 和 Egress gateway 之间启用双向 TLS" />
&lt;/a>
&lt;/div>
&lt;figcaption>在 Sidecar 和 Egress gateway 之间启用双向 TLS&lt;/figcaption>
&lt;/figure>
&lt;h3 id="-5-sni-proxy--egress-gateway">案例 5：带有 SNI proxy 的 Egress gateway&lt;/h3>
&lt;p>这个场景中，因为 Envoy 目前存在的一些限制，需要另一个代理来访问通配符域名。这里创建了一个 Nginx 代理，在 Egress gateway Pod 中作为 Sidecar 来使用。&lt;/p>
&lt;figure style="width:70%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:65.2762119503946%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/egress-performance/./case5_egressgw_sni_proxy3.png" title="带有 SNI proxy 的 Egress gateway">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/egress-performance/./case5_egressgw_sni_proxy3.png" alt="带有 SNI proxy 的 Egress gateway" />
&lt;/a>
&lt;/div>
&lt;figcaption>带有 SNI proxy 的 Egress gateway&lt;/figcaption>
&lt;/figure>
&lt;h2 id="heading">环境&lt;/h2>
&lt;ul>
&lt;li>Istio 版本： 1.0.2&lt;/li>
&lt;li>&lt;code>K8s&lt;/code> 版本：&lt;code>1.10.5_1517&lt;/code>&lt;/li>
&lt;li>Acmeair 应用：4 个服务（每个服务一个实例），跨服务事务，外部 MongoDB，平均载荷：620 字节。&lt;/li>
&lt;/ul>
&lt;h2 id="heading-1">结果&lt;/h2>
&lt;p>使用 &lt;code>Jmeter&lt;/code> 来生成负载，负载包含了一组持续五分钟的访问，每个阶段都会逐步提高客户端数量来发出 http 请求。客户端数量为：1、5、10、20、30、40、50 和 60。&lt;/p>
&lt;h3 id="heading-2">吞吐量&lt;/h3>
&lt;p>下图展示了不同案例中的吞吐量：&lt;/p>
&lt;figure style="width:75%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:54.29638854296388%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/egress-performance/./throughput3.png" title="不同案例中的吞吐量">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/egress-performance/./throughput3.png" alt="不同案例中的吞吐量" />
&lt;/a>
&lt;/div>
&lt;figcaption>不同案例中的吞吐量&lt;/figcaption>
&lt;/figure>
&lt;p>如图可见，在应用和外部数据库中加入 Sidecar 和 Egress gateway 并没有对性能产生太大影响；但是启用双向 TLS、又加入 SNI 代理之后，吞吐量分别下降了 10% 和 24%。&lt;/p>
&lt;h3 id="heading-3">响应时间&lt;/h3>
&lt;p>在 20 客户端的情况下，我们对不同请求的平均响应时间也进行了记录。下图展示了各个案例中平均、中位数、90%、95% 以及 99% 百分位的响应时间。&lt;/p>
&lt;figure style="width:75%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:48.76783398184176%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/egress-performance/./response_times3.png" title="不同配置中的响应时间">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/egress-performance/./response_times3.png" alt="不同配置中的响应时间" />
&lt;/a>
&lt;/div>
&lt;figcaption>不同配置中的响应时间&lt;/figcaption>
&lt;/figure>
&lt;p>跟吞吐量类似，前面三个案例的响应时间没有很大区别，但是双向 TLS 和 额外的代理造成了明显的延迟。&lt;/p>
&lt;h3 id="cpu-">CPU 用量&lt;/h3>
&lt;p>运行过程中还搜集了所有 Istio 组件以及 Sidecar 的 CPU 使用情况。为了公平起见，用吞吐量对 Istio 的 CPU 用量进行了归一化。下图中展示了这一结果：&lt;/p>
&lt;figure style="width:75%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:53.96174863387978%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2019/egress-performance/./cpu_usage3.png" title="使用 TPS 进行归一化的 CPU 用量">
&lt;img class="element-to-stretch" src="/v1.1/blog/2019/egress-performance/./cpu_usage3.png" alt="使用 TPS 进行归一化的 CPU 用量" />
&lt;/a>
&lt;/div>
&lt;figcaption>使用 TPS 进行归一化的 CPU 用量&lt;/figcaption>
&lt;/figure>
&lt;p>经过归一化处理之后的 CPU 用量数据表明，Istio 在使用 Egress gateway + SNI 代理的情况下，消耗了更多的 CPU。&lt;/p>
&lt;h2 id="heading-4">结论&lt;/h2>
&lt;p>在这一系列的测试之中，我们用不同的方式来访问一个启用了 TLS 的 MongoDB 来进行性能对比。Egress gateway 的引用没有对性能和 CPU 消耗的显著影响。但是启用了 Sidecar 和 Egress gateway 之间的双向 TLS 或者为通配符域名使用了额外的 SNI 代理之后，会看到性能降级的现象。&lt;/p></description><pubDate>Thu, 31 Jan 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/egress-performance/</link><author>Jose Nativio, IBM</author><guid isPermaLink="true">/v1.1/zh/blog/2019/egress-performance/</guid><category>performance</category><category>traffic-management</category><category>egress</category><category>mongo</category></item><item><title>使用 AppSwitch 进行 Sidestepping 依赖性排序</title><description>&lt;p>我们正在经历一个有趣事情，对应用程序进行拆分和重组。虽然微服务需要把单体应用分解为多个微型服务，但服务网格会把这些服务连结为一个应用程序。因此，微服务是逻辑上分离而又不是相互独立的。它们通常是紧密相互依赖的，而拆分单体应用的同时会引入了许多新的问题，例如服务之间需要双向认证等。而 Istio 恰巧能解决大多数问题。&lt;/p>
&lt;h2 id="heading">依赖性排序问题&lt;/h2>
&lt;p>依赖性排序的问题是由于应用程序拆分而导致的问题且 Istio 也尚未解决 - 确保应用程序整体快速正确地的顺序启动应用程序的各个服务。在单体应用程序中，内置所有组件，组件之间的依赖顺序由内部锁机制强制执行。但是，如果单个服务分散在服务网格的集群中，则启动服务需要首先检查它所依赖的服务是否已启动且可用。&lt;/p>
&lt;p>依赖性排序由于存在许多相互关联的问题而具有欺骗性。对单个服务进行排序需要具有服务的依赖关系图，以便它们可以从叶节点开始返回到根节点。由于相互依赖性随着应用程序的行为而发展，因此构建这样的图并随时保持更新并不容易。即使以某种方式提供依赖图，强制执行排序本身并不容易。简单地按指定的顺序启动服务显然是行不通的。服务可能已启动但尚未准备好提供服务。例如 docker-compose 中的 depends-on 标签就存在这样的问题。&lt;/p>
&lt;p>除了在服务启动之间引入足够长的睡眠之外，还有一个常见的模式是，在启动服务之前检查被依赖的服务是否已经准备就绪。在 Kubernetes 中，可以用在 Pod 的 Init 容器中加入等待脚本的方式来完成。但是，这意味着整个应用程序将被暂停，直到所有的依赖服务都准备就绪。有时，应用程序会在启动第一次出站连接之前花几分钟时间初始化自己。不允许服务启动会增加应用程序整体启动时间的大量开销。此外，等待 init 容器的策略不适用于同一 pod 中的多个服务相互依赖的情况。&lt;/p>
&lt;h3 id="ibm-websphere-nd">示例场景：IBM WebSphere ND&lt;/h3>
&lt;p>IBM WebSphere ND 是一个常见的应用程序中间件，通过对它的观察，能够更好地理解这种问题。它本身就是一个相当复杂的框架，由一个名为 Deployment manager（&lt;code>dmgr&lt;/code>）的中央组件组成，它管理一组节点实例。它使用 UDP 协商节点之间的集群成员资格，并要求部署管理器在任何节点实例出现并加入集群之前已启动并可运行。&lt;/p>
&lt;p>为什么我们在现代云原生环境中讨论传统应用程序？事实证明，通过使它们能够在 Kubernetes 和 Istio 平台上运行，可以获得显著的收益。从本质上讲，它是现代化之旅的一部分，它允许在同一现代平台上运行传统应用程序和全新的现代应用程序，以促进两者之间的互操作。实际上，WebSphere ND 是一个要求很高的应用程序。它期望具有特定网络接口属性的一致网络环境等。AppSwitch 可以满足这些要求。本博客的将重点关注依赖顺序需求以及 AppSwitch 在这方面的解决方法。&lt;/p>
&lt;p>在 Kubernetes 集群上简单地部署 &lt;code>dmgr&lt;/code> 和节点实例作为 pod 是行不通的。&lt;code>dmgr&lt;/code> 和节点实例碰巧有一个很长的初始化过程，可能需要几分钟。如果他们被同时部署，那么应用程序通常会处于一个有趣的状态。当一个节点实例出现并发现缺少 &lt;code>dmgr&lt;/code> 时，它将需要一个备用启动路径。相反，如果它立即退出，Kubernetes 崩溃循环将接管，也许应用程序会出现。但即使在这种情况下，事实证明及时启动并不能得到保证。&lt;/p>
&lt;p>一个 &lt;code>dmgr&lt;/code> 及其节点实例是 WebSphere ND 的基本部署配置。构建在生产环境中运行的 WebSphere ND 之上的 IBM Business Process Manager 等应用程序包括其他一些服务。在这些配置中，可能存在一系列相互依赖关系。根据节点实例托管的应用程序，它们之间也可能存在排序要求。使用较长的服务初始化时间和崩溃循环重启，应用程序几乎没有机会在任何合理的时间内启动。&lt;/p>
&lt;h3 id="istio--sidecar-">Istio 中的 Sidecar 依赖&lt;/h3>
&lt;p>Istio 本身受依赖性排序问题版本的影响。由于在 Istio 下运行的服务的连接通过其 sidecar 代理重定向，因此在应用程序服务及其 sidecar 之间创建了隐式依赖关系。除非 sidecar 完全正常运行，否则所有来自服务的请求都将被丢弃。&lt;/p>
&lt;h2 id="-appswitch-">使用 AppSwitch 进行依赖性排序&lt;/h2>
&lt;p>那么我们如何解决这些问题呢？一种方法是将其推迟到应用程序并说它们应该“表现良好”并实施适当的逻辑以使自己免受启动顺序问题的影响。但是，许多应用程序（尤其是传统应用程序）如果错误则会超时或死锁。即使对于新的应用程序，为每个服务实现一个关闭逻辑也是最大的额外负担，最好避免。服务网格需要围绕这些问题提供足够的支持。毕竟，将常见模式分解为底层框架实际上是服务网格的重点。&lt;/p>
&lt;p>&lt;a href="http://appswitch.io">AppSwitch&lt;/a> 明确地解决了依赖性排序。它位于应用程序在集群中的客户端和服务之间的网络交互的控制路径上，并且通过进行 &lt;code>connect&lt;/code> 调用以及当特定服务通过使 &lt;code>listen&lt;/code> 准备好接受连接时，准确地知道服务何时成为客户端。调用它的 &lt;em>service router&lt;/em> 组件在集群中传播有关这些事件的信息，并仲裁客户端和服务器之间的交互。AppSwitch 就是以这种简单有效的方式实现负载均衡和隔离等功能的。利用应用程序的网络控制路径的相同战略位置，可以想象这些服务所做的“连接”和“监听”调用可以以更精细的粒度排列，而不是按照依赖关系图对整个服务进行粗略排序。这将有效地解决多级依赖问题和加速应用程序启动。&lt;/p>
&lt;p>但这仍然需要一个依赖图。存在许多产品和工具来帮助发现服务依赖性。但它们通常基于对网络流量的被动监控，并且无法预先为任意应用程序提供信息。由于加密和隧道导致的网络级混淆也使它们不可靠。发现和指定依赖项的负担最终落在应用程序的开发人员或操作员身上。实际上，甚至一致性检查依赖性规范本身也非常复杂，相对来说，能够避免使用依赖图的任何方法都会更加理想。&lt;/p>
&lt;p>依赖图的要点是知道哪些客户端依赖于特定服务，以便让客户端能够等待被依赖服务准备就绪。但具体客户真的重要吗？归根结底，一个服务的所有客户端，都是依赖这个服务的。AppSwitch 正是利用这一点来解决依赖问题。事实上，这完全避免了依赖性排序。可以同时调度应用程序中的所有服务，而无需考虑启动顺序。它们之间的相互依赖性会根据各个请求和响应的粒度自动完成，从而实现快速，正确的应用程序启动。&lt;/p>
&lt;h3 id="appswitch-">AppSwitch 模型和构造&lt;/h3>
&lt;p>既然我们对 AppSwitch 的高级方法有了概念性的理解，那么让我们来看看所涉及的结构。但首先要对使用模型进行快速总结。尽管它是针对不同的上下文编写的，但在此主题上查看我之前的&lt;a href="/v1.1/zh/blog/2018/delayering-istio/">blog&lt;/a>也很有用。为了完整起见，我还要注意 AppSwitch 不会打扰非网络依赖。例如，两个服务可能使用 IPC 机制或通过共享文件系统进行交互。像这样的深层联系的流程通常是同一服务的一部分，并且不需要主动地对应用程序的正常执行进行干预。&lt;/p>
&lt;p>AppSwitch 的核心能够使用 BSD Socket API 及其相关的其它调用（例如 &lt;code>fcntl&lt;/code> 和 ioctl）来完成对 Socket 的处理。它的实现细节很有意思，但是为了防止偏离本文的主题，这里仅对其独特的关键属性进行一个总结。
（1）速度很快。它使用 &lt;code>seccomp&lt;/code> 过滤和二进制检测的组合来积极地限制应用程序正常执行的干预。AppSwitch 特别适用于服务网格和应用程序网络用例，因为它实现了这些功能，而无需实际触摸数据。相反，网络级方法会导致每个数据包的成本。看看这个&lt;a href="/v1.1/zh/blog/2018/delayering-istio/">博客&lt;/a>进行一些性能测量。
（2）它不需要任何内核支持，内核模块或补丁，可以在标准的发行版内核上运行
（3）它可以作为普通用户运行（非 root）。事实上，该机制甚至可以通过删除对网络容器的根要求来运行&lt;a href="https://linuxpiter.com/en/materials/2478">非 root 的 Docker 守护进程&lt;/a>
（4）它可以不加更改的用于任何类型的应用程序上，适用于任何类型的应用程序 - 从 WebSphere ND 和 SAP 到自定义 C 应用程序，再到静态链接的 Golang 应用程序。Linux/x86 是仅有的运行需求。&lt;/p>
&lt;h3 id="heading-1">将服务与其引用分离&lt;/h3>
&lt;p>AppSwitch 建立在应用程序应与其引用分离的基本前提之上。传统上，应用程序的标识源自它们运行的​​主机的标识。但是，应用程序和主机是需要独立引用的非常不同的对象。本&lt;a href="http://cn.arxiv.org/abs/1711.02294">主题&lt;/a>介绍了围绕此主题的详细讨论以及 AppSwitch 的概念基础。&lt;/p>
&lt;p>实现服务对象及其身份之间解耦的中央 AppSwitch 构造是 &lt;em>service reference&lt;/em>（简称 &lt;em>reference&lt;/em> ）。AppSwitch 基于上面概述的 API 检测机制实现服务引用。服务引用由 IP：端口对（以及可选的 DNS 名称）和标签选择器组成，标签选择器选择引用所代表的服务以及此引用所适用的客户端。引用支持一些关键属性。（1）它的名称可以独立于它所引用的对象的名称。也就是说，服务可能正在侦听 IP 和端口，但是引用允许在用户选择的任何其他 IP 和端口上达到该服务。这使 AppSwitch 能够运行从源环境中捕获的传统应用程序，通过静态 IP 配置在 Kubernetes 上运行，为其提供必要的 IP 地址和端口，而不管目标网络环境如何。（2）即使目标服务的位置发生变化，它也保持不变。引用自动重定向自身，因为其标签选择器现在解析为新的服务实例（3）对于此讨论最重要的是，在目标服务的启动过程中，引用就已经生效了。&lt;/p>
&lt;p>为了便于发现可通过服务引用访问的服务，AppSwitch 提供了一个 &lt;em>auto-curated 服务注册表&lt;/em>。根据 AppSwitch 跟踪的网络 API，当服务进出集群时，注册表会自动保持最新。注册表中的每个条目都包含相应服务绑定的 IP 和端口。除此之外，它还包括一组标签，指示此服务所属的应用程序，应用程序在创建服务时通过 Socket API 传递的 IP 和端口，AppSwitch 实际绑定基础主机上的服务的 IP 和端口此外，在 AppSwitch 下创建的应用程序带有一组用户传递的标签，用于描述应用程序以及一些默认系统标签，指示创建应用程序的用户和运行应用程序的主机等。这些标签都可以在服务引用所携带的标签选择器中表示。通过创建服务引用，可以使客户端访问注册表中的服务。然后，客户端将能够以引用的名称（IP：端口）访问服务。现在让我们来看看 AppSwitch 如何在目标服务尚在启动的过程中就让服务引用开始生效的。&lt;/p>
&lt;h3 id="heading-2">非阻塞请求&lt;/h3>
&lt;p>AppSwitch 利用 BSD Socket API 的语义，确保服务引用从客户的角度看起来是有效的，因为相应的服务出现了。当客户端对一个尚未启动的服务发起阻塞式连接调用时，AppSwitch 会阻止该调用一段时间等待目标服务变为活动状态。由于已知目标服务是应用程序的一部分并且预计很快就会出现，因此客户端会被阻塞，而不是收到 ECONNREFUSED 之类的返回信息导致启动失败。如果服务没有及时出现，则会向应用程序返回一个错误，以便像 Kubernetes 崩溃循环这样的框架级机制可以启动。&lt;/p>
&lt;p>如果客户端请求被标记为非阻塞，则 AppSwitch 通过返回 &lt;code>EAGAIN&lt;/code> 来处理该请求以通知应用程序重试而不是放弃。再次，这与 Socket API 的语义一致，并防止由于启动竞争而导致的失败。AppSwitch 通过对 BSD Socket API 的支持，将重试逻辑内置到应用程序之中，从而透明的为应用提供了依赖排序支持。&lt;/p>
&lt;h3 id="heading-3">应用程序超时&lt;/h3>
&lt;p>如果应用程序基于其自己的内部计时器超时怎么办？说实话，如果需要，AppSwitch 还可以伪造应用程序对时间的感知，但这种做法不仅越界，而且并无必要。应用程序决定并知道它应该等待多长时间，这对 AppSwitch 来说是不合适的。应用程序超过保守时长，如果目标服务仍未及时出现，则不太可能是依赖性排序问题。一定是出现了其它问题，AppSwitch 不应掩盖这些问题。&lt;/p>
&lt;h3 id="-sidecar-">为 Sidecar 提供服务引用的通配符支持&lt;/h3>
&lt;p>服务引用可用于解决前面提到的 Istio sidecar 依赖性问题。AppSwitch 用 IP：端口的方式来描述对服务的引用，这种描述中是可以使用通配符的。也就是说，服务引用描述中可以用IP 掩码的形式来表达要捕捉的 IP 地址的范围。如果服务引用的标签选择器指向 sidecar 服务，则应用此服务引用的任何应用程序的所有传出连接将被透明地重定向到 sidecar。当然，在 Sidecar 启动过程中，服务引用仍然是有效的。&lt;/p>
&lt;p>使用 sidecar 依赖性排序的服务引用也隐式地将应用程序的连接重定向到 sidecar ，而不需要 iptables 和随之而来的权限问题。基本上它就像应用程序直接连接到 sidecar 而不是目标目的地一样工作，让 sidecar 负责做什么。AppSwitch 将使用 sidecar 可以在将连接传递到应用程序之前解码的代理协议将关于原始目的地等的元数据插入到连接的数据流中。其中一些细节已在&lt;a href="/v1.1/zh/blog/2018/delayering-istio/">此处&lt;/a>进行了讨论。出站连接是这样处理的，那么入站连接呢？由于所有服务及其 sidecar 都在 AppSwitch 下运行，因此来自远程节点的任何传入连接都将被重定向到各自的远程 sidecar 。所以传入连接没有什么特别处理。&lt;/p>
&lt;h2 id="heading-4">总结&lt;/h2>
&lt;p>依赖顺序是一个讨厌的问题。这主要是由于无法访问有关服务间交互的细粒度应用程序级事件。解决这个问题通常需要应用程序来实现自己的内部逻辑。但 AppSwitch 使这些内部应用程序事件无需更改应用程序即可进行检测。然后，AppSwitch 利用对 BSD Socket API 的普遍支持来回避排序依赖关系的要求。&lt;/p>
&lt;h2 id="heading-5">致谢&lt;/h2>
&lt;p>感谢 Eric Herness 和团队对 IBM WebSphere 和 BPM 产品的见解和支持，我们将它们应用到现代化 Kubernetes 平台，还要感谢 Mandar Jog，Martin Taillefer 和 Shriram Rajagopalan 对于此博客早期草稿的评审。&lt;/p></description><pubDate>Mon, 14 Jan 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/appswitch/</link><author>Dinesh Subhraveti (AppOrbit and Columbia University)</author><guid isPermaLink="true">/v1.1/zh/blog/2019/appswitch/</guid><category>appswitch</category><category>performance</category></item><item><title>宣布 discuss.istio.io</title><description>&lt;p>Istio 社区一直致力于为用户寻找与社区成员互动的最佳媒介——提出问题、互帮互助、与项目开发人员互动等。&lt;/p>
&lt;p>我们尝试了几种方式，但每种方式都有些缺点。我们最近尝试的是 RocketChat，但它缺乏某些功能（例如分线程），这意味着它不适合针对单个问题做长时间的讨论。这还导致一些用户陷入两难境地——到底什么时候发邮件到 &lt;a href="mailto:istio-users@googlegroups.com">istio-users@googlegroups.com&lt;/a>，什么时候使用 RocketChat？&lt;/p>
&lt;p>我们高兴地宣布 &lt;a href="https://discuss.istio.io">discuss.istio.io&lt;/a>，我们认为在这个单一平台上找到了很好的功能平衡。这是一个功能齐全的论坛，我们将从这里开始讨论 Istio。它将允许您提出问题并获得分线程回复！更棒的是您可以使用 GitHub 身份登录。&lt;/p>
&lt;p>如果您更喜欢电子邮件，则可以将其配置为像 Google group 一样发送电子邮件。&lt;/p>
&lt;p>我们会将 Google group 标记为 “read only”，以便保留内容，但我们会要求您将更多问题发送至 &lt;a href="https://discuss.istio.io">discuss.istio.io&lt;/a>。如果您在小组中有任何未解决的问题或讨论，请转移到讨论板。&lt;/p>
&lt;p>Happy meshing!&lt;/p></description><pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/announcing-discuss.istio.io/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2019/announcing-discuss.istio.io/</guid></item><item><title>使用 Cert-Manager 部署一个自定义 Ingress 网关</title><description>&lt;p>本文介绍了手工创建自定义 Ingress &lt;a href="/v1.1/docs/reference/config/networking/v1alpha3/gateway/">Gateway&lt;/a> 的过程，其中使用 cert-manager 完成了证书的自动管理。&lt;/p>
&lt;p>自定义 Ingress 网关在使用不同负载均衡器来隔离通信的情况下很有帮助。&lt;/p>
&lt;h2 id="-before-you-begin">开始之前 {#before you begin}&lt;/h2>
&lt;ul>
&lt;li>根据&lt;a href="/v1.1/zh/docs/setup/">安装指南&lt;/a>完成 Istio 的部署。&lt;/li>
&lt;li>用 Helm &lt;a href="https://github.com/helm/charts/tree/master/stable/cert-manager#installing-the-chart">Chart&lt;/a> 部署 &lt;code>cert-manager&lt;/code>。&lt;/li>
&lt;li>我们会使用 &lt;code>demo.mydemo.com&lt;/code> 进行演示，因此你的 DNS 解析要能够解析这个域名。&lt;/li>
&lt;/ul>
&lt;h2 id="configuring-the-custom-ingress-gateway">配置自定义 Ingress 网关&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>用下面的 &lt;code>helm&lt;/code> 命令检查 &lt;a href="https://github.com/helm/charts/tree/master/stable/cert-manager">cert-manager&lt;/a> 是否已经完成部署：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ helm ls
&lt;/code>&lt;/pre>
&lt;p>该命令的输出大概如下所示，其中的 &lt;code>cert-manager&lt;/code> 的 &lt;code>STATUS&lt;/code> 字段应该是 &lt;code>DEPLOYED&lt;/code>&lt;/p>
&lt;pre>&lt;code class='language-plain' data-expandlinks='true' >NAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE
istio 1 Thu Oct 11 13:34:24 2018 DEPLOYED istio-1.0.X 1.0.X istio-system
cert 1 Wed Oct 24 14:08:36 2018 DEPLOYED cert-manager-v0.6.0-dev.2 v0.6.0-dev.2 istio-system
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>要创建集群的证书签发者，可以使用如下的配置：&lt;/p>
&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">用自己的配置修改集群的&lt;a href="https://cert-manager.readthedocs.io/en/latest/reference/issuers.html#issuers">证书签发者&lt;/a>。例子中使用的是 &lt;code>route53&lt;/code>。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: certmanager.k8s.io/v1alpha1
kind: ClusterIssuer
metadata:
name: letsencrypt-demo
namespace: kube-system
spec:
acme:
# ACME 服务器地址
server: https://acme-v02.api.letsencrypt.org/directory
# ACME 注册的 Email 地址
email: &amp;lt;REDACTED&amp;gt;
# Secret 的名字，用于保存 ACME 账号的私钥
privateKeySecretRef:
name: letsencrypt-demo
dns01:
# 这里定义了一个列表，包含了 DNS-01 的相关内容，用于应对 DNS Challenge。
providers:
- name: your-dns
route53:
accessKeyID: &amp;lt;REDACTED&amp;gt;
region: eu-central-1
secretAccessKeySecretRef:
name: prod-route53-credentials-secret
key: secret-access-key
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>如果使用的是 &lt;code>route53&lt;/code> &lt;a href="https://cert-manager.readthedocs.io/en/latest/tasks/acme/configuring-dns01/route53.html">provider&lt;/a>，必须提供一个 Secret 来进行 DNS 的 ACME 验证。可以使用下面的配置来创建需要的 Secret：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: v1
kind: Secret
metadata:
name: prod-route53-credentials-secret
type: Opaque
data:
secret-access-key: &amp;lt;REDACTED BASE64&amp;gt;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>创建自己的证书：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: certmanager.k8s.io/v1alpha1
kind: Certificate
metadata:
name: demo-certificate
namespace: istio-system
spec:
acme:
config:
- dns01:
provider: your-dns
domains:
- &amp;#39;*.mydemo.com&amp;#39;
commonName: &amp;#39;*.mydemo.com&amp;#39;
dnsNames:
- &amp;#39;*.mydemo.com&amp;#39;
issuerRef:
kind: ClusterIssuer
name: letsencrypt-demo
secretName: istio-customingressgateway-certs
&lt;/code>&lt;/pre>
&lt;p>记录一下 &lt;code>secretName&lt;/code> 的值，后面会使用它。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>要进行自动扩容，可以新建一个 HPA 对象：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
name: my-ingressgateway
namespace: istio-system
spec:
maxReplicas: 5
minReplicas: 1
scaleTargetRef:
apiVersion: apps/v1beta1
kind: Deployment
name: my-ingressgateway
targetCPUUtilizationPercentage: 80
status:
currentCPUUtilizationPercentage: 0
currentReplicas: 1
desiredReplicas: 1
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>使用&lt;a href="/v1.1/blog/2019/custom-ingress-gateway/deployment-custom-ingress.yaml">附件 YAML 中的定义&lt;/a>进行部署。&lt;/p>
&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">其中类似 &lt;code>aws-load-balancer-type&lt;/code> 这样的注解，只对 AWS 生效。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>创建你的服务：&lt;/p>
&lt;div>
&lt;aside class="callout warning">
&lt;div class="type">
&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-warning"/>&lt;/svg>
&lt;/div>
&lt;div class="content">&lt;code>NodePort&lt;/code> 需要是一个可用端口。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: v1
kind: Service
metadata:
name: my-ingressgateway
annotations:
service.beta.kubernetes.io/aws-load-balancer-type: nlb
labels:
app: my-ingressgateway
istio: my-ingressgateway
spec:
type: LoadBalancer
selector:
app: my-ingressgateway
istio: my-ingressgateway
ports:
-
name: http2
nodePort: 32380
port: 80
targetPort: 80
-
name: https
nodePort: 32390
port: 443
-
name: tcp
nodePort: 32400
port: 31400
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>创建你的自定义 Ingress 网关配置对象：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
annotations:
name: istio-custom-gateway
namespace: default
spec:
selector:
istio: my-ingressgateway
servers:
- hosts:
- &amp;#39;*.mydemo.com&amp;#39;
port:
name: http
number: 80
protocol: HTTP
tls:
httpsRedirect: true
- hosts:
- &amp;#39;*.mydemo.com&amp;#39;
port:
name: https
number: 443
protocol: HTTPS
tls:
mode: SIMPLE
privateKey: /etc/istio/ingressgateway-certs/tls.key
serverCertificate: /etc/istio/ingressgateway-certs/tls.crt
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>使用 &lt;code>VirtualService&lt;/code> 连接 &lt;code>istio-custom-gateway&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: my-virtualservice
spec:
hosts:
- &amp;#34;demo.mydemo.com&amp;#34;
gateways:
- istio-custom-gateway
http:
- route:
- destination:
host: my-demoapp
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>服务器返回了正确的证书，并成功完成验证（&lt;code>SSL certificate verify ok&lt;/code>）：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ curl -v `https://demo.mydemo.com`
Server certificate:
SSL certificate verify ok.
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>恭喜你！&lt;/strong> 现在你可以使用自定义的 &lt;code>istio-custom-gateway&lt;/code> &lt;a href="/v1.1/docs/reference/config/networking/v1alpha3/gateway/">网关&lt;/a>对象了。&lt;/p></description><pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2019/custom-ingress-gateway/</link><author>Julien Senon</author><guid isPermaLink="true">/v1.1/zh/blog/2019/custom-ingress-gateway/</guid><category>ingress</category><category>traffic-management</category></item><item><title>Istio 1.0.5 发布</title><description>&lt;p>在此高兴地宣布：Istio 1.0.5 已经发布。请查看&lt;a href="/v1.1/zh/about/notes/1.0.5/">发行说明&lt;/a> 来了解和下载新版本。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.0.5"
data-updateadvice='Before you download 1.0.5, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.0.8'
data-updatehref="/v1.1/zh/about/notes/1.0.8">
1.0.5 下载
&lt;/button>
&lt;a class="btn" href="https://archive.istio.io/v1.0">1.0.5 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.0.4...1.0.5">1.0.5 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/announcing-1.0.5/</link><author>Istio 团队</author><guid isPermaLink="true">/v1.1/zh/blog/2018/announcing-1.0.5/</guid></item><item><title>增量式应用 Istio 第一部分，流量管理</title><description>&lt;p>流量管理是 Istio 提供的重要优势之一。Istio 流量管理的核心是在将通信流量和基础设施的伸缩进行解耦。如果没有 Istio 这样的服务网格，这种流量控制方式是不可能实现的。&lt;/p>
&lt;p>例如，您希望执行一次&lt;a href="https://martinfowler.com/bliki/CanaryRelease.html">金丝雀发布&lt;/a>。当使用 Istio 时，您可以指定 service 的 &lt;strong>v1&lt;/strong> 版本接收 90% 的传入流量，而该 service &lt;strong>v2&lt;/strong> 版本仅接收 10%。如果使用标准的 Kubernetes deployment，实现此目的的唯一方法是手动控制每个版本的可用 Pod 数量，例如使 9 个 Pod 运行 v1 版本，使 1 个 Pod 运行 v2 版本。这种类型的手动控制难以实现，并且随着时间的推移可能无法扩展。有关更多信息，请查看&lt;a href="/v1.1/zh/blog/2017/0.1-canary/">使用 Istio 进行金丝雀发布&lt;/a>。&lt;/p>
&lt;p>部署现有 service 的更新时存在同样的问题。虽然您可以使用 Kubernetes 更新 deployment，但它需要将 v1 Pod 替换为 v2 Pod。使用 Istio，您可以部署 service 的 v2 版本，并使用内置流量管理机制在网络层面将流量转移到更新后的 service，然后删除 v1 版本的 Pod。&lt;/p>
&lt;p>除了金丝雀发布和一般流量转移之外，Istio 还使您能够实现动态请求路由（基于 HTTP header）、故障恢复、重试、断路器和故障注入。有关更多信息，请查看&lt;a href="/v1.1/zh/docs/concepts/traffic-management/">流量管理文档&lt;/a>。&lt;/p>
&lt;p>这篇文章介绍的技术重点突出了一种特别有用的方法，可以逐步实现 Istio（在这种情况下，只有流量管理功能），而无需单独更新每个 Pod。&lt;/p>
&lt;h2 id="-istio-">设置：为什么要实施 Istio 流量管理功能？&lt;/h2>
&lt;p>当然，第一个问题是：为什么要这样做？&lt;/p>
&lt;p>如果你是众多拥有大量团队和大型集群的组织中的一员，那么答案是很清楚的。假设 A 团队正在开始使用 Istio，并希望在 service A 上开始一些金丝雀发布，但是 B 团队还没有开始使用 Istio，所以他们没有部署 sidecar。&lt;/p>
&lt;p>使用 Istio，A 团队仍然可以让 service B 通过 Istio 的 ingress gateway 调用 service A 来实现他们的金丝雀发布。&lt;/p>
&lt;h2 id="istio-">背景：Istio 网格中的流量路由&lt;/h2>
&lt;p>但是，如何在不更新每个应用程序的 Pod 的情况下，使用 Istio 的流量管理功能来包含 Istio sidecar？在回答这个问题之前，让我们以高层视角，快速地看看流量如何进入 Istio 网格以及如何被路由。&lt;/p>
&lt;p>Pod 包含一个 sidecar 代理，该代理作为 Istio 网格的一部分，负责协调 Pod 的所有入站和出站流量。在 Istio 网格中，Pilot 负责将高级路由规则转换为配置并将它们传播到 sidecar 代理。这意味着当服务彼此通信时，它们的路由决策是由客户端确定的。&lt;/p>
&lt;p>假设您有 service A 和 service B 两个服务，他们是 Istio 网格的一部分。当 A 想要与 B 通信时，Pod A 的 sidecar 代理负责将流量引导到 service B。例如，如果你希望到 service B v1 版本和 v2 版本之间的流量按 50/50 分割，流量将按如下方式流动：&lt;/p>
&lt;figure style="width:60%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:42.66666666666667%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/incremental-traffic-management/./fifty-fifty.png" title="50/50 流量分割">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/incremental-traffic-management/./fifty-fifty.png" alt="50/50 流量分割" />
&lt;/a>
&lt;/div>
&lt;figcaption>50/50 流量分割&lt;/figcaption>
&lt;/figure>
&lt;p>如果 service A 和 B 不是 Istio 网格的一部分，则没有 sidecar 代理知道如何将流量路由到 service B 的不同版本。在这种情况下，您需要使用另一种方法来使 service A 到 service B 的流量遵循您设置的 50/50 规则。&lt;/p>
&lt;p>幸运的是，标准的 Istio 部署已经包含了一个 &lt;a href="/v1.1/zh/docs/concepts/traffic-management/#gateway">Gateway&lt;/a>，它专门处理 Istio 网格之外的入口流量。此 Gateway 用于允许通过外部负载均衡器进入的集群外部入口流量；或来自 Kubernetes 集群，但在服务网格之外的入口流量。网关可以进行配置，对没有 Sidecar 支持的入口流量进行代理，引导流量进入相应的 Pod。这种方法允许您利用 Istio 的流量管理功能，其代价是通过入口网关的流量将产生额外的跃点。&lt;/p>
&lt;figure style="width:60%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:54.83870967741935%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/incremental-traffic-management/./fifty-fifty-ingress-gateway.png" title="使用 Ingress Gateway 的 50/50 流量分割">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/incremental-traffic-management/./fifty-fifty-ingress-gateway.png" alt="使用 Ingress Gateway 的 50/50 流量分割" />
&lt;/a>
&lt;/div>
&lt;figcaption>使用 Ingress Gateway 的 50/50 流量分割&lt;/figcaption>
&lt;/figure>
&lt;h2 id="istio--1">实践：Istio 流量路由&lt;/h2>
&lt;p>一种实践的简单方法是首先按照&lt;a href="/v1.1/zh/docs/setup/kubernetes/prepare/platform-setup/">平台设置&lt;/a>说明设置 Kubernetes 环境，然后使用 &lt;a href="/v1.1/zh/docs/setup/kubernetes/install/helm/">Helm&lt;/a> 安装仅包含流量管理组件（ingress gateway、egress gateway、Pilot）的 Istio。下面的示例使用 &lt;a href="https://cloud.google.com/gke">Google Kubernetes Engine&lt;/a>。&lt;/p>
&lt;p>首先，&lt;strong>安装并配置 &lt;a href="/v1.1/zh/docs/setup/kubernetes/prepare/platform-setup/gke/">GKE&lt;/a>&lt;/strong>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ gcloud container clusters create istio-inc --zone us-central1-f
$ gcloud container clusters get-credentials istio-inc
$ kubectl create clusterrolebinding cluster-admin-binding \
--clusterrole=cluster-admin \
--user=$(gcloud config get-value core/account)
&lt;/code>&lt;/pre>
&lt;p>然后，&lt;strong>&lt;a href="https://helm.sh/docs/securing_installation/">安装 Helm&lt;/a> 并&lt;a href="/v1.1/zh/docs/setup/kubernetes/install/helm/">生成 Istio 最小配置安装&lt;/a>&lt;/strong> &amp;ndash; 只有流量管理组件：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ helm template install/kubernetes/helm/istio \
--name istio \
--namespace istio-system \
--set security.enabled=false \
--set galley.enabled=false \
--set sidecarInjectorWebhook.enabled=false \
--set mixer.enabled=false \
--set prometheus.enabled=false \
--set pilot.sidecar=false &amp;gt; istio-minimal.yaml
&lt;/code>&lt;/pre>
&lt;p>然后&lt;strong>创建 &lt;code>istio-system&lt;/code> namespace 并部署 Istio&lt;/strong>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl create namespace istio-system
$ kubectl apply -f istio-minimal.yaml
&lt;/code>&lt;/pre>
&lt;p>然后，在没有 Istio sidecar 容器的前提下&lt;strong>部署 Bookinfo 示例&lt;/strong>：&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/bookinfo/platform/kube/bookinfo.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;p>现在，&lt;strong>配置一个新的 Gateway&lt;/strong> 允许从 Istio 网格外部访问 reviews service；一个新的 &lt;code>VirtualService&lt;/code> 用于平均分配到 reviews service v1 和 v2 版本的流量；以及一系列新的、将目标子集与服务版本相匹配的 &lt;code>DestinationRule&lt;/code> 资源：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
name: reviews-gateway
spec:
selector:
istio: ingressgateway # 使用 istio 默认控制器
servers:
- port:
number: 80
name: http
protocol: HTTP
hosts:
- &amp;#34;*&amp;#34;
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: reviews
spec:
hosts:
- &amp;#34;*&amp;#34;
gateways:
- reviews-gateway
http:
- match:
- uri:
prefix: /reviews
route:
- destination:
host: reviews
subset: v1
weight: 50
- destination:
host: reviews
subset: v2
weight: 50
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: reviews
spec:
host: reviews
subsets:
- name: v1
labels:
version: v1
- name: v2
labels:
version: v2
- name: v3
labels:
version: v3
EOF
&lt;/code>&lt;/pre>
&lt;p>最后，使用 &lt;code>curl&lt;/code> &lt;strong>部署一个用于测试的 Pod&lt;/strong>（没有 Istio sidecar 容器）：&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/sleep/sleep.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/sleep/sleep.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;h2 id="heading">测试您的部署&lt;/h2>
&lt;p>现在就可以通过 Sleep pod 使用 curl 命令来测试不同的行为了。&lt;/p>
&lt;p>第一个示例是使用标准 Kubernetes service DNS 行为向 reviews service 发出请求（&lt;strong>注意&lt;/strong>：下面的示例中使用了 &lt;a href="https://stedolan.github.io/jq/">&lt;code>jq&lt;/code>&lt;/a> 来过滤 &lt;code>curl&lt;/code> 的输出）：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export SLEEP_POD=$(kubectl get pod -l app=sleep \
-o jsonpath={.items..metadata.name})
$ for i in `seq 3`; do \
kubectl exec -it $SLEEP_POD curl http://reviews:9080/reviews/0 | \
jq &amp;#39;.reviews|.[]|.rating?&amp;#39;; \
done
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class='language-json' data-expandlinks='true' >{
&amp;#34;stars&amp;#34;: 5,
&amp;#34;color&amp;#34;: &amp;#34;black&amp;#34;
}
{
&amp;#34;stars&amp;#34;: 4,
&amp;#34;color&amp;#34;: &amp;#34;black&amp;#34;
}
null
null
{
&amp;#34;stars&amp;#34;: 5,
&amp;#34;color&amp;#34;: &amp;#34;red&amp;#34;
}
{
&amp;#34;stars&amp;#34;: 4,
&amp;#34;color&amp;#34;: &amp;#34;red&amp;#34;
}
&lt;/code>&lt;/pre>
&lt;p>请注意我们是如何从 reviews service 的所有三个版本获得响应（&lt;code>null&lt;/code> 来自 reviews v1 版本，它没有评级数据）并且流量没有在 v1 和 v2 版本间平均拆分。这是预期的行为，因为 &lt;code>curl&lt;/code> 命令在 reviews service 所有三个版本之间进行 Kubernetes service 负载均衡。为了以 50/50 的流量拆分形式访问 reviews，我们需要通过 ingress Gateway 访问 service：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ for i in `seq 4`; do \
kubectl exec -it $SLEEP_POD curl http://istio-ingressgateway.istio-system/reviews/0 | \
jq &amp;#39;.reviews|.[]|.rating?&amp;#39;; \
done
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class='language-json' data-expandlinks='true' >{
&amp;#34;stars&amp;#34;: 5,
&amp;#34;color&amp;#34;: &amp;#34;black&amp;#34;
}
{
&amp;#34;stars&amp;#34;: 4,
&amp;#34;color&amp;#34;: &amp;#34;black&amp;#34;
}
null
null
{
&amp;#34;stars&amp;#34;: 5,
&amp;#34;color&amp;#34;: &amp;#34;black&amp;#34;
}
{
&amp;#34;stars&amp;#34;: 4,
&amp;#34;color&amp;#34;: &amp;#34;black&amp;#34;
}
null
null
&lt;/code>&lt;/pre>
&lt;p>任务完成！这篇文章展示了如何部署仅包含流量管理组件（Pilot、ingress Gateway）的 Istio 的最小安装，然后使用这些组件将流量定向到特定版本的 reviews service。由于不是必须通过部署 Istio sidecar 代理来获得这些功能，因此几乎没有给现有工作负载或应用程序造成中断。&lt;/p>
&lt;p>这篇文章展示了如何利用 Istio 及内置的 ingress Gateway（以及一些 &lt;code>VirtualService&lt;/code> 和 &lt;code>DestinationRule&lt;/code> 资源），轻松实现集群外部入口流量和集群内部服务到服务的流量管理。这种技术是增量式应用 Istio 的一个很好的例子，在 Pod 由不同团队拥有，或部署到不同命名空间的现实案例中尤其有用。&lt;/p></description><pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/incremental-traffic-management/</link><author>Sandeep Parikh</author><guid isPermaLink="true">/v1.1/zh/blog/2018/incremental-traffic-management/</guid><category>traffic-management</category><category>gateway</category></item><item><title>Istio 1.0.4 发布</title><description>&lt;p>在此高兴地宣布：Istio 1.0.4 已经发布。请查看&lt;a href="/v1.1/zh/about/notes/1.0.4/">发行说明&lt;/a> 来了解和下载新版本。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.0.4"
data-updateadvice='Before you download 1.0.4, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.0.8'
data-updatehref="/v1.1/zh/about/notes/1.0.8">
1.0.4 下载
&lt;/button>
&lt;a class="btn" href="https://archive.istio.io/v1.0">1.0.4 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.0.3...1.0.4">1.0.4 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Wed, 21 Nov 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/announcing-1.0.4/</link><author>Istio 团队</author><guid isPermaLink="true">/v1.1/zh/blog/2018/announcing-1.0.4/</guid></item><item><title>使用外部 MongoDB 服务</title><description>&lt;p>在&lt;a href="/v1.1/zh/blog/2018/egress-tcp/">使用外部 TCP 服务&lt;/a>博文中，我描述了网格内的 Istio 应用程序如何通过 TCP 使用外部服务。在本文中，我将演示如何使用外部 MongoDB
服务。您将使用 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Istio Bookinfo 示例应用程序&lt;/a>，它的书籍评级数据保存在 MongoDB 数据库中。您会将此数据库部署在集群外部，并配置 &lt;code>ratings&lt;/code>
微服务使用它。您将学习控制到外部 MongoDB 服务流量的多种选择及其利弊。&lt;/p>
&lt;h2 id="-ratings--bookinfo">使用外部 ratings 数据库的 Bookinfo&lt;/h2>
&lt;p>首先，在您的 Kubernetes 集群外部建立一个 MongoDB 数据库实例以保存书籍评级数据。然后修改 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Bookinfo 示例应用程序&lt;/a>使用该数据库。&lt;/p>
&lt;h3 id="-ratings-">建立 ratings 数据库&lt;/h3>
&lt;p>在这个任务中您将建立一个 &lt;a href="https://www.mongodb.com">MongoDB&lt;/a> 实例。您可以使用任何 MongoDB 实例；我使用 &lt;a href="https://www.ibm.com/cloud/compose/mongodb">Compose for MongoDB&lt;/a>。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>为 &lt;code>admin&lt;/code> 用户的密码设置一个环境变量。为了避免密码被保存在 Bash 历史记录中，在运行命令之后，请立即使用 &lt;a href="https://www.gnu.org/software/bash/manual/html_node/Bash-History-Builtins.html#Bash-History-Builtins">history -d&lt;/a> 将其从历史记录中删除。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export MONGO_ADMIN_PASSWORD=&amp;lt;your MongoDB admin password&amp;gt;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>为需要创建的新用户（即 &lt;code>bookinfo&lt;/code>）的密码设置环境变量，并使用 &lt;a href="https://www.gnu.org/software/bash/manual/html_node/Bash-History-Builtins.html#Bash-History-Builtins">history -d&lt;/a> 将其从历史记录中删除。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export BOOKINFO_PASSWORD=&amp;lt;password&amp;gt;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>为您的 MongoDB 服务设置环境变量 &lt;code>MONGODB_HOST&lt;/code> 和 &lt;code>MONGODB_PORT&lt;/code>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>创建 &lt;code>bookinfo&lt;/code> 用户：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | mongo --ssl --sslAllowInvalidCertificates $MONGODB_HOST:$MONGODB_PORT -u admin -p $MONGO_ADMIN_PASSWORD --authenticationDatabase admin
use test
db.createUser(
{
user: &amp;#34;bookinfo&amp;#34;,
pwd: &amp;#34;$BOOKINFO_PASSWORD&amp;#34;,
roles: [ &amp;#34;read&amp;#34;]
}
);
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>创建一个 &lt;code>collection&lt;/code> 来保存评级数据。以下命令将两个评级都设置为 &lt;code>1&lt;/code>，以便在 Bookinfo &lt;code>ratings&lt;/code> service 使用数据库时提供视觉验证（默认 Bookinfo &lt;code>ratings&lt;/code>
为 &lt;code>4&lt;/code> 和 &lt;code>5&lt;/code>）&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | mongo --ssl --sslAllowInvalidCertificates $MONGODB_HOST:$MONGODB_PORT -u admin -p $MONGO_ADMIN_PASSWORD --authenticationDatabase admin
use test
db.createCollection(&amp;#34;ratings&amp;#34;);
db.ratings.insert(
[{rating: 1},
{rating: 1}]
);
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>检查 &lt;code>bookinfo&lt;/code> 用户是否可以获取评级数据:&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | mongo --ssl --sslAllowInvalidCertificates $MONGODB_HOST:$MONGODB_PORT -u bookinfo -p $BOOKINFO_PASSWORD --authenticationDatabase test
use test
db.ratings.find({});
EOF
&lt;/code>&lt;/pre>
&lt;p>输出应该类似于:&lt;/p>
&lt;pre>&lt;code class='language-plain' data-expandlinks='true' >MongoDB server version: 3.4.10
switched to db test
{ &amp;#34;_id&amp;#34; : ObjectId(&amp;#34;5b7c29efd7596e65b6ed2572&amp;#34;), &amp;#34;rating&amp;#34; : 1 }
{ &amp;#34;_id&amp;#34; : ObjectId(&amp;#34;5b7c29efd7596e65b6ed2573&amp;#34;), &amp;#34;rating&amp;#34; : 1 }
bye
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h3 id="bookinfo-">Bookinfo 应用程序的初始设置&lt;/h3>
&lt;p>为了演示使用外部数据库的场景，请首先运行一个&lt;a href="/v1.1/zh/docs/setup/kubernetes/install/kubernetes/#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4">安装了 Istio&lt;/a> 的 Kubernetes 集群。然后部署
&lt;a href="/v1.1/zh/docs/examples/bookinfo/">Istio Bookinfo 示例应用程序&lt;/a>并&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E5%BA%94%E7%94%A8%E7%BC%BA%E7%9C%81%E7%9B%AE%E6%A0%87%E8%A7%84%E5%88%99">应用默认 destination rules&lt;/a>。&lt;/p>
&lt;p>此应用程序从 &lt;code>ratings&lt;/code> 微服务获取书籍评级（1 到 5 的数字）。评级以星标形式显示每条评论。&lt;code>ratings&lt;/code> 微服务有几个版本。在下一小节中，请部署使用 &lt;a href="https://www.mongodb.com">MongoDB&lt;/a>
作为 ratings 数据库的版本。&lt;/p>
&lt;p>本博文中的示例命令适用于 Istio 1.0。&lt;/p>
&lt;p>作为提醒，这是 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Bookinfo 示例应用程序&lt;/a> 的端到端架构。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:59.086918235567985%">
&lt;a data-skipendnotes="true" href="/v1.1/docs/examples/bookinfo/withistio.svg" title="初始 Bookinfo 应用程序">
&lt;img class="element-to-stretch" src="/v1.1/docs/examples/bookinfo/withistio.svg" alt="初始 Bookinfo 应用程序" />
&lt;/a>
&lt;/div>
&lt;figcaption>初始 Bookinfo 应用程序&lt;/figcaption>
&lt;/figure>
&lt;h3 id="-bookinfo-">在 Bookinfo 应用程序中使用外部数据库&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>部署使用 MongoDB 数据库的 &lt;code>ratings&lt;/code> 微服务（&lt;code>ratings v2&lt;/code>），并设置 &lt;code>MONGO_DB_URL&lt;/code> 环境变量：&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-ratings-v2.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/bookinfo/platform/kube/bookinfo-ratings-v2.yaml@ --dry-run -o yaml | kubectl set env --local -f - &amp;#34;MONGO_DB_URL=mongodb://bookinfo:$BOOKINFO_PASSWORD@$MONGODB_HOST:$MONGODB_PORT/test?authSource=test&amp;amp;ssl=true&amp;#34; -o yaml | kubectl apply -f -
deployment &amp;#34;ratings-v2&amp;#34; created
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;li>
&lt;p>将所有到 &lt;code>reviews&lt;/code> service 的流量路由到它的 &lt;code>v3&lt;/code> 版本，以确保 &lt;code>reviews&lt;/code> service 总是调用 &lt;code>ratings&lt;/code> service。此外，将所有到 &lt;code>ratings&lt;/code> service
的流量路由到使用外部数据库的 &lt;code>ratings v2版本&lt;/code>。&lt;/p>
&lt;p>通过添加两个 &lt;a href="/v1.1/zh/docs/reference/config/istio.networking.v1alpha3/#VirtualService">virtual service&lt;/a> 来为以上两个 service 指定路由。这些 virtual service
在 Istio 发布包中有指定。
&lt;em>&lt;strong>重要：&lt;/strong>&lt;/em> 请确保在运行以下命令之前&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E5%BA%94%E7%94%A8%E7%BC%BA%E7%9C%81%E7%9B%AE%E6%A0%87%E8%A7%84%E5%88%99">应用了默认的 destination rules&lt;/a>。&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/virtual-service-ratings-db.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/bookinfo/networking/virtual-service-ratings-db.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;/ol>
&lt;p>更新的架构如下所示。请注意，网格内的蓝色箭头标记对应于我们添加的 virtual service。根据 virtual service，流量将被发送到 &lt;code>reviews v3&lt;/code> 和 &lt;code>ratings v2&lt;/code>。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:59.314858206480224%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-mongo/./bookinfo-ratings-v2-mongodb-external.svg" title="使用 ratings v2 和外部 MongoDB 数据库的 Bookinfo 应用程序">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-mongo/./bookinfo-ratings-v2-mongodb-external.svg" alt="使用 ratings v2 和外部 MongoDB 数据库的 Bookinfo 应用程序" />
&lt;/a>
&lt;/div>
&lt;figcaption>使用 ratings v2 和外部 MongoDB 数据库的 Bookinfo 应用程序&lt;/figcaption>
&lt;/figure>
&lt;p>请注意，MongoDB 数据库位于 Istio 服务网格之外，或者更确切地说是在 Kubernetes 集群之外。服务网格的边界使用虚线标记。&lt;/p>
&lt;h3 id="heading">访问网页&lt;/h3>
&lt;p>&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E7%A1%AE%E5%AE%9A-Ingress-%E7%9A%84-IP-%E5%92%8C%E7%AB%AF%E5%8F%A3">确认 ingress IP 和端口之后&lt;/a>，访问应用程序的网页。&lt;/p>
&lt;p>由于您尚未配置 egress 流量控制，所以 Istio 会阻止到 MongoDB 服务的访问。这就是为什么您当前不能看到评级的星标，只能看到 &lt;em>&amp;ldquo;Ratings service is currently unavailable&amp;rdquo;&lt;/em> 的信息：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:36.18705035971223%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-mongo/./errorFetchingBookRating.png" title="Ratings service 错误信息">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-mongo/./errorFetchingBookRating.png" alt="Ratings service 错误信息" />
&lt;/a>
&lt;/div>
&lt;figcaption>Ratings service 错误信息&lt;/figcaption>
&lt;/figure>
&lt;p>在以下部分中，您将使用不同的 Istio egress 控制选项，配置对外部 MongoDB 服务的访问。&lt;/p>
&lt;h2 id="tcp--egress-">TCP 的 egress 控制&lt;/h2>
&lt;p>由于 &lt;a href="https://docs.mongodb.com/manual/reference/mongodb-wire-protocol/">MongoDB 协议&lt;/a>运行在 TCP 之上，您可以像控制到&lt;a href="/v1.1/zh/blog/2018/egress-tcp/">其余 TCP 服务&lt;/a>的流量一样控制到 MongoDB 的 egress 流量。为了控制 TCP 流量，您必须指定一个 &lt;a href="https://tools.ietf.org/html/rfc2317">CIDR&lt;/a> 表示的 IP 块，该 IP 块包含 MongoDB 的地址。需要注意的是，有时候 MongoDB 主机的 IP 并不稳定或无法事先得知。&lt;/p>
&lt;p>在 MongoDB IP 不稳定的情况下，可以以 TLS 方式控制 egress 流量，或绕过 Istio sidecar &lt;a href="/v1.1/zh/docs/tasks/traffic-management/egress/#%E7%9B%B4%E6%8E%A5%E8%B0%83%E7%94%A8%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1">直接&lt;/a>路由流量。&lt;/p>
&lt;p>获取 MongoDB 数据库实例的 IP 地址。一种选择是使用 &lt;a href="https://linux.die.net/man/1/host">host&lt;/a> 命令。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export MONGODB_IP=$(host $MONGODB_HOST | grep &amp;#34; has address &amp;#34; | cut -d&amp;#34; &amp;#34; -f4)
&lt;/code>&lt;/pre>
&lt;h3 id="-gateway--tcp-egress-">在没有 gateway 的情况下控制 TCP egress 流量&lt;/h3>
&lt;p>如果您不用通过 &lt;a href="/v1.1/zh/docs/examples/advanced-gateways/egress-gateway/#%E7%94%A8%E4%BE%8B">egress gateway&lt;/a> 定向流量，例如不要求所有流量都通过 gateway 流出网格时，请遵循以下部分的说明。或者，如果您确实希望通过 egress gateway 定向流量，请继续阅读&lt;a href="#%E9%80%9A%E8%BF%87-egress-gateway-%E5%AE%9A%E5%90%91-TCP-egress-%E6%B5%81%E9%87%8F">通过 egress gateway 定向 TCP egress 流量&lt;/a>。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>定义一个网格外 TCP service entry：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: mongo
spec:
hosts:
- my-mongo.tcp.svc
addresses:
- $MONGODB_IP/32
ports:
- number: $MONGODB_PORT
name: tcp
protocol: TCP
location: MESH_EXTERNAL
resolution: STATIC
endpoints:
- address: $MONGODB_IP
EOF
&lt;/code>&lt;/pre>
&lt;p>请注意，protocol 被指定为 &lt;code>TCP&lt;/code> 而不是 &lt;code>MONGO&lt;/code>，因为如果 &lt;a href="https://docs.mongodb.com/manual/tutorial/configure-ssl/">MongoDB 协议运行在 TLS 之上时&lt;/a>，流量可以加密。如果加密了流量，该加密的 MongoDB 协议就不能被 Istio 代理解析。&lt;/p>
&lt;p>如果您知道使用的是未加密的 MongoDB 协议，可以指定 protocol 为 &lt;code>MONGO&lt;/code>，从而使 Istio 代理产生 &lt;a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/network_filters/mongo_proxy_filter#statistics">MongoDB 相关的统计数据&lt;/a>。还要注意，当指定 protocol &lt;code>TCP&lt;/code> 时，配置不是特定于 MongoDB 的，对于其余使用基于 TCP 协议的数据库同样适用。&lt;/p>
&lt;p>请注意，MongoDB 的 host 不用于 TCP 路由，因此可以使用任何 host，例如 &lt;code>my-mongo.tcp.svc&lt;/code>。注意 &lt;code>STATIC&lt;/code> 会解析具有 MongoDB 服务 IP 的端点。定义此类端点后，可以访问没有域名的 MongoDB 服务。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>刷新应用程序的网页。应用程序现在应该显示评级数据而非错误：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:36.69064748201439%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-mongo/./externalDBRatings.png" title="Book 评级显式正确">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-mongo/./externalDBRatings.png" alt="Book 评级显式正确" />
&lt;/a>
&lt;/div>
&lt;figcaption>Book 评级显式正确&lt;/figcaption>
&lt;/figure>
&lt;p>请注意，和预期的一样，您会看到两个显示评论的一星评级。您将评级设置为一星，以作为外部数据库确实被使用了的证据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果要通过 egress gateway 引导流量，请继续下一部分。否则，执行&lt;a href="#%E6%B8%85%E7%90%86-TCP-egress-%E6%B5%81%E9%87%8F%E7%9A%84%E9%85%8D%E7%BD%AE">清理&lt;/a>。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="-egress-gateway--tcp-egress-">通过 egress gateway 定向 TCP egress 流量&lt;/h3>
&lt;p>在本节中，您将处理通过 &lt;a href="/v1.1/zh/docs/examples/advanced-gateways/egress-gateway/#%E7%94%A8%E4%BE%8B">egress gateway&lt;/a> 定向流量的情况。Sidecar 代理通过匹配 MongoDB 主机的 IP 地址（一个 32 位长度的 CIDR 块），将 TCP 连接从 MongoDB 客户端路由到 egress gateway。Egress gateway 按照其 hostname，转发流量到 MongoDB 主机。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="/v1.1/docs/examples/advanced-gateways/egress-gateway/#deploy-istio-egress-gateway">部署 Istio egress gateway&lt;/a>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>如果您没有执行&lt;a href="#%E5%9C%A8%E6%B2%A1%E6%9C%89-gateway-%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E6%8E%A7%E5%88%B6-TCP-egress-%E6%B5%81%E9%87%8F">上一节&lt;/a>中的步骤，现在就执行它们。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>继续以下部分。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="-sidecar--egress-gateway--tcp-">配置从 sidecar 到 egress gateway 的 TCP 流量&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>定义 &lt;code>EGRESS_GATEWAY_MONGODB_PORT&lt;/code> 环境变量来保存用于通过 egress gateway 定向流量的端口，例如 &lt;code>7777&lt;/code>。必须选择没有被网格中其余 service 使用的端口。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export EGRESS_GATEWAY_MONGODB_PORT=7777
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>添加选择的端口到 &lt;code>istio-egressgateway&lt;/code> service。您需要使用和安装 Istio 时一样的端口，特别是必须指定前面配置 &lt;code>istio-egressgateway&lt;/code> 的所有端口。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ helm template install/kubernetes/helm/istio/ --name istio-egressgateway --namespace istio-system -x charts/gateways/templates/service.yaml --set gateways.istio-ingressgateway.enabled=false --set gateways.istio-egressgateway.enabled=true --set gateways.istio-egressgateway.ports[0].port=80 --set gateways.istio-egressgateway.ports[0].name=http --set gateways.istio-egressgateway.ports[1].port=443 --set gateways.istio-egressgateway.ports[1].name=https --set gateways.istio-egressgateway.ports[2].port=$EGRESS_GATEWAY_MONGODB_PORT --set gateways.istio-egressgateway.ports[2].name=mongo | kubectl apply -f -
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>检查 &lt;code>istio-egressgateway&lt;/code> service 确实有选择的端口：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get svc istio-egressgateway -n istio-system
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
istio-egressgateway ClusterIP 172.21.202.204 &amp;lt;none&amp;gt; 80/TCP,443/TCP,7777/TCP 34d
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>为您的 MongoDB service 创建一个 egress &lt;code>Gateway&lt;/code>、一个 destination rules 和 virtual services，以定向流量到 egress gateway，并从 egress gateway 发送到外部服务。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
name: istio-egressgateway
spec:
selector:
istio: egressgateway
servers:
- port:
number: $EGRESS_GATEWAY_MONGODB_PORT
name: tcp
protocol: TCP
hosts:
- my-mongo.tcp.svc
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: egressgateway-for-mongo
spec:
host: istio-egressgateway.istio-system.svc.cluster.local
subsets:
- name: mongo
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: mongo
spec:
host: my-mongo.tcp.svc
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: direct-mongo-through-egress-gateway
spec:
hosts:
- my-mongo.tcp.svc
gateways:
- mesh
- istio-egressgateway
tcp:
- match:
- gateways:
- mesh
destinationSubnets:
- $MONGODB_IP/32
port: $MONGODB_PORT
route:
- destination:
host: istio-egressgateway.istio-system.svc.cluster.local
subset: mongo
port:
number: $EGRESS_GATEWAY_MONGODB_PORT
- match:
- gateways:
- istio-egressgateway
port: $EGRESS_GATEWAY_MONGODB_PORT
route:
- destination:
host: my-mongo.tcp.svc
port:
number: $MONGODB_PORT
weight: 100
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#%E9%AA%8C%E8%AF%81-TCP-egress-%E6%B5%81%E9%87%8F%E6%98%AF%E5%90%A6%E9%80%9A%E8%BF%87-egress-gateway-%E5%AE%9A%E5%90%91">继续验证 TCP egress 流量是否被定向到 egress gateway&lt;/a>。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="sidecar--egress-gateway--tls">Sidecar 代理和 egress gateway 之间的双向 TLS&lt;/h4>
&lt;p>您可能希望启用 sidecar 代理和 MongoDB 客户端之间以及 egress gateway 的&lt;a href="/v1.1/zh/docs/tasks/security/mutual-tls/">双向 TLS 认证&lt;/a>，以使 egress gateway 监控来源 pod 的身份并基于该 identity 启用 Mixer 策略。启用双向 TLS 时同样对流量进行了加密。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>删除前面小节中的配置：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete gateway istio-egressgateway --ignore-not-found=true
$ kubectl delete virtualservice direct-mongo-through-egress-gateway --ignore-not-found=true
$ kubectl delete destinationrule egressgateway-for-mongo mongo --ignore-not-found=true
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>为您的 MongoDB service 创建一个 egress &lt;code>Gateway&lt;/code>、一个 destination rules 和 virtual services，以定向流量到 egress gateway，并从 egress gateway 发送到外部服务。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
name: istio-egressgateway
spec:
selector:
istio: egressgateway
servers:
- port:
number: 443
name: tls
protocol: TLS
hosts:
- my-mongo.tcp.svc
tls:
mode: MUTUAL
serverCertificate: /etc/certs/cert-chain.pem
privateKey: /etc/certs/key.pem
caCertificates: /etc/certs/root-cert.pem
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: egressgateway-for-mongo
spec:
host: istio-egressgateway.istio-system.svc.cluster.local
trafficPolicy:
loadBalancer:
simple: ROUND_ROBIN
portLevelSettings:
- port:
number: 443
tls:
mode: ISTIO_MUTUAL
subsets:
- name: mongo
trafficPolicy:
loadBalancer:
simple: ROUND_ROBIN
portLevelSettings:
- port:
number: 443
tls:
mode: ISTIO_MUTUAL
sni: my-mongo.tcp.svc
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: mongo
spec:
host: my-mongo.tcp.svc
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: direct-mongo-through-egress-gateway
spec:
hosts:
- my-mongo.tcp.svc
gateways:
- mesh
- istio-egressgateway
tcp:
- match:
- gateways:
- mesh
destinationSubnets:
- $MONGODB_IP/32
port: $MONGODB_PORT
route:
- destination:
host: istio-egressgateway.istio-system.svc.cluster.local
subset: mongo
port:
number: 443
- match:
- gateways:
- istio-egressgateway
port: 443
route:
- destination:
host: my-mongo.tcp.svc
port:
number: $MONGODB_PORT
weight: 100
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>继续下一小节。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="-tcp-egress--egress-gateway-">验证 TCP egress 流量是否通过 egress gateway 定向&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>再次刷新应用程序的网页，验证评级数据仍然显示正确。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="/v1.1/zh/docs/tasks/telemetry/logs/access-log/#%E5%BC%80%E5%90%AF-Envoy-%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97">启动 Envoy’s 访问日志&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>检查 egress gateway 的 Envoy 的统计数据，找到对应请求 MongoDB service 的 counter。如果 Istio 步骤在 &lt;code>istio-system&lt;/code> namespace 中，打印 counter 的命令为：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl logs -l istio=egressgateway -n istio-system
[2019-04-14T06:12:07.636Z] &amp;#34;- - -&amp;#34; 0 - &amp;#34;-&amp;#34; 1591 4393 94 - &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;&amp;lt;Your MongoDB IP&amp;gt;:&amp;lt;your MongoDB port&amp;gt;&amp;#34; outbound|&amp;lt;your MongoDB port&amp;gt;||my-mongo.tcp.svc 172.30.146.119:59924 172.30.146.119:443 172.30.230.1:59206 -
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h4 id="-tcp-egress-">清理 TCP egress 流量的配置&lt;/h4>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete serviceentry mongo
$ kubectl delete gateway istio-egressgateway --ignore-not-found=true
$ kubectl delete virtualservice direct-mongo-through-egress-gateway --ignore-not-found=true
$ kubectl delete destinationrule egressgateway-for-mongo mongo --ignore-not-found=true
&lt;/code>&lt;/pre>
&lt;h2 id="tls-egress-">TLS egress 控制&lt;/h2>
&lt;p>在现实生活中，绝大多数到外部服务的通信都必须被加密，而 &lt;a href="https://docs.mongodb.com/manual/tutorial/configure-ssl/">MongoDB 协议在 TLS 之上运行&lt;/a>。并且，TLS 客户端经常发送&lt;a href="https://en.wikipedia.org/wiki/Server_Name_Indication">服务器名称指示（Server Name Indication，SNI）&lt;/a>作为握手的一部分。如果您的 MongoDB 服务器运行 TLS 且 MongoDB 客户端发送 SNI 作为握手的一部分，您就可以像任何其余带有 SNI 的 TLS 流量一样控制 MongoDB egress 流量。您不需要指定 MongoDB 服务器的IP地址，而只需指定他们的主机名称，这样会更加方便，因为您无需依赖 IP 地址的稳定性。您还可以指定通配符为主机名的前缀，例如允许从 &lt;code>*.com&lt;/code> 域访问任意服务器。&lt;/p>
&lt;p>要想检查您的 MongoDB 服务器是否支持 TLS，请运行：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ openssl s_client -connect $MONGODB_HOST:$MONGODB_PORT -servername $MONGODB_HOST
&lt;/code>&lt;/pre>
&lt;p>如果上述命令打印了一个服务器返回的证书，说明该服务器支持 TLS。如果没有，您就需要像前面小节描述的一样在 TCP 层面控制 MongoDB egress 流量。&lt;/p>
&lt;h3 id="-gateway--tls-egress-">无 gateway 情况下控制 TLS egress 流量&lt;/h3>
&lt;p>如果您&lt;a href="/v1.1/zh/docs/examples/advanced-gateways/egress-gateway/#%E7%94%A8%E4%BE%8B">不需要 egress gateway&lt;/a>，请遵循本小节中的说明。如果您需要通过 egress gateway 定向流量，请继续阅读&lt;a href="#%E9%80%9A%E8%BF%87-egress-gateway-%E5%AE%9A%E5%90%91-TCP-egress-%E6%B5%81%E9%87%8F">通过 egress gateway 定向 TCP Egress 流量&lt;/a>。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>为 MongoDB service 创建一个 &lt;code>ServiceEntry&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: mongo
spec:
hosts:
- $MONGODB_HOST
ports:
- number: $MONGODB_PORT
name: tls
protocol: TLS
resolution: DNS
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>刷新应用程序的网页。应用程序应该正确显示评级数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="-tls--egress-">清理 TLS 的 egress 配置&lt;/h4>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete serviceentry mongo
&lt;/code>&lt;/pre>
&lt;h3 id="-egress-gateway--tls-egress-">通过 egress gateway 定向 TLS Egress 流量&lt;/h3>
&lt;p>在本小节中，您将处理通过 &lt;a href="/v1.1/zh/docs/examples/advanced-gateways/egress-gateway/#%E7%94%A8%E4%BE%8B">egress gateway&lt;/a> 定向流量的情况。Sidecar 代理通过匹配 MongoDB 主机的 SNI，将 TLS 连接从 MongoDB 客户端路由到 egress gateway。Egress gateway 再将流量转发到 MongoDB 主机。请注意，sidecar 代理会将目的端口重写为 443。Egress gateway 在 443 端口上接受 MongoDB 流量，按照 SNI 匹配 MongoDB 主机，并再次将端口重写为 MongoDB 服务器的端口。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="/v1.1/docs/examples/advanced-gateways/egress-gateway/#deploy-istio-egress-gateway">部署 Istio egress gateway&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>为 MongoDB service 创建一个 &lt;code>ServiceEntry&lt;/code>:&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: mongo
spec:
hosts:
- $MONGODB_HOST
ports:
- number: $MONGODB_PORT
name: tls
protocol: TLS
- number: 443
name: tls-port-for-egress-gateway
protocol: TLS
resolution: DNS
location: MESH_EXTERNAL
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>刷新应用程序的网页并验证评级数据是否显示正常。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>为您的 MongoDB service 创建一个 egress &lt;code>Gateway&lt;/code>、一个 destination rules 和 virtual services，以将流量定向到 egress gateway，并从 egress gateway 发送到外部服务。&lt;/p>
&lt;p>如果要在应用程序 pod 的 Sidecar 代理和和 egress gateway 之间启用&lt;a href="/v1.1/zh/docs/tasks/security/mutual-tls/">双向 TLS 身份验证&lt;/a>，请使用以下命令。（您可能希望启用双向 TLS
出口网关监视源容器的身份并基于此启用 Mixer 策略实施身份。）&lt;/p>
&lt;div id="tabset-zh-blog-2018-egress-mongo-2" role="tablist" class="tabset">
&lt;div class="tab-strip" data-cookie-name="mtls">&lt;button aria-selected="true" data-cookie-value="enabled"
aria-controls="tabset-zh-blog-2018-egress-mongo-2-0-panel" id="tabset-zh-blog-2018-egress-mongo-2-0-tab" role="tab">&lt;span>mutual TLS enabled&lt;/span>
&lt;/button>&lt;button tabindex="-1" data-cookie-value="disabled"
aria-controls="tabset-zh-blog-2018-egress-mongo-2-1-panel" id="tabset-zh-blog-2018-egress-mongo-2-1-tab" role="tab">&lt;span>mutual TLS disabled&lt;/span>
&lt;/button>&lt;/div>
&lt;div class="tab-content">&lt;div id="tabset-zh-blog-2018-egress-mongo-2-0-panel" role="tabpanel" tabindex="0" aria-labelledby="tabset-zh-blog-2018-egress-mongo-2-0-tab">&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl apply -f - &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#e6db74">apiVersion: networking.istio.io/v1alpha3
&lt;/span>&lt;span style="color:#e6db74">kind: Gateway
&lt;/span>&lt;span style="color:#e6db74">metadata:
&lt;/span>&lt;span style="color:#e6db74"> name: istio-egressgateway
&lt;/span>&lt;span style="color:#e6db74">spec:
&lt;/span>&lt;span style="color:#e6db74"> selector:
&lt;/span>&lt;span style="color:#e6db74"> istio: egressgateway
&lt;/span>&lt;span style="color:#e6db74"> servers:
&lt;/span>&lt;span style="color:#e6db74"> - port:
&lt;/span>&lt;span style="color:#e6db74"> number: 443
&lt;/span>&lt;span style="color:#e6db74"> name: tls
&lt;/span>&lt;span style="color:#e6db74"> protocol: TLS
&lt;/span>&lt;span style="color:#e6db74"> hosts:
&lt;/span>&lt;span style="color:#e6db74"> - $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74"> tls:
&lt;/span>&lt;span style="color:#e6db74"> mode: MUTUAL
&lt;/span>&lt;span style="color:#e6db74"> serverCertificate: /etc/certs/cert-chain.pem
&lt;/span>&lt;span style="color:#e6db74"> privateKey: /etc/certs/key.pem
&lt;/span>&lt;span style="color:#e6db74"> caCertificates: /etc/certs/root-cert.pem
&lt;/span>&lt;span style="color:#e6db74">---
&lt;/span>&lt;span style="color:#e6db74">apiVersion: networking.istio.io/v1alpha3
&lt;/span>&lt;span style="color:#e6db74">kind: DestinationRule
&lt;/span>&lt;span style="color:#e6db74">metadata:
&lt;/span>&lt;span style="color:#e6db74"> name: egressgateway-for-mongo
&lt;/span>&lt;span style="color:#e6db74">spec:
&lt;/span>&lt;span style="color:#e6db74"> host: istio-egressgateway.istio-system.svc.cluster.local
&lt;/span>&lt;span style="color:#e6db74"> trafficPolicy:
&lt;/span>&lt;span style="color:#e6db74"> loadBalancer:
&lt;/span>&lt;span style="color:#e6db74"> simple: ROUND_ROBIN
&lt;/span>&lt;span style="color:#e6db74"> portLevelSettings:
&lt;/span>&lt;span style="color:#e6db74"> - port:
&lt;/span>&lt;span style="color:#e6db74"> number: 443
&lt;/span>&lt;span style="color:#e6db74"> tls:
&lt;/span>&lt;span style="color:#e6db74"> mode: ISTIO_MUTUAL
&lt;/span>&lt;span style="color:#e6db74"> subsets:
&lt;/span>&lt;span style="color:#e6db74"> - name: mongo
&lt;/span>&lt;span style="color:#e6db74"> trafficPolicy:
&lt;/span>&lt;span style="color:#e6db74"> loadBalancer:
&lt;/span>&lt;span style="color:#e6db74"> simple: ROUND_ROBIN
&lt;/span>&lt;span style="color:#e6db74"> portLevelSettings:
&lt;/span>&lt;span style="color:#e6db74"> - port:
&lt;/span>&lt;span style="color:#e6db74"> number: 443
&lt;/span>&lt;span style="color:#e6db74"> tls:
&lt;/span>&lt;span style="color:#e6db74"> mode: ISTIO_MUTUAL
&lt;/span>&lt;span style="color:#e6db74"> sni: $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74">---
&lt;/span>&lt;span style="color:#e6db74">apiVersion: networking.istio.io/v1alpha3
&lt;/span>&lt;span style="color:#e6db74">kind: VirtualService
&lt;/span>&lt;span style="color:#e6db74">metadata:
&lt;/span>&lt;span style="color:#e6db74"> name: direct-mongo-through-egress-gateway
&lt;/span>&lt;span style="color:#e6db74">spec:
&lt;/span>&lt;span style="color:#e6db74"> hosts:
&lt;/span>&lt;span style="color:#e6db74"> - $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74"> gateways:
&lt;/span>&lt;span style="color:#e6db74"> - mesh
&lt;/span>&lt;span style="color:#e6db74"> - istio-egressgateway
&lt;/span>&lt;span style="color:#e6db74"> tls:
&lt;/span>&lt;span style="color:#e6db74"> - match:
&lt;/span>&lt;span style="color:#e6db74"> - gateways:
&lt;/span>&lt;span style="color:#e6db74"> - mesh
&lt;/span>&lt;span style="color:#e6db74"> port: $MONGODB_PORT
&lt;/span>&lt;span style="color:#e6db74"> sni_hosts:
&lt;/span>&lt;span style="color:#e6db74"> - $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74"> route:
&lt;/span>&lt;span style="color:#e6db74"> - destination:
&lt;/span>&lt;span style="color:#e6db74"> host: istio-egressgateway.istio-system.svc.cluster.local
&lt;/span>&lt;span style="color:#e6db74"> subset: mongo
&lt;/span>&lt;span style="color:#e6db74"> port:
&lt;/span>&lt;span style="color:#e6db74"> number: 443
&lt;/span>&lt;span style="color:#e6db74"> tcp:
&lt;/span>&lt;span style="color:#e6db74"> - match:
&lt;/span>&lt;span style="color:#e6db74"> - gateways:
&lt;/span>&lt;span style="color:#e6db74"> - istio-egressgateway
&lt;/span>&lt;span style="color:#e6db74"> port: 443
&lt;/span>&lt;span style="color:#e6db74"> route:
&lt;/span>&lt;span style="color:#e6db74"> - destination:
&lt;/span>&lt;span style="color:#e6db74"> host: $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74"> port:
&lt;/span>&lt;span style="color:#e6db74"> number: $MONGODB_PORT
&lt;/span>&lt;span style="color:#e6db74"> weight: 100
&lt;/span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;/div>&lt;div hidden id="tabset-zh-blog-2018-egress-mongo-2-1-panel" role="tabpanel" tabindex="0" aria-labelledby="tabset-zh-blog-2018-egress-mongo-2-1-tab">&lt;div class="highlight">&lt;pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl apply -f - &lt;span style="color:#e6db74">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#e6db74">apiVersion: networking.istio.io/v1alpha3
&lt;/span>&lt;span style="color:#e6db74">kind: Gateway
&lt;/span>&lt;span style="color:#e6db74">metadata:
&lt;/span>&lt;span style="color:#e6db74"> name: istio-egressgateway
&lt;/span>&lt;span style="color:#e6db74">spec:
&lt;/span>&lt;span style="color:#e6db74"> selector:
&lt;/span>&lt;span style="color:#e6db74"> istio: egressgateway
&lt;/span>&lt;span style="color:#e6db74"> servers:
&lt;/span>&lt;span style="color:#e6db74"> - port:
&lt;/span>&lt;span style="color:#e6db74"> number: 443
&lt;/span>&lt;span style="color:#e6db74"> name: tls
&lt;/span>&lt;span style="color:#e6db74"> protocol: TLS
&lt;/span>&lt;span style="color:#e6db74"> hosts:
&lt;/span>&lt;span style="color:#e6db74"> - $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74"> tls:
&lt;/span>&lt;span style="color:#e6db74"> mode: PASSTHROUGH
&lt;/span>&lt;span style="color:#e6db74">---
&lt;/span>&lt;span style="color:#e6db74">apiVersion: networking.istio.io/v1alpha3
&lt;/span>&lt;span style="color:#e6db74">kind: DestinationRule
&lt;/span>&lt;span style="color:#e6db74">metadata:
&lt;/span>&lt;span style="color:#e6db74"> name: egressgateway-for-mongo
&lt;/span>&lt;span style="color:#e6db74">spec:
&lt;/span>&lt;span style="color:#e6db74"> host: istio-egressgateway.istio-system.svc.cluster.local
&lt;/span>&lt;span style="color:#e6db74"> subsets:
&lt;/span>&lt;span style="color:#e6db74"> - name: mongo
&lt;/span>&lt;span style="color:#e6db74">---
&lt;/span>&lt;span style="color:#e6db74">apiVersion: networking.istio.io/v1alpha3
&lt;/span>&lt;span style="color:#e6db74">kind: VirtualService
&lt;/span>&lt;span style="color:#e6db74">metadata:
&lt;/span>&lt;span style="color:#e6db74"> name: direct-mongo-through-egress-gateway
&lt;/span>&lt;span style="color:#e6db74">spec:
&lt;/span>&lt;span style="color:#e6db74"> hosts:
&lt;/span>&lt;span style="color:#e6db74"> - $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74"> gateways:
&lt;/span>&lt;span style="color:#e6db74"> - mesh
&lt;/span>&lt;span style="color:#e6db74"> - istio-egressgateway
&lt;/span>&lt;span style="color:#e6db74"> tls:
&lt;/span>&lt;span style="color:#e6db74"> - match:
&lt;/span>&lt;span style="color:#e6db74"> - gateways:
&lt;/span>&lt;span style="color:#e6db74"> - mesh
&lt;/span>&lt;span style="color:#e6db74"> port: $MONGODB_PORT
&lt;/span>&lt;span style="color:#e6db74"> sni_hosts:
&lt;/span>&lt;span style="color:#e6db74"> - $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74"> route:
&lt;/span>&lt;span style="color:#e6db74"> - destination:
&lt;/span>&lt;span style="color:#e6db74"> host: istio-egressgateway.istio-system.svc.cluster.local
&lt;/span>&lt;span style="color:#e6db74"> subset: mongo
&lt;/span>&lt;span style="color:#e6db74"> port:
&lt;/span>&lt;span style="color:#e6db74"> number: 443
&lt;/span>&lt;span style="color:#e6db74"> - match:
&lt;/span>&lt;span style="color:#e6db74"> - gateways:
&lt;/span>&lt;span style="color:#e6db74"> - istio-egressgateway
&lt;/span>&lt;span style="color:#e6db74"> port: 443
&lt;/span>&lt;span style="color:#e6db74"> sni_hosts:
&lt;/span>&lt;span style="color:#e6db74"> - $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74"> route:
&lt;/span>&lt;span style="color:#e6db74"> - destination:
&lt;/span>&lt;span style="color:#e6db74"> host: $MONGODB_HOST
&lt;/span>&lt;span style="color:#e6db74"> port:
&lt;/span>&lt;span style="color:#e6db74"> number: $MONGODB_PORT
&lt;/span>&lt;span style="color:#e6db74"> weight: 100
&lt;/span>&lt;span style="color:#e6db74">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;/div>&lt;/div>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#%E9%AA%8C%E8%AF%81-TCP-egress-%E6%B5%81%E9%87%8F%E6%98%AF%E5%90%A6%E9%80%9A%E8%BF%87-egress-gateway-%E5%AE%9A%E5%90%91">继续验证 TCP egress 流量是否被定向到 egress gateway&lt;/a>。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="-egress-gateway--tls-egress--1">清除通过 egress gateway 定向 TLS Egress 流量的配置&lt;/h4>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete serviceentry mongo
$ kubectl delete gateway istio-egressgateway
$ kubectl delete virtualservice direct-mongo-through-egress-gateway
$ kubectl delete destinationrule egressgateway-for-mongo
&lt;/code>&lt;/pre>
&lt;h3 id="-mongodb-tls-egress-">启用到任意通配符域名的 MongoDB TLS egress 流量&lt;/h3>
&lt;p>有时，您希望将 egress 流量配置为来自同一域的多个主机名，例如到 &lt;code>*.&amp;lt;your company domain&amp;gt;.com&lt;/code> 中的所有 MongoDB service。您不希望创建多个配置项，而是一个用于公司中所有 MongoDB service 的通用配置项。要想通过一个配置来控制到所有相同域中的外部服务的访问，您需要使用&lt;em>通配符&lt;/em>主机。&lt;/p>
&lt;p>在本节中，您将为通配域配置出口流量。我在 &lt;code>composedb.com&lt;/code> 域使用了 MongoDB 实例，因此配置 &lt;code>*.com&lt;/code> 的出口流量对我有效（我本可以使用 &lt;code>*.composedb.com&lt;/code>）。
您可以根据 MongoDB 主机选择通配域。&lt;/p>
&lt;p>要为通配符域名配置 egress gateway 流量，您需要使用&lt;a href="/v1.1/zh/docs/examples/advanced-gateways/wildcard-egress-hosts/#%E9%85%8D%E7%BD%AE%E5%88%B0%E9%80%9A%E9%85%8D%E7%AC%A6%E4%B8%BB%E6%9C%BA%E7%9A%84%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F">一个额外的 SNI 代理&lt;/a>来部署一个自定义的 egress gateway。由于 Envoy（Istio egress gateway 使用的标准代理）目前的限制，这是必须的。&lt;/p>
&lt;h4 id="-sni--egress-gateway">准备一个使用 SNI 代理的新 egress gateway&lt;/h4>
&lt;p>在本小节中，除了标准的 Istio Envoy 代理之外，您还将部署具有 SNI 代理的 egress gateway。您可以使用任何能够根据任意未预先配置的 SNI 值路由流量的 SNI 代理；我们使用 &lt;a href="http://nginx.org">Nginx&lt;/a> 来实现这一功能。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>为 Nginx SNI 代理创建配置文件。如果需要，您可以编辑该文件以指定其他 Nginx 设置。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF &amp;gt; ./sni-proxy.conf
user www-data;
events {
}
stream {
log_format log_stream &amp;#39;\$remote_addr [\$time_local] \$protocol [\$ssl_preread_server_name]&amp;#39;
&amp;#39;\$status \$bytes_sent \$bytes_received \$session_time&amp;#39;;
access_log /var/log/nginx/access.log log_stream;
error_log /var/log/nginx/error.log;
# SNI tcp 转发代理
server {
resolver 8.8.8.8 ipv6=off;
listen 127.0.0.1:$MONGODB_PORT;
proxy_pass \$ssl_preread_server_name:$MONGODB_PORT;
ssl_preread on;
}
}
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>创建一个 Kubernetes &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/">ConfigMap&lt;/a> 来保存 Nginx SNI 代理的配置：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl create configmap egress-sni-proxy-configmap -n istio-system --from-file=nginx.conf=./sni-proxy.conf
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>下面的命令将产生用于编辑和部署的 &lt;code>istio-egressgateway-with-sni-proxy.yaml&lt;/code> 文件。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | helm template install/kubernetes/helm/istio/ --name istio-egressgateway-with-sni-proxy --namespace istio-system -x charts/gateways/templates/deployment.yaml -x charts/gateways/templates/service.yaml -x charts/gateways/templates/serviceaccount.yaml -x charts/gateways/templates/autoscale.yaml -x charts/gateways/templates/clusterrole.yaml -x charts/gateways/templates/clusterrolebindings.yaml --set global.mtls.enabled=true --set global.istioNamespace=istio-system -f - &amp;gt; ./istio-egressgateway-with-sni-proxy.yaml
gateways:
enabled: true
istio-ingressgateway:
enabled: false
istio-egressgateway:
enabled: false
istio-egressgateway-with-sni-proxy:
enabled: true
labels:
app: istio-egressgateway-with-sni-proxy
istio: egressgateway-with-sni-proxy
replicaCount: 1
autoscaleMin: 1
autoscaleMax: 5
cpu:
targetAverageUtilization: 80
serviceAnnotations: {}
type: ClusterIP
ports:
- port: 443
name: https
secretVolumes:
- name: egressgateway-certs
secretName: istio-egressgateway-certs
mountPath: /etc/istio/egressgateway-certs
- name: egressgateway-ca-certs
secretName: istio-egressgateway-ca-certs
mountPath: /etc/istio/egressgateway-ca-certs
configVolumes:
- name: sni-proxy-config
configMapName: egress-sni-proxy-configmap
additionalContainers:
- name: sni-proxy
image: nginx
volumeMounts:
- name: sni-proxy-config
mountPath: /etc/nginx
readOnly: true
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>部署新的 egress gateway：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f ./istio-egressgateway-with-sni-proxy.yaml
serviceaccount &amp;#34;istio-egressgateway-with-sni-proxy-service-account&amp;#34; created
clusterrole &amp;#34;istio-egressgateway-with-sni-proxy-istio-system&amp;#34; created
clusterrolebinding &amp;#34;istio-egressgateway-with-sni-proxy-istio-system&amp;#34; created
service &amp;#34;istio-egressgateway-with-sni-proxy&amp;#34; created
deployment &amp;#34;istio-egressgateway-with-sni-proxy&amp;#34; created
horizontalpodautoscaler &amp;#34;istio-egressgateway-with-sni-proxy&amp;#34; created
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>验证新 egress gateway 是否工作正常。请注意 pod 有两个容器（一个是 Envoy 代理，另一个是 SNI 代理）。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pod -l istio=egressgateway-with-sni-proxy -n istio-system
NAME READY STATUS RESTARTS AGE
istio-egressgateway-with-sni-proxy-79f6744569-pf9t2 2/2 Running 0 17s
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>创建一个使用静态地址 127.0.0.1 (&lt;code>localhost&lt;/code>) 的 service entry，并对定向到新 service entry 的流量禁用双向 TLS：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: sni-proxy
spec:
hosts:
- sni-proxy.local
location: MESH_EXTERNAL
ports:
- number: $MONGODB_PORT
name: tcp
protocol: TCP
resolution: STATIC
endpoints:
- address: 127.0.0.1
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: disable-mtls-for-sni-proxy
spec:
host: sni-proxy.local
trafficPolicy:
tls:
mode: DISABLE
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h4 id="-egress-gateway--com-">使用新 egress gateway 配置到 &lt;code>*.com&lt;/code> 的访问&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>为 &lt;code>*.com&lt;/code> 定义一个 &lt;code>ServiceEntry&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: mongo
spec:
hosts:
- &amp;#34;*.com&amp;#34;
ports:
- number: 443
name: tls
protocol: TLS
- number: $MONGODB_PORT
name: tls-mongodb
protocol: TLS
location: MESH_EXTERNAL
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>为 &lt;code>*.com&lt;/code> 创建一个 egress &lt;code>Gateway&lt;/code>，使用 443 端口和 TLS 协议。创建一个 destination rule 来为 gateway 设置 &lt;a href="https://en.wikipedia.org/wiki/Server_Name_Indication">SNI&lt;/a>，以及一个 virtual service 以将目的为 &lt;code>*.com&lt;/code> 流量定向到 gateway。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
name: istio-egressgateway-with-sni-proxy
spec:
selector:
istio: egressgateway-with-sni-proxy
servers:
- port:
number: 443
name: tls
protocol: TLS
hosts:
- &amp;#34;*.com&amp;#34;
tls:
mode: MUTUAL
serverCertificate: /etc/certs/cert-chain.pem
privateKey: /etc/certs/key.pem
caCertificates: /etc/certs/root-cert.pem
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: mtls-for-egress-gateway
spec:
host: istio-egressgateway-with-sni-proxy.istio-system.svc.cluster.local
trafficPolicy:
loadBalancer:
simple: ROUND_ROBIN
portLevelSettings:
- port:
number: 443
tls:
mode: ISTIO_MUTUAL
subsets:
- name: mongo
trafficPolicy:
loadBalancer:
simple: ROUND_ROBIN
portLevelSettings:
- port:
number: 443
tls:
mode: ISTIO_MUTUAL
---
# 以下过滤器用于将原始 SNI（由应用程序发送）转发为双向 TLS 连接的 SNI。
# 转发的 SNI 将报告给Mixer，以便根据原始 SNI 值强制执行策略。
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
name: forward-downstream-sni
spec:
filters:
- listenerMatch:
portNumber: $MONGODB_PORT
listenerType: SIDECAR_OUTBOUND
filterName: forward_downstream_sni
filterType: NETWORK
filterConfig: {}
---
# 以下过滤器验证双向 TLS 连接的 SNI（报告给 Mixer 的 SNI）与应用程序发布的原始 SNI（SNI 代理用于路由的 SNI）相同。
# 过滤器可防止 Mixer 被恶意应用程序欺骗：路由到一个 SNI，同时报告其他一些 SNI 值。
# 如果原始 SNI 与双向 TLS 连接的 SNI 不匹配，则过滤器将阻止与外部服务的连接。
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
name: egress-gateway-sni-verifier
spec:
workloadLabels:
app: istio-egressgateway-with-sni-proxy
filters:
- listenerMatch:
portNumber: 443
listenerType: GATEWAY
filterName: sni_verifier
filterType: NETWORK
filterConfig: {}
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>将目的为 &lt;code>*.com&lt;/code> 的流量路由到 egress gateway，并从 egress gateway 路由到 SNI 代理。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: direct-mongo-through-egress-gateway
spec:
hosts:
- &amp;#34;*.com&amp;#34;
gateways:
- mesh
- istio-egressgateway-with-sni-proxy
tls:
- match:
- gateways:
- mesh
port: $MONGODB_PORT
sni_hosts:
- &amp;#34;*.com&amp;#34;
route:
- destination:
host: istio-egressgateway-with-sni-proxy.istio-system.svc.cluster.local
subset: mongo
port:
number: 443
weight: 100
tcp:
- match:
- gateways:
- istio-egressgateway-with-sni-proxy
port: 443
route:
- destination:
host: sni-proxy.local
port:
number: $MONGODB_PORT
weight: 100
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>再次刷新应用程序的网页，验证评级数据仍然显示正确。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="/v1.1/zh/docs/tasks/telemetry/logs/access-log/#%E5%BC%80%E5%90%AF-Envoy-%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97">启动 Envoy’s 访问日志&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>检查 egress gateway 的 Envoy 的统计数据，找到对应请求 &lt;code>*.com&lt;/code> 的 counter（到 SNI 代理的流量的 counter）。如果 Istio 部署在 &lt;code>istio-system&lt;/code> namespace 中，打印 counter 的命令为：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl logs -l istio=egressgateway-with-sni-proxy -c istio-proxy -n istio-system
&lt;/code>&lt;/pre>
&lt;p>您应该看到类似于以下内容的行：&lt;/p>
&lt;pre>&lt;code class='language-plain' data-expandlinks='true' >[2019-01-02T17:22:04.602Z] &amp;#34;- - -&amp;#34; 0 - 768 1863 88 - &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;127.0.0.1:28543&amp;#34; outbound|28543||sni-proxy.local 127.0.0.1:49976 172.30.146.115:443 172.30.146.118:58510 &amp;lt;your MongoDB host&amp;gt;
[2019-01-02T17:22:04.713Z] &amp;#34;- - -&amp;#34; 0 - 1534 2590 85 - &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;-&amp;#34; &amp;#34;127.0.0.1:28543&amp;#34; outbound|28543||sni-proxy.local 127.0.0.1:49988 172.30.146.115:443 172.30.146.118:58522 &amp;lt;your MongoDB host&amp;gt;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>检查 SNI 代理的日志。如果 Istio 部署在 &lt;code>istio-system&lt;/code> namespace 中，打印日志的命令为：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl logs -l istio=egressgateway-with-sni-proxy -n istio-system -c sni-proxy
127.0.0.1 [23/Aug/2018:03:28:18 +0000] TCP [&amp;lt;your MongoDB host&amp;gt;]200 1863 482 0.089
127.0.0.1 [23/Aug/2018:03:28:18 +0000] TCP [&amp;lt;your MongoDB host&amp;gt;]200 2590 1248 0.095
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h4 id="heading-1">理解发生了什么&lt;/h4>
&lt;p>在本节中，您使用通配符域名为您的 MongoDB 主机配置了 egress 流量。对于单个 MongoDB 主机使用通配符域名没有任何好处（可以指定确切的主机名），而当集群中的应用程序需要访问多个匹配某个通配符域名的 MongoDB 主机时可能有用。例如，如果应用程序需要访问 &lt;code>mongodb1.composedb.com&lt;/code>、&lt;code>mongodb2.composedb.com&lt;/code> 和 &lt;code>mongodb3.composedb.com&lt;/code> 时，egress 流量可以使用针对 &lt;code>*.composedb.com&lt;/code> 的单个配置实现。&lt;/p>
&lt;p>当配置一个应用使用另一个主机名匹配本小节中的通配符域名的 MongoDB 实例时，不需要额外的 Istio 配置。我将这留作一个练习，让读者自行验证。&lt;/p>
&lt;h4 id="-mongodb-tls-egress--1">清理到任意通配符域名的 MongoDB TLS egress 流量的配置&lt;/h4>
&lt;ol>
&lt;li>
&lt;p>删除针对 &lt;code>*.com&lt;/code> 的配置项：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete serviceentry mongo
$ kubectl delete gateway istio-egressgateway-with-sni-proxy
$ kubectl delete virtualservice direct-mongo-through-egress-gateway
$ kubectl delete destinationrule mtls-for-egress-gateway
$ kubectl delete envoyfilter forward-downstream-sni egress-gateway-sni-verifier
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>删除 &lt;code>egressgateway-with-sni-proxy&lt;/code> &lt;code>Deployment&lt;/code> 的配置项：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete serviceentry sni-proxy
$ kubectl delete destinationrule disable-mtls-for-sni-proxy
$ kubectl delete -f ./istio-egressgateway-with-sni-proxy.yaml
$ kubectl delete configmap egress-sni-proxy-configmap -n istio-system
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>删除您创建的配置文件：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ rm ./istio-egressgateway-with-sni-proxy.yaml
$ rm ./nginx-sni-proxy.conf
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="heading-2">清理&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>删除&lt;code>bookinfo&lt;/code>用户：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | mongo --ssl --sslAllowInvalidCertificates $MONGODB_HOST:$MONGODB_PORT -u admin -p $MONGO_ADMIN_PASSWORD --authenticationDatabase admin
use test
db.dropUser(&amp;#34;bookinfo&amp;#34;);
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>删除 &lt;code>ratings&lt;/code> 集合：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | mongo --ssl --sslAllowInvalidCertificates $MONGODB_HOST:$MONGODB_PORT -u admin -p $MONGO_ADMIN_PASSWORD --authenticationDatabase admin
use test
db.ratings.drop();
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>取消您使用的环境变量：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ unset MONGO_ADMIN_PASSWORD BOOKINFO_PASSWORD MONGODB_HOST MONGODB_PORT MONGODB_IP
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>删除 virtual services：&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/virtual-service-ratings-db.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete -f @samples/bookinfo/networking/virtual-service-ratings-db.yaml@
Deleted config: virtual-service/default/reviews
Deleted config: virtual-service/default/ratings
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;li>
&lt;p>删除 &lt;code>ratings v2-mongodb&lt;/code> deployment：&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-ratings-v2.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete -f @samples/bookinfo/platform/kube/bookinfo-ratings-v2.yaml@
deployment &amp;#34;ratings-v2&amp;#34; deleted
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;/ol>
&lt;h2 id="heading-3">总结&lt;/h2>
&lt;p>在这篇博文中，我演示了 MongoDB egress 流量控制的各种选项。您可以在 TCP 或 TLS 层面上控制 MongoDB egress 流量。根据您的组织的安全需求，在 TCP 和 TLS 场景下您都可以将流量从 sidecar 代理定向到外部 MongoDB 主机，或者通过一个 egress gateway 进行转发。在后面一种场景中，您还可以决定是否禁用 sidecar 代理到 egress gateway 的双向 TLS 认证。如果您想要通过指定类似 &lt;code>*.com&lt;/code> 的通配符域名来从 TLS 层面控制 MongoDB 的 egress 流量，并且通过 egress gateway 定向流量时，您必须部署一个使用 SNI 代理的自定义 egress gateway。&lt;/p>
&lt;p>请注意，本博客文章中描述的 MongoDB 配置和注意事项与 TCP/TLS 之上的其他非 HTTP 协议相同。&lt;/p></description><pubDate>Fri, 16 Nov 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/egress-mongo/</link><author>Vadim Eisenberg</author><guid isPermaLink="true">/v1.1/zh/blog/2018/egress-mongo/</guid><category>traffic-management</category><category>egress</category><category>tcp</category><category>mongo</category></item><item><title>Istio 1.0.3 发布</title><description>&lt;p>在此高兴地宣布：Istio 1.0.3 已经发布。请查看&lt;a href="/v1.1/zh/about/notes/1.0.3/">发行说明&lt;/a> 来了解和下载新版本。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.0.3"
data-updateadvice='Before you download 1.0.3, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.0.8'
data-updatehref="/v1.1/zh/about/notes/1.0.8">
1.0.3 下载
&lt;/button>
&lt;a class="btn" href="https://archive.istio.io/v1.0">1.0.3 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.0.2...1.0.3">1.0.3 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Tue, 30 Oct 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/announcing-1.0.3/</link><author>Istio 团队</author><guid isPermaLink="true">/v1.1/zh/blog/2018/announcing-1.0.3/</guid></item><item><title>Istio 1.0.2 发布</title><description>&lt;p>在此高兴地宣布：Istio 1.0.2 已经发布。请查看&lt;a href="/v1.1/zh/about/notes/1.0.2/">发行说明&lt;/a> 来了解和下载新版本。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.0.2"
data-updateadvice='Before you download 1.0.2, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.0.8'
data-updatehref="/v1.1/zh/about/notes/1.0.8">
1.0.2 下载
&lt;/button>
&lt;a class="btn" href="https://archive.istio.io/v1.0">1.0.2 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.0.1...1.0.2">1.0.2 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Thu, 06 Sep 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/announcing-1.0.2/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2018/announcing-1.0.2/</guid></item><item><title>Istio 1.0.1 发布</title><description>&lt;p>在此高兴地宣布：Istio 1.0.1 已经发布。请查看&lt;a href="/v1.1/zh/about/notes/1.0.1/">发行说明&lt;/a> 来了解和下载新版本。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.0.1"
data-updateadvice='Before you download 1.0.1, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.0.8'
data-updatehref="/v1.1/zh/about/notes/1.0.8">
1.0.1 下载
&lt;/button>
&lt;a class="btn" href="https://archive.istio.io/v1.0">1.0.1 文档&lt;/a>
&lt;a class="btn" href="https://github.com/istio/istio/compare/1.0.0...1.0.1">1.0.1 变更&lt;/a>
&lt;/div>
&lt;!-- raw HTML omitted --></description><pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/announcing-1.0.1/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2018/announcing-1.0.1/</guid></item><item><title>Istio 在 Twitch 上全天直播</title><description>&lt;p>为了庆祝 Istio 1.0 版本的发布并向更广泛的受众推广该软件，8月17日 Istio 社区在 Twitch 举办了一天的实况直播。&lt;/p>
&lt;h2 id="twitch-">Twitch 是什么？&lt;/h2>
&lt;p>&lt;a href="https://twitch.tv/">Twitch&lt;/a> 是一个流行的视频游戏直播流媒体平台，最近也看到了大量的编码内容出现。IBM 的倡导者一直在那里进行实况编码和演示，这是很有趣的事情。虽然大多数都是和游戏有关的内容，但是在网站上与编程相关的分享和观看的&lt;a href="https://www.twitch.tv/communities/programming">社区&lt;/a>在持续增长中。&lt;/p>
&lt;h2 id="-istio-">它用 Istio 做了什么？&lt;/h2>
&lt;p>Istio 在平台上发布的全天的内容，希望可以给观众讲解如何将深度技术内容、初级内容和业务线内容做良好融合。我们有开发人员、用户和布道者来分享示例和故事。期待现场编码，QA 和一些惊喜。我们有来自 IBM、Google、Datadog、Pivotal和更多的明星嘉宾。&lt;/p>
&lt;h2 id="heading">如何观看&lt;/h2>
&lt;p>很简单！只要在8月17日上午10点导航到&lt;a href="https://twitch.tv/ibmcode">这里&lt;/a>。&lt;/p>
&lt;h2 id="heading-1">安排&lt;/h2>
&lt;p>任何时候都是 &lt;code>PDT&lt;/code>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>时间&lt;/th>
&lt;th>演讲者&lt;/th>
&lt;th>从属关系&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>10:00 - 10:30&lt;/td>
&lt;td>&lt;code>Spencer Krum + Lisa-Marie Namphy&lt;/code>&lt;/td>
&lt;td>&lt;code>IBM / Portworx&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10:30 - 11:00&lt;/td>
&lt;td>&lt;code>Lin Sun / Spencer Krum / Sven Mawson&lt;/code>&lt;/td>
&lt;td>&lt;code>IBM / Google&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11:00 - 11:10&lt;/td>
&lt;td>&lt;code>Lin Sun / Spencer Krum&lt;/code>&lt;/td>
&lt;td>&lt;code>IBM&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11:10 - 11:30&lt;/td>
&lt;td>&lt;code>Jason Yee / Ilan Rabinovich&lt;/code>&lt;/td>
&lt;td>&lt;code>Datadog&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11:30 - 11:50&lt;/td>
&lt;td>&lt;code>April Nassl&lt;/code>&lt;/td>
&lt;td>&lt;code>Google&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11:50 - 12:10&lt;/td>
&lt;td>&lt;code>Spike Curtis&lt;/code>&lt;/td>
&lt;td>&lt;code>Tigera&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12:10 - 12:30&lt;/td>
&lt;td>&lt;code>Shannon Coen&lt;/code>&lt;/td>
&lt;td>&lt;code>Pivotal&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12:30 - 1:00&lt;/td>
&lt;td>&lt;code>Matt Klein&lt;/code>&lt;/td>
&lt;td>&lt;code>Lyft&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1:00 - 1:20&lt;/td>
&lt;td>&lt;code>Zach Jory&lt;/code>&lt;/td>
&lt;td>&lt;code>F5/Aspen Mesh&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1:20 - 1:40&lt;/td>
&lt;td>&lt;code>Dan Ciruli&lt;/code>&lt;/td>
&lt;td>&lt;code>Google&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1:40 - 2:00&lt;/td>
&lt;td>&lt;code>Isaiah Snell-Feikema&lt;/code> / &lt;code>Greg Hanson&lt;/code>&lt;/td>
&lt;td>&lt;code>IBM&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2:00 - 2:20&lt;/td>
&lt;td>&lt;code>Zach Butcher&lt;/code>&lt;/td>
&lt;td>&lt;code>Tetrate&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2:20 - 2:40&lt;/td>
&lt;td>&lt;code>Ray Hudaihed&lt;/code>&lt;/td>
&lt;td>&lt;code>American Airlines&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2:40 - 3:00&lt;/td>
&lt;td>&lt;code>Christian Posta&lt;/code>&lt;/td>
&lt;td>&lt;code>Red Hat&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3:00 - 3:20&lt;/td>
&lt;td>&lt;code>Google/IBM China&lt;/code>&lt;/td>
&lt;td>&lt;code>Google / IBM&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3:20 - 3:40&lt;/td>
&lt;td>&lt;code>Colby Dyess&lt;/code>&lt;/td>
&lt;td>&lt;code>Tuffin&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3:40 - 4:00&lt;/td>
&lt;td>&lt;code>Rohit Agarwalla&lt;/code>&lt;/td>
&lt;td>&lt;code>Cisco&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="heading-2">参加直播活动&lt;/h2>
&lt;p>我们希望你也能来。在 Istio slack 或 rocket chat 上联系 &lt;code>@nibalizer&lt;/code> 就可以开始了。&lt;/p></description><pubDate>Fri, 03 Aug 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/istio-twitch-stream/</link><author>Spencer Krum, IBM</author><guid isPermaLink="true">/v1.1/zh/blog/2018/istio-twitch-stream/</guid></item><item><title>宣布 Istio 1.0</title><description>&lt;p>今天，我们很高兴地宣布 &lt;a href="/v1.1/zh/about/notes/1.0">Istio 1.0&lt;/a>。这距离最初的 0.1 版本发布以来已经过了一年多时间了。从 0.1 起，Istio 就在蓬勃发展的社区、贡献者和用户的帮助下迅速成长。现在已经有许多公司成功将 Istio 应用于生产，并通过 Istio 提供的洞察力和控制力获得了真正的价值。我们帮助大型企业和快速发展的创业公司，如 &lt;a href="https://www.ebay.com/">eBay&lt;/a>、&lt;a href="https://www.autotrader.co.uk/">Auto Trader UK&lt;/a>、&lt;a href="http://www.descarteslabs.com/">Descartes Labs&lt;/a>、&lt;a href="https://www.fitstation.com/">HP FitStation&lt;/a>、&lt;a href="https://www.namely.com/">Namely&lt;/a>、&lt;a href="https://juspay.in">JUSPAY&lt;/a>、&lt;a href="https://www.pubnub.com/">PubNub&lt;/a> 和 &lt;a href="https://www.trulia.com/">Trulia&lt;/a> 使用 Istio 从头开始连接、管理和保护他们的服务。将此版本作为 1.0 发布是对我们核心功能的认可，用户们可以依赖这些功能进行生产。&lt;/p>
&lt;div class="call-to-action">
&lt;button class="btn update-notice"
data-title='Update Notice'
data-downloadhref="https://github.com/istio/istio/releases/tag/1.0.0"
data-updateadvice='Before you download 1.0, you should know that there&amp;#39;s a newer patch release with the latest bug fixes and perf improvements.'
data-updatebutton='LEARN ABOUT ISTIO 1.0.8'
data-updatehref="/v1.1/zh/about/notes/1.0.8">
1.0 下载
&lt;/button>
&lt;a class="btn" href="https://archive.istio.io/v1.0">1.0 文档&lt;/a>
&lt;a class="btn" href="/v1.1/zh/about/notes/1.0/">1.0 发布说明&lt;/a>
&lt;/div>
&lt;h2 id="heading">生态系统&lt;/h2>
&lt;p>去年，我们看到了 Istio 生态系统的大幅扩张。&lt;a href="https://www.envoyproxy.io/">Envoy&lt;/a> 继续其令人印象深刻的增长，并增加了许多对生产级别服务网格至关重要的功能。像 &lt;a href="https://www.datadoghq.com/">Datadog&lt;/a>、
&lt;a href="https://www.solarwinds.com/">SolarWinds&lt;/a>、 &lt;a href="https://sysdig.com/blog/monitor-istio/">Sysdig&lt;/a>、&lt;a href="https://cloud.google.com/stackdriver/">Google Stackdriver&lt;/a> 和 &lt;a href="https://aws.amazon.com/cloudwatch/">Amazon CloudWatch&lt;/a> 这样的可观察性提供商也编写了插件来将 Istio 与他们的产品集成在一起。&lt;a href="https://www.tigera.io/resources/using-network-policy-concert-istio-2/">Tigera&lt;/a>、&lt;a href="https://www.aporeto.com/">Aporeto&lt;/a>、&lt;a href="https://cilium.io/">Cilium&lt;/a>
和 &lt;a href="https://styra.com/">Styra&lt;/a> 为我们的策略实施和网络功能构建了扩展。&lt;a href="https://www.redhat.com/en">Red Hat&lt;/a> 构建的 Kiali 为网格管理和可观察性提供了良好的用户体验。&lt;a href="https://www.cloudfoundry.org/">Cloud Foundry&lt;/a> 正在为 Istio 建立下一代流量路由堆栈，最近宣布的 &lt;a href="https://github.com/knative/docs">Knative&lt;/a> 无服务器项目也正在做同样的事情，&lt;a href="https://apigee.com/">Apigee&lt;/a> 宣布计划在他们的 API 管理解决方案中使用它。这些只是社区去年增加的项目的一些汇总。&lt;/p>
&lt;h2 id="heading-1">功能&lt;/h2>
&lt;p>自 0.8 发布以来，我们添加了一些重要的新功能，更重要的是将许多现有的功能标记为 Beta，表明它们可以用于生产。这在&lt;a href="/v1.1/zh/about/notes/1.0/">发行说明&lt;/a>中有更详细的介绍，值得一提是：&lt;/p>
&lt;ul>
&lt;li>现在可以将多个 Kubernetes 集群&lt;a href="/v1.1/zh/docs/setup/kubernetes/install/multicluster">添加到单个网格中&lt;/a>，并启用跨集群通信和一致的策略实施。多集群支持现在处于 Beta 阶段。&lt;/li>
&lt;li>通过网格实现对流量的细粒度控制的网络 API 现在处于 Beta 阶段。使用网关显式地对 ingress 和 egress 进行建模，让运维人员能够&lt;a href="/v1.1/zh/blog/2018/v1alpha3-routing/">控制网络拓扑&lt;/a>并满足边缘访问的安全要求。&lt;/li>
&lt;li>现在可以&lt;a href="/v1.1/zh/docs/tasks/security/mtls-migration">增量上线&lt;/a>双向 TLS，而无需更新服务的所有客户端。这是一项关键功能，可以解除在现有生产上部署采用 Istio 的障碍。&lt;/li>
&lt;li>Mixer 现在支持&lt;a href="https://github.com/istio/istio/wiki/Out-Of-Process-gRPC-Adapter-Dev-Guide">开发进程外适配器&lt;/a>。进程外适配器的构建更加简单，在未来版本中，这是扩展 Mixer 的默认方式。&lt;/li>
&lt;li>现在，Envoy 在本地完全评估了控制服务访问的&lt;a href="/v1.1/zh/docs/concepts/security/#%E8%AE%A4%E8%AF%81%E7%AD%96%E7%95%A5">授权策略&lt;/a>，从而提高了它们的性能和可靠性。&lt;/li>
&lt;li>&lt;a href="/v1.1/zh/docs/setup/kubernetes/install/helm/">Helm chart 安装&lt;/a> 现在是推荐的安装方法，提供丰富的自定义选项，以便根据您的需求配置 Istio。&lt;/li>
&lt;li>我们在性能方面投入了大量精力，包括连续回归测试、大规模环境模拟和目标修复。我们对结果非常满意，并将在未来几周内详细分享。&lt;/li>
&lt;/ul>
&lt;h2 id="heading-2">下一步&lt;/h2>
&lt;p>虽然这是该项目的一个重要里程碑，但还有很多工作要做。在与采用者合作时，我们已经获得了很多关于下一步要关注的重要反馈。我们已经听到了关于支持混合云、安装模块化、更丰富的网络功能和大规模部署可扩展性的一致主题。我们在 1.0 版本中已经考虑到了一些反馈，在未来几个月内我们将继续积极地处理这些工作。&lt;/p>
&lt;h2 id="heading-3">快速开始&lt;/h2>
&lt;p>如果您是 Istio 的新手，并希望将其用于部署，我们很乐意听取您的意见。查看我们的&lt;a href="/v1.1/zh/docs/">文档&lt;/a>，访问我们的&lt;a href="https://istio.rocket.chat">聊天论坛&lt;/a>或&lt;a href="https://groups.google.com/forum/#!forum/istio-dev">邮件列表&lt;/a>。如果您想更深入地为该项目做出贡献，请参加我们的&lt;a href="/v1.1/zh/about/community">社区会议&lt;/a>并打个招呼。&lt;/p>
&lt;h2 id="heading-4">最后&lt;/h2>
&lt;p>Istio 团队非常感谢为项目做出贡献的每个人。没有你们的帮助，它不会有今天的成就。去年的成就非常惊人，我们期待未来与我们社区成员一起实现更伟大的成就。&lt;/p></description><pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/announcing-1.0/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2018/announcing-1.0/</guid></item><item><title>Istio 是惠普 FitStation 平台的改变者</title><description>&lt;p>惠普的 FitStation 团队坚信， 未来的 Kubernetes、BPF 和服务网络是云基础设施的下一个标准。
我们也很高兴看到 Istio 即将推出其官方的 Istio 1.0 版本 - 这要归功于 2017 年 5 月从 Google、IBM 和 Lyft 开始的联合合作。&lt;/p>
&lt;p>在 FitStation 大规模和渐进式云平台的开发过程中，Istio 、Cilium 和 Kubernetes 的技术提供了大量机会，使我们的系统更加健壮和易于扩展。
Istio 在创建可靠和动态的网络通信方面改变了游戏规则。&lt;/p>
&lt;p>&lt;a href="http://www.fitstation.com">由惠普提供支持的 FitStation&lt;/a> 是一个技术平台，可捕获 3D 生物识别数据，设计个性化鞋类，
完美贴合个人足部尺寸和形状以及步态轮廓。它使用 3D 扫描，压力感应，3D 打印和可变密度注塑成型来制造独特的鞋类。
Brooks、Steitz Secura 或 Superfeet 等鞋类品牌正在连接 FitStation，以打造下一代高性能运动鞋，专业鞋和医用鞋。&lt;/p>
&lt;p>FitStation 建立在对用户生物识别数据的最终安全性和隐私承诺的基础上。Istio 是我们在云中实现数据传输的基石。
通过在基础架构级别管理这些方面，我们专注于解决业务问题，而不是花时间在安全服务通信的单独实现上。
使用 Istio 可以大大降低维护大量库和服务的复杂性，从而提供安全的通信服务。&lt;/p>
&lt;p>作为使用 Istio 1.0 的额外好处，我们获得了网络可见性，指标和开箱即用的追踪。这极大地改善了我们开发和 DevOps 团队的的决策和响应质量
。团队深入了解包括新应用程序和传统应用程序在内的整个平台的网络通信。 Cilium 与 Envoy 的整合，
让 Istio 服务网格网络通信能力的性能取得了显著提升，同时还提供了内核驱动的细粒度 L7 网络安全层。这归功于 Cilium 为 Istio 提供的 BPF 功能。
我们认为未来将推动 Linux 的内核安全。&lt;/p>
&lt;p>跟随 Istio 的成长一直非常令人兴奋。我们已经能够看到不同开发版本的性能和稳定性的明显改进。
版本 0.7 和 0.8 之间的改进使我们的团队对 1.0 版本感到满意，我们可以说 Istio 现在已经准备好用于实际生产。&lt;/p>
&lt;p>我们期待着 Istio 、Envoy 、Cilium 和 CNCF 充满希望的路线图。&lt;/p></description><pubDate>Tue, 31 Jul 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/hp/</link><author>Steven Ceuppens, Chief Software Architect @ HP FitStation, Open Source Advocate &amp; Contributor</author><guid isPermaLink="true">/v1.1/zh/blog/2018/hp/</guid></item><item><title>使用 AppSwitch 精简 Istio 层次</title><description>&lt;div>
&lt;aside class="callout quote">
&lt;div class="type">
&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-quote"/>&lt;/svg>
&lt;/div>
&lt;div class="content">计算机科学中的所有问题，都可以用另一个层来解决，除了层数太多的问题。—— David Wheeler&lt;/div>
&lt;/aside>
&lt;/div>
&lt;p>Sidecar proxy 模式成就了很多奇迹。Sidecar 身处微服务的数据路径之中，能够精确的了解到应用程序的意图。它在应用程序级别而非网络层级别对流量进行监控和协议的检测，从而实现了深度的可见性、访问控制以及流量管理能力。&lt;/p>
&lt;p>如果我们仔细观察，在执行应用流量的高级分析之前，数据必须通过许多中间层。这些层中的大部分都是基础设施的一部分，是数据传输的管道。这样一来，就增加了数据通信的延迟，也提高了整个系统的复杂性。&lt;/p>
&lt;p>多年以来，在网络数据路径层内实现积极的细粒度优化方面，已经有了很多集体努力。每次迭代可能都节省了几个毫秒。但这些层本身的必要性却无人质疑。&lt;/p>
&lt;h2 id="heading">优化层还是删除层&lt;/h2>
&lt;p>在我看来，在对某些东西进行优化之前，应该先行考虑的是这个方面的需求是否可以取消。我最初的操作系统级虚拟化&lt;a href="https://apporbit.com/a-brief-history-of-containers-from-reality-to-hype/">工作目标&lt;/a>，就是&lt;a href="https://apporbit.com/a-brief-history-of-containers-from-reality-to-hype/">移除虚拟机&lt;/a>，用 Linux 容器直接在主机操作系统上运行应用，从而免除中间层的困扰。很长一段时间里，业界都在努力的对虚拟机进行优化，这是一场错误的战斗，真正应该做的是删除附加层。&lt;/p>
&lt;p>在微服务以及网络的连接方面，历史再次重演。网络已经经历了物理服务器十年前所经历的变化。新引进层和结构正被深入集成到了协议栈甚至是晶片之中，却没有人认真考虑替代的可能。也许移除这些附加层才是更好的办法。&lt;/p>
&lt;p>这个问题我已经思考了一段时间，我相信网络栈应该可以借鉴容器的做法，能够从基础上简化应用端点跨越复杂中间层进行连接的过程。我从容器的原始工作中总结的原则，也被应用到了创建 &lt;a href="http://appswitch.io">AppSwitch&lt;/a> 的过程之中。容器所提供的接口可以直接被应用拿来消费，AppSwitch 会直接插入到应用程序所用的定义良好的网络 API 之中，并跳过所有中间层，直接将应用程序的客户端连接到适当的服务器上。这才是网络的应有之意。&lt;/p>
&lt;p>在详细说明 AppSwitch 如何从 Istio 栈中清理无用层次之前，首先介绍一下产品架构。更多内容请移步浏览&lt;a href="https://appswitch.readthedocs.io/en/latest/">产品文档页面&lt;/a>。&lt;/p>
&lt;h2 id="appswitch">AppSwitch&lt;/h2>
&lt;p>和容器运行时相比，AppSwitch 由客户端和一个守护进程组成，二者通过 HTTP 协议的 REST API 进行通信。客户端和守护进程构建为一个自包含的二进制文件 &lt;code>ax&lt;/code>。客户端透明的注入s应用程序，并追踪网络相关的系统调用，随后通知守护进程。例如一个应用进行了 &lt;code>connect(2)&lt;/code> 系统调用，目标是一个 Kubernetes 服务的 IP。AppSwitch 客户端拦截这一调用并令其失效，然后把这一事件及其参数和上下文环境告知守护进程。守护进程会处理系统调用，例如代表应用程序直接连接到上游服务器的 Pod IP。&lt;/p>
&lt;p>值得注意的一点是，AppSwitch 的客户端和服务器之间不做任何数据转发。它们中间会通过 Unix socket 交换文件描述符，从而避免数据拷贝。另外客户端也不是独立进程，而是运行在应用本身的上下文之中的，因此应用和 AppSwitch 之间也不存在数据拷贝的操作。&lt;/p>
&lt;h2 id="heading-1">删减堆栈层&lt;/h2>
&lt;p>现在我们大概知道了 AppSwitch 的功能，接下来看看它从标准服务网格中优化掉的层。&lt;/p>
&lt;h3 id="heading-2">网络的去虚拟化&lt;/h3>
&lt;p>Kubernetes 为运行其上的微服务应用提供了简单的设计优良的网络结构。然而为了支持这些设计，也为下层网络强加了特定的&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">需求&lt;/a>。要符合这些需求并不简单。通常会通过另加一层的方式来满足要求。典型方案就是在下层网络和 Kubernetes 之间加入叠加层。应用产生的流量会在源头进行封包，在目的地进行解包，这一过程消耗的不仅是网络资源，还包括 CPU。&lt;/p>
&lt;p>AppSwitch 会通过和平台之间的接触，来决定应用程序的可见范围。它会为应用程序提供一个关于下层网络的虚拟视图，这一视图类似于叠加层，但是不会在数据路径中引入额外的处理工作。和容器的情况类似，容器内部看起来也像是一个虚拟机，但是其基础实现不会干扰低级中断之类的高发事件的控制过程。&lt;/p>
&lt;p>AppSwitch 能注入到标准的 Kubernetes 清单文件之中（和 Istio 注入类似），这样应用的网络就直接被 AppSwitch 控制，跳过任何的网络叠加过程。稍后介绍更多细节。&lt;/p>
&lt;h3 id="heading-3">容器网络的组件&lt;/h3>
&lt;p>将网络从主机扩展到容器是一个&lt;a href="https://kubernetes.io/blog/2016/01/why-kubernetes-doesnt-use-libnetwork/">巨大挑战&lt;/a>。新的网络层应运而生。容器中的进程只是主机上的一个进程。然而应用所期待的网络抽象和容器网络命名空间之间存在一个&lt;a href="http://appswitch.io/blog/kubernetes_istio_and_network_function_devirtualization_with_appswitch/">错位&lt;/a>，进程无法直接访问主机网络。应用程序眼里的网络是 Socket 或者 Session，而网络命名空间暴露的是设备抽象。一旦进入网络命名空间，进程会失去所有连接。为此发明了 &lt;code>veth-pair&lt;/code> 之类的工具用来弥合这一鸿沟。数据现在必须从主机接口进入虚拟交换机，然后通过 &lt;code>veth-pair&lt;/code> 才能进入容器网络空间里面的虚拟网络接口。&lt;/p>
&lt;p>AppSwitch 能够有效的移除连接两端的虚拟交换机和 &lt;code>veth-pair&lt;/code> 层。运行在主机上的守护进程所用的主机网络既然已经就绪，就无需再使用网桥方式把主机网络桥接到容器了。主机上创建的 Socket 文件描述符被传递给运行在 Pod 网络命名空间中的应用程序。应用收到 FD 之后，控制路径的所有工作都已就绪，就可以使用 FD 进行实际 IO 了。&lt;/p>
&lt;h3 id="-tcpip">跳过共生端点的 TCP/IP&lt;/h3>
&lt;p>TCP/IP 几乎是所有通信过程的媒介。如果恰好应用端点处于同一主机，还有必要继续使用 TCP/IP 么？毕竟 TCP/IP 会完成很多工作，并且非常复杂。Unix Socket 是为主机内通信设计的，AppSwitch 可以透明的将共生端点之间的通信切换到 Unix Socket 上。&lt;/p>
&lt;p>应用所监听的每个 Socket，AppSwitch 都会管理两个监听 Socket，一个对应 TCP，一个对应 Unix。当客户端尝试连接到的服务器恰好在同一主机，AppSwitch 守护进程就会选择连接到服务器的 Unix 监听 Socket 上。连接两端的 Unix Socket 被传递给相应的应用程序之中。返回完成的连接 FD 之后，应用会把它当做简单的字节管道。协议真的不重要。有个应用偶尔会做一些协议相关的调用，例如 &lt;code>getsockname(2)&lt;/code>，AppSwitch 会自行处理。它会提供一致的响应，保证程序持续运行。&lt;/p>
&lt;h3 id="heading-4">数据推送代理&lt;/h3>
&lt;p>我们一直在讨论移除层的问题，再回头看看代理层自身的需求。有时候代理服务器可能退化成为普通的数据推送装置：&lt;/p>
&lt;ul>
&lt;li>可能不需要协议解码。&lt;/li>
&lt;li>协议可能不被代理支持。&lt;/li>
&lt;li>通信过程是加密的，代理无法访问其中的 Header。&lt;/li>
&lt;li>应用（Redis、Memcached 等）对延迟非常敏感，无法忍受中间代理服务器的花销。&lt;/li>
&lt;/ul>
&lt;p>这些情况下，代理服务器和低层的管道层并无区别。实际上，代理无法对这种情况作出合适的优化，这种延迟可能会更高。&lt;/p>
&lt;p>举例说明，看看下图的应用。其中包含了一个 Python 应用以及一组 Memcached。根据连接时间进行路由，选择了一个 Memcached 作为上游服务器。速度是这里的首要考量。&lt;/p>
&lt;figure style="width:75%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:38.63965267727931%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/delayering-istio/memcached.png" title="延迟敏感的应用场景">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/delayering-istio/memcached.png" alt="无代理的数据路径" />
&lt;/a>
&lt;/div>
&lt;figcaption>延迟敏感的应用场景&lt;/figcaption>
&lt;/figure>
&lt;p>浏览图中的数据流，Python 应用发起了到 Memcached IP 的连接。该连接被重定向到客户端的 Sidecar。Sidecar 把连接路由到一个 Memcached 服务器，并在两个 Socket 之间复制数据——一端是应用，一端是 Memcached。同样的事情也发生在服务端的 Sidecar 和 Memcached 之间。这一场景之下，代理服务器的角色只是无聊的在两个 Socket 之间传送字节流，同时还给端到端连接造成了大量延迟。&lt;/p>
&lt;p>想象一下，如果应用跳过两个代理，直接连接到 Memcached。数据会在应用和 Memcached 服务之间直接流动。AppSwitch 在 Python 应用发起 &lt;code>connect(2)&lt;/code> 系统调用时会透明地对系统调用的目标地址进行调整。&lt;/p>
&lt;h3 id="heading-5">无代理的协议解码&lt;/h3>
&lt;p>这一节要谈的事情有点奇怪。在前面我们看到，在无需了解流量内容的情况下，可以跳过代理服务器。但是其它场景呢？也是可以的。&lt;/p>
&lt;p>在典型的微服务通信场景中，Header 中会有很多有用的信息。Header 之后会是信息本体或者负载，这些构成通信的主体。这里代理服务器再一次退行成为数据推送器。AppSwitch 为这些用例提供了一个花招，用于跳过代理服务器。&lt;/p>
&lt;p>虽说 AppSwitch 并非代理，它会对应用端点之间的连接进行仲裁，还能访问对应 Socket 的文件描述符。通常 AppSwitch 简单的把这些 FD 传给应用。但是它也可以使用 &lt;code>recvfrom(2)&lt;/code> 系统调用的 &lt;code>MSG_PEEK&lt;/code> 选项查看连接上收到的初始消息。这样 AppSwitch 就能在不从 Socket 缓冲区中取出信息的情况下获取流量的内容。当 AppSwitch 将 FD 发给应用并退出数据路径之后，应用程序才会对连接进行真正的读取。AppSwitch 使用这一技术，对应用级的流量进行深层分析，在不进入数据路径的前提下，实现下面将要谈到的复杂网络功能。&lt;/p>
&lt;h3 id="heading-6">零损耗的负载均衡器、防火墙和网络分析器&lt;/h3>
&lt;p>负载均衡器和防火墙这样的典型网络功能，通常的实现方式都是引入一个中间层，介入到数据/包之中。Kubernetes 的负载均衡器（&lt;code>kube-proxy&lt;/code>）实现利用 iptables 完成对数据包流的探测，Istio 也在代理层中实现了同样的功能。但是如果目标只是根据策略来对连接进行重定向或者丢弃，那么在整个连接过程中都介入到数据通路上是不必要的。AppSwitch 能够更有效的处理这些任务，只要简单的在 API 层面处理控制路径即可。AppSwitch 和应用紧密结合，因此还能够获取更多的应用信息，例如堆栈动态和堆利用情况、服务就绪时间以及活动连接属性等，这些信息可以为监控和分析提供更大发的操作空间。&lt;/p>
&lt;p>更进一步，AppSwitch 还能够利用从 Socket 缓冲区获取的协议数据，完成七层负载均衡和防火墙功能。它能够利用从 Pilot 获取的策略信息，合成协议数据和各种其它信号，从而实现高效的路由和访问控制能力。实际上，无需对应用程序自身或配置做出任何变动，AppSwitch 也可以“诱导”应用连接到正确的后端服务器。看起来好像应用程序本身就具备了策略和流量管理能力。&lt;/p>
&lt;p>实际上还存在一种可能的黑魔法就是，无需进入数据路径，也能够对应用的数据流进行修改，后面我会专门撰文描述这一功能。目前如果有对应用协议流量进行修改的需要，AppSwitch 当前的实现是使用一个代理，AppSwitch 使用一种高度优化的机制来完成代理任务，下一节将继续说明。&lt;/p>
&lt;h3 id="heading-7">流量重定向&lt;/h3>
&lt;p>Sidecar 代理要获知应用的协议流量，首先需要接入连接。利用包过滤层改写包，让数据包进入对应的 Sidecar，从而完成对应用程序进入和发出连接的重定向任务。要实现重定向规则，就意味着要编写大量规则，这是很繁琐的工作。Sidecar 捕获的目标子网发生变化时，规则的应用和更新也是很昂贵的操作。&lt;/p>
&lt;p>虽说 Linux 社区正在解决一些性能问题，但是还有特权相关的问题：不论何时策略发生变化，iptables 的规则都要随之更新。当前架构下，所有特权操作都是在初始化容器中执行，初始化容器只会在应用启动时执行一次，然后这一特权就会被删除；更新 iptables 规则需要 root 权限，所以如果不重新启动应用，则无法再次执行更新。&lt;/p>
&lt;p>AppSwitch 提供了无需 root 特权就能重定向应用连接的方法。这样一个无特权应用也能够连接任何主机。应用程序的所有者无需额外权限，就可以修改应用的 &lt;code>connect(2)&lt;/code> 调用时的主机地址。&lt;/p>
&lt;h4 id="socket-">Socket 委托&lt;/h4>
&lt;p>接下来看看 AppSwitch 如何在不使用 iptables 的情况下进行连接重定向。想象一下，应用程序能够以某种方式主动传递它用于与 Sidecar 通信的 Socket 文件描述符的话，就不需要 iptables 了。AppSwitch 提供了一种称为 &lt;strong>Socket 委托&lt;/strong>的机制用于完成这一任务。这个功能让 Sidecar 在不更改应用程序的情况下，透明的访问应用程序用于通信的 Socket 文件描述符的副本。&lt;/p>
&lt;p>举个例子，在我们的 Python 示例应用中用于完成这一目标的几个步骤。&lt;/p>
&lt;ol>
&lt;li>应用初始化一个到 memcached 服务 IP 的连接请求。&lt;/li>
&lt;li>客户端发出的连接请求被转发给守护进程。&lt;/li>
&lt;li>守护进程创建一对预连接的 Unix socket（用 &lt;code>socketpair(2)&lt;/code> 系统调用）。&lt;/li>
&lt;li>发送 Socket 对中的一端给应用，应用会用这个 FD 进行读写。它还要确保应用始终视其为合法 Socket，以便于侵入所有对连接属性的查询。&lt;/li>
&lt;li>另外一端会通过一个不同的用于开放守护进程 API 的 Unix Socket 发送给 Sidecar。原始目的之类的信息也会由相同的接口进行传输。&lt;/li>
&lt;/ol>
&lt;figure style="width:50%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:22.442748091603054%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/delayering-istio/socket-delegation.png" title="基于 Socket 委托的连接重定向">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/delayering-istio/socket-delegation.png" alt="Socket 委托协议" />
&lt;/a>
&lt;/div>
&lt;figcaption>基于 Socket 委托的连接重定向&lt;/figcaption>
&lt;/figure>
&lt;p>应用和 Sidecar 连接之后，接下来的事情就很普通了。Sidecar 初始化一个到上游服务器的连接，并在从守护进程接收到的 Socket 和连接到上游服务器的 Socket 之间充当数据代理。这里的主要区别在于，Sidecar 得到的连接不是通过 &lt;code>accept(2)&lt;/code> 系统调用而来的，而是由守护进程的 Unix socket 来的。Sidecar 不再通过监听来自应用的 &lt;code>accept(2)&lt;/code> 通道，而是连接到 AppSwitch 守护进程的 REST 端点获取到的 Socket。&lt;/p>
&lt;p>为了完整叙述，再说说服务端发生的事情：&lt;/p>
&lt;ol>
&lt;li>应用接收到一个连接。&lt;/li>
&lt;li>AppSwitch 守护进程代应用程序接受这个连接。&lt;/li>
&lt;li>用 &lt;code>socketpair(2)&lt;/code> 系统调用创建一对预连接的 Unix Socket。&lt;/li>
&lt;li>Socket 对的一端通过 &lt;code>accept(2)&lt;/code> 系统调用返回给应用。&lt;/li>
&lt;li>Socket 对的另外一端会和守护进程以应用程序身份接收的 Socket 一起发送给 Sidecar。&lt;/li>
&lt;li>Sidecar 会解开这两个 Socket FD - 一个 Unix Socket FD 连接到应用，另一个 TCP Socket FD 连接到远程客户端。&lt;/li>
&lt;li>Sidecar 会读取守护进程提供的关于远程客户端的元数据，并执行正常操作。&lt;/li>
&lt;/ol>
&lt;h4 id="sidecar-">Sidecar 感知的应用&lt;/h4>
&lt;p>Socket 委托功能对于知晓 Sidecar 存在并希望利用其能力的应用非常有用。应用程序可以通过相同的功能，把 Socket 传递给 Sidecar，委托其进行网络交互。一定程度上，AppSwitch 透明的把每个应用都转换成为了 Sidecar 感知应用。&lt;/p>
&lt;h2 id="heading-8">如何整合这些功能？&lt;/h2>
&lt;p>退一步看，Istio 把应用程序的连接问题转嫁给 Sidecar，由 Sidecar 代表应用执行这些功能。AppSwitch 对这些服务网格进行了简化和优化，绕过了中间层，仅在确实需要时才调用代理。&lt;/p>
&lt;p>接下来讲讲 AppSwitch 可以如何初步集成到 Istio 之中。这不是设计文档，其中涉及到的可能的集成方式并没有经过完全的验证，一些细节还没能解决。这里的尝试是一个大致的将两个系统组合在一起的概要。在这一方案中 AppSwitch 作为一个类似垫片的东西出现在 Istio（应该是网格内应用——译者注）和真正的代理之间。它会作为一个快速通路，用绕过 Sidecar 代理的方式更高效的运作。对于需要使用代理的场合，也会通过移除层的方式缩短数据路径。这篇&lt;a href="http://appswitch.io/blog/kubernetes_istio_and_network_function_devirtualization_with_appswitch/">博客&lt;/a>中记录了更多这方面的细节。&lt;/p>
&lt;h3 id="appswitch-">AppSwitch 的客户端注入&lt;/h3>
&lt;p>和 Istio 的 sidecar-injector 类似，AppSwitch 提供了一个 &lt;code>ax-injector&lt;/code> 工具用来把 AppSwitch 客户端注入到标准的 Kubernetes 清单文件中。被注入的客户端会透明的监测应用，并把应用程序生成的控制路径上的网络 API 事件报告给 AppSwitch 守护进程。&lt;/p>
&lt;p>如果使用了 AppSwitch CNI 插件，还可能仅使用标准 Kubernetes 清单文件，无需注入。这种情况下，CNI 插件会在获得初始化回调时完成必要的注入任务。注入器有以下优点：&lt;/p>
&lt;ol>
&lt;li>可以在 GKE 这样的的严格受控的环境中工作。&lt;/li>
&lt;li>可以轻松的扩展到其它平台，例如 Mesos。&lt;/li>
&lt;li>同一个集群中可以同时运行标准应用以及启用 AppSwitch 支持的应用。&lt;/li>
&lt;/ol>
&lt;h3 id="appswitch-daemonset">AppSwitch &lt;code>DaemonSet&lt;/code>&lt;/h3>
&lt;p>AppSwitch 守护进程可以配置成 &lt;code>DaemonSet&lt;/code> 的运行方式，也可以作为直接注入应用程序清单的扩展。两种方式下都能够处理来自受支持应用的网络事件。&lt;/p>
&lt;h3 id="-agent">用于获取策略的 Agent&lt;/h3>
&lt;p>这一组件用来将 Istio 的配置和策略转达给 AppSwitch。它实现了 xDS API，用来监听 Pilot，并调用对应的 AppSwitch API，来完成守护进程的程控。例如可以把 &lt;code>istioctl&lt;/code> 制定的负载均衡策略翻译成等效的 AppSwitch 能力。&lt;/p>
&lt;h3 id="appswitch--1">AppSwitch 服务注册表的平台适配器&lt;/h3>
&lt;p>AppSwitch 是存在于应用网络 API 的控制路径上的，因此也就具备了访问集群上服务拓扑的能力。AppSwitch 用服务注册表的形式公布信息，这个注册表会随着应用和服务的变化来自动的进行同步更新。Kubernetes 之外的平台适配器，例如 Eureka，会为 Istio 提供上游服务的详细信息。这虽然并非必要，但更有助于上面提到的 AppSwitch Agent 关联从 Pilot 接收到的端点信息。&lt;/p>
&lt;h3 id="heading-9">代理集成和链路&lt;/h3>
&lt;p>通过前面讨论过的 Socket 委托机制，能够将需要深度扫描和应用程序流量突变的连接传递给外部代理。这一过程使用了一个扩展的&lt;a href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">代理服务器协议&lt;/a>。在简单的代理协议参数之外，加入了其它的元数据（包括从 Socket 缓冲区中获取的协议 Header）以及活跃的 Socket FD（代表应用的连接），转发给代理服务器。&lt;/p>
&lt;p>代理服务器取得元数据之后会进行处理决策。可能接受连接执行代理任务，也可能重定向给 AppSwitch 从而把连接送入快速通道，当然，还有可能直接丢弃连接。&lt;/p>
&lt;p>这个机制中有一点比较有趣，当代理从 AppSwitch 接收一个 Socket 的时候，它可以把这个委托转交给其它代理。实际上 AppSwitch 目前就是这么做的。它会用一个简单的内置代理来检查元数据，然后决定在内部处理连接，还是交出去给外部代理（Envoy）。这种机制可以扩展为插件链条，每个环节都
在其中查找特定签名，链条最后一节完成真正的代理工作。&lt;/p>
&lt;h2 id="heading-10">不仅仅是性能&lt;/h2>
&lt;p>从数据路径上移除中间层，不仅仅意味着性能的提高。性能提高是好事，但这仅仅是一个副产品。在 API 级别上，有更多的重要提升。&lt;/p>
&lt;h3 id="heading-11">应用接入和策略生成的自动化&lt;/h3>
&lt;p>在微服务和服务网格之前，流量管理由负载均衡器完成，而访问控制则由防火墙完成。通过 IP 地址和 相对静态的 DNS 名称来鉴别应用。实际上这仍然是当前大多数环境的现状。这样的环境将从服务网格中受益匪浅。相对于新功能的开发来说，转型难度并不高，但是需要对整个基础设施进行重新思考和重新实现，这就需要投入了。目前多数策略和配置存在于负载均衡和防火墙中，现存的上下文需要有一个可扩展的路径来完成到服务网格模型的过渡。&lt;/p>
&lt;p>AppSwitch 能够大大简化接入流程。它可以把应用程序源环境投射到目标环境。如果传统应用的配置文件包含了硬编码的 IP 地址或者 DNS 名称，通常是难于迁移的。AppSwitch 可以协助捕获这些应用及其配置，在无需更改的情况下将其接入服务网格。&lt;/p>
&lt;h3 id="heading-12">更大范围的应用和协议支持&lt;/h3>
&lt;p>众所周知，HTTP 是的现代应用程序领域的主导协议，但是一旦涉及传统应用和环境，我们会遇到各种协议和传输方式。有时候连 UDP 的支持都是必要选项，例如 IBM 的 WebSphere 就广泛的依赖 UDP，多数多媒体应用也在使用 UDP 媒体流。当然，DNS 可能是最多使用的 UDP 应用。AppSwitch 在 API 级别为 UDP 提供了和 TCP 非常相似的支持，它检测到 UDP 连接之后，会透明的在快速路径中进行处理，而不是委托给代理。&lt;/p>
&lt;h3 id="-ip-">保留客户端 IP 以及端到端原则&lt;/h3>
&lt;p>和保留源网络环境的机制类似，同样也可以保留服务器视角的客户端 IP 地址。在 Sidecar 的干扰下，连接通常是来自于 Sidecar 而非客户端，这样服务端应用看到的对端地址就被替换为代理服务器的 IP。AppSwitch 能让服务器看到客户端的真实地址，进行正确的记录，并且能够通过该地址进行准确的决策。另外 AppSwitch 保留了&lt;a href="https://en.wikipedia.org/wiki/End-to-end_principle">端到端原则&lt;/a>，中间层的存在会打破这一规则，并对真正的底层上下文造成混淆。&lt;/p>
&lt;h3 id="-header-">访问加密 Header 以增强应用信号&lt;/h3>
&lt;p>加密流量会阻止服务网格对流量的分析。API 级别的介入提供了一种可行的解决方法。AppSwitch 目前的实现能够在系统调用层面获得对应用网络 API 的访问。然而还有进一步的可能，在应用尚未加密或者已经加密的高级 API 边界上对应用程序施加影响。最终的视角上，应用生成明文数据，在发出之前的某个时间点进行加密。既然 AppSwitch 运行在应用的内存上下文中，因此就可能在更高层的数据中获取到明文。当然要完成这种功能，应用需要进行明确定义并且适合介入。同时这一功能还要访问应用二进制文件的符号表。目前 AppSwitch 还没有实现这一功能。&lt;/p>
&lt;h2 id="heading-13">所以收益如何？&lt;/h2>
&lt;p>AppSwitch 从标准服务网格中移除了一组层次和操作。到底会对性能造成什么影响？&lt;/p>
&lt;p>我们做了一些初级的实验，来对前面提到的 AppSwitch 集成方式在提高性能方面的优化进行定性。这个实验运行在 GKE 上，对应的软件系统包括 Fortio 0.11.0、Istio 0.8.0 以及 AppSwitch 0.4.0-2。在无代理测试中，AppSwitch 守护进程以 &lt;code>DaemonSet&lt;/code> 的形式运行在 Kubernetes 集群中，并给 Fortio Pod 注入了 AppSwitch 客户端。这是仅有的两个步骤。这个测试的目的是衡量 100 并发连接的情况下，GRPC 的延迟情况。&lt;/p>
&lt;figure style="width:100%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:54.66034755134282%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/delayering-istio/perf.png" title="有无 AppSwitch 的对比。">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/delayering-istio/perf.png" alt="性能对比" />
&lt;/a>
&lt;/div>
&lt;figcaption>有无 AppSwitch 的对比。&lt;/figcaption>
&lt;/figure>
&lt;p>初步显示，p50 延迟在有无 AppSwitch 的情况下有高达 18 倍的差距（3.99 毫秒 vs 72.96 毫秒）。如果禁用了日志和 Mixer，差距会缩减为 8 倍。很明显，这一差距就是因为数据路径上的多余层造成的。客户端和服务器分属两台不同主机，因此 Unix Socket 优化在这一场景上没有触发，有理由相信，如果客户端和服务器恰好在同一节点上，延迟会进一步缩小。究其根本，在 Kubernetes 上各自 Pod 中运行的服务器和客户端是通过 GKE 网络上的 TCP Socket 直接连接的——没有隧道、网桥或者代理。&lt;/p>
&lt;h2 id="net-net">Net Net&lt;/h2>
&lt;p>从 David Wheeler 的引言开始说到，另起一层并非解决层次过多问题的方案。我的博客中经常提到，目前的网络栈已经层次太多，应该精简，但是 AppSwitch 是不是又加了一层？&lt;/p>
&lt;p>是的，AppSwitch 的确是另外一层。然而它的存在，能够移除更多层。这样一来，就把新的服务网格层和传统的网络层无缝的结合在一起。AppSwitch 不但抵消了 Sidecar 的成本，并且随着 Istio 1.0 的到来，还提供了一个从现有应用及其网络环境过度到服务网格世界的桥梁。&lt;/p>
&lt;p>可能 Wheeler 的引言可以换个说法：&lt;/p>
&lt;div>
&lt;aside class="callout quote">
&lt;div class="type">
&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-quote"/>&lt;/svg>
&lt;/div>
&lt;div class="content">计算机科学中的所有问题，都可以用另一个层来解决，&lt;strong>即使&lt;/strong>是层数太多的问题。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;h2 id="heading-14">感谢&lt;/h2>
&lt;p>感谢 Mandar Jog（Google）进行了多次沟通，讨论 AppSwitch 对 Istio 的存在价值。同时也要感谢对本文稿件进行 Review 的几位朋友（以字母排序）：&lt;/p>
&lt;ul>
&lt;li>Frank Budinsky (IBM)&lt;/li>
&lt;li>Lin Sun (IBM)&lt;/li>
&lt;li>Shriram Rajagopalan (VMware)&lt;/li>
&lt;/ul></description><pubDate>Mon, 30 Jul 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/delayering-istio/</link><author>Dinesh Subhraveti (AppOrbit and Columbia University)</author><guid isPermaLink="true">/v1.1/zh/blog/2018/delayering-istio/</guid><category>appswitch</category><category>performance</category></item><item><title>基于 Istio 的 Micro-Segmentation 授权</title><description>&lt;p>Micro-Segmentation 是一种安全技术，可在云部署中创建安全区域，并允许组织使用将工作负载彼此隔离并单独保护它们。
&lt;a href="/v1.1/zh/docs/concepts/security/#%E6%8E%88%E6%9D%83">Istio 的授权功能&lt;/a>也称为 Istio 基于角色的访问控制，为 Istio 网格中的服务提供
Micro-Segmentation。它的特点是：&lt;/p>
&lt;ul>
&lt;li>不同粒度级别的授权、包括命名空间级别、服务级别和方法级别。&lt;/li>
&lt;li>服务间和最终用户到服务授权。&lt;/li>
&lt;li>高性能，因为它在 Envoy 上本地执行。&lt;/li>
&lt;li>基于角色的语义，使其易于使用。&lt;/li>
&lt;li>灵活性高，因为它允许用户定义条件&lt;a href="/v1.1/docs/reference/config/authorization/constraints-and-properties/">属性组合&lt;/a>。&lt;/li>
&lt;/ul>
&lt;p>在这篇博客文章中，您将了解主要授权功能以及如何在不同情况下使用它们。&lt;/p>
&lt;h2 id="heading">特点&lt;/h2>
&lt;h3 id="rpc-">RPC 级别授权&lt;/h3>
&lt;p>授权在各个 RPC 级别执行。具体来说，它控制“谁可以访问我的 &lt;code>bookstore&lt;/code> 服务”，或者“谁可以在我的 &lt;code>bookstore&lt;/code> 服务中访问
&lt;code>getBook&lt;/code> 方法 ”。它不是为了控制对特定于应用程序的访问而设计的
资源实例，例如访问“存储桶 X ”或访问“第二层架上的第 3 本书”。今天这种应用特定的访问控制逻辑需要由应用程序本身处理。&lt;/p>
&lt;h3 id="heading-1">具有条件的基于角色的访问控制&lt;/h3>
&lt;p>授权是&lt;a href="https://en.wikipedia.org/wiki/Role-based_access_control">基于角色的访问控制（RBAC）&lt;/a>系统，
将此与&lt;a href="https://en.wikipedia.org/wiki/Attribute-based_access_control">基于属性的访问控制（ABAC）&lt;/a>对比系统。
与 ABAC 相比，RBAC 具有以下优势：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>角色允许对属性进行分组。&lt;/strong> 角色是权限组，用于指定允许的操作在系统上执行。用户根据组织内的角色进行分组。
您可以针对不同的情况定义角色并重用他们。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>关于谁有权访问，更容易理解和推理。&lt;/strong> RBAC 概念自然地映射到业务概念。例如，数据库管理员可能拥有对数据库后端服务的所有访问权限，
而 Web 客户端可能只能查看数据库后端服务前端服务。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>它减少了无意的错误。&lt;/strong> RBAC 策略使得复杂的安全更改变得更加容易。你不会有在多个位置重复配置，以后在需要进行更改时忘记更新其中一些配置。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>另一方面，Istio 的授权系统不是传统的 RBAC 系统。它还允许用户使用定义&lt;strong>条件&lt;/strong>&lt;a href="/v1.1/docs/reference/config/authorization/constraints-and-properties/">属性组合&lt;/a>。这给了 Istio 表达复杂的访问控制策略的灵活性。实际上，**Istio 授权采用“RBAC + 条件”模型，具有 RBAC 系统的所有优点，并支持通常是 ABAC 系统提供的灵活性。**你会在下面看到一些&lt;a href="#%E7%A4%BA%E4%BE%8B">示例&lt;/a>。&lt;/p>
&lt;h3 id="heading-2">高性能&lt;/h3>
&lt;p>由于其简单的语义， 在 Envoy 本地上执行 Istio 授权。在运行时，授权决策完全在 Envoy 过滤器内部完成，不依赖于任何外部模块。
这允许 Istio 授权实现高性能和可用性。&lt;/p>
&lt;h3 id="heading-3">使用/不使用主要标识&lt;/h3>
&lt;p>与任何其他 RBAC 系统一样，Istio 授权具有身份识别功能。在 Istio 授权政策中，有一个主要的
身份称为 &lt;code>user&lt;/code>，代表客户的主体。&lt;/p>
&lt;p>除主要标识外，您还可以自己定义标识。例如，您可以将客户端标识指定为“用户 &lt;code>Alice&lt;/code> 从 &lt;code>Bookstore&lt;/code> 前端服务调用”，在这种情况下，
你有一个调用服务（&lt;code>Bookstore frontend&lt;/code>）和最终用户（&lt;code>Alice&lt;/code>）的组合身份。&lt;/p>
&lt;p>要提高安全性，您应该启用&lt;a href="/v1.1/zh/docs/concepts/security/#%E8%AE%A4%E8%AF%81">认证功能&lt;/a>,并在授权策略中使用经过验证的身份。但是，
使用授权不强迫一定要有身份验证。Istio 授权可以使用或不使用身份。如果您正在使用遗留系统，您可能没有网格的双向 TLS 或 JWT 身份验证
设置。在这种情况下，识别客户端的唯一方法是，例如，通过 IP。您仍然可以使用 Istio 授权来控制允许哪些 IP 地址或 IP 范围访问您的服务。&lt;/p>
&lt;h2 id="heading-4">示例&lt;/h2>
&lt;p>&lt;a href="/v1.1/zh/docs/tasks/security/authz-http/">授权任务&lt;/a>向您展示如何操作使用 Istio 的授权功能来控制命名空间级别
和服务级别访问 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Bookinfo 应用&lt;/a>。在本节中，您将看到有关如何实现的更多示例使用 Istio 授权进行
微分割。&lt;/p>
&lt;h3 id="-rbac---segmentation">通过 RBAC + 条件进行命名空间级别 segmentation&lt;/h3>
&lt;p>假设你在 &lt;code>frontend&lt;/code> 和 &lt;code>backend&lt;/code> 命名空间中有服务。您想要允许所有服务在 &lt;code>frontend&lt;/code> 命名空间中访问 &lt;code>backend&lt;/code> 命名空间中标记
为 &lt;code>external&lt;/code> 的所有服务。&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: &amp;#34;rbac.istio.io/v1alpha1&amp;#34;
kind: ServiceRole
metadata:
name: external-api-caller
namespace: backend
spec:
rules:
- services: [&amp;#34;*&amp;#34;]
methods: [&amp;#34;*”]
constraints:
- key: &amp;#34;destination.labels[visibility]”
values: [&amp;#34;external&amp;#34;]
---
apiVersion: &amp;#34;rbac.istio.io/v1alpha1&amp;#34;
kind: ServiceRoleBinding
metadata:
name: external-api-caller
namespace: backend
spec:
subjects:
- properties:
source.namespace: &amp;#34;frontend”
roleRef:
kind: ServiceRole
name: &amp;#34;external-api-caller&amp;#34;
&lt;/code>&lt;/pre>
&lt;p>上面的 &lt;code>ServiceRole&lt;/code> 和 &lt;code>ServiceRoleBinding&lt;/code> 表示“允许&lt;em>谁&lt;/em> 在 &lt;em>条件&lt;/em>下执行&lt;em>什么&lt;/em> ”
（RBAC +条件）。特别：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>“谁”&lt;/strong> 是 &lt;code>frontend&lt;/code> 命名空间中的服务。&lt;/li>
&lt;li>&lt;strong>“什么”&lt;/strong> 是在 &lt;code>backend&lt;/code> 命名空间中调用服务。&lt;/li>
&lt;li>&lt;strong>“条件”&lt;/strong> 是具有值 &lt;code>external&lt;/code> 的目标服务的 &lt;code>visibility&lt;/code> 标签。&lt;/li>
&lt;/ul>
&lt;h3 id="heading-5">具有/不具有主要身份的服务/方法级别隔离&lt;/h3>
&lt;p>这是演示另一个服务/方法级别的细粒度访问控制的示例。第一步是定义一个 &lt;code>book-reader&lt;/code> &lt;code>ServiceRole&lt;/code>，它允许对 &lt;code>bookstore&lt;/code> 服务中的 &lt;code>/books/*&lt;/code> 资源进行 READ 访问。&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: &amp;#34;rbac.istio.io/v1alpha1&amp;#34;
kind: ServiceRole
metadata:
name: book-reader
namespace: default
spec:
rules:
- services: [&amp;#34;bookstore.default.svc.cluster.local&amp;#34;]
paths: [&amp;#34;/books/*”]
methods: [&amp;#34;GET”]
&lt;/code>&lt;/pre>
&lt;h4 id="heading-6">使用经过身份验证的客户端身份&lt;/h4>
&lt;p>假设你想把这个 &lt;code>book-reader&lt;/code> 角色授予你的 &lt;code>bookstore-frontend&lt;/code> 服务。如果您已启用
您的网格的&lt;a href="/v1.1/zh/docs/concepts/security/#%E5%8F%8C%E5%90%91-tls-%E8%AE%A4%E8%AF%81">双向 TLS 身份验证&lt;/a>,您可以使用服务帐户，以识别您的 &lt;code>bookstore-frontend&lt;/code> 服务。授予 &lt;code>book-reader&lt;/code> 角色到 &lt;code>bookstore-frontend&lt;/code> 服务可以通过创建一个 &lt;code>ServiceRoleBinding&lt;/code> 来完成，如下所示：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: &amp;#34;rbac.istio.io/v1alpha1&amp;#34;
kind: ServiceRoleBinding
metadata:
name: book-reader
namespace: default
spec:
subjects:
- user: &amp;#34;cluster.local/ns/default/sa/bookstore-frontend”
roleRef:
kind: ServiceRole
name: &amp;#34;book-reader&amp;#34;
&lt;/code>&lt;/pre>
&lt;p>您可能希望通过添加“仅属于 &lt;code>qualified-reviewer&lt;/code> 组的用户的条件来进一步限制此操作允许阅读书籍“。&lt;code>qualified-reviewer&lt;/code> 组是经过身份验证的最终用户身份 &lt;a href="/v1.1/zh/docs/concepts/security/#%E8%AE%A4%E8%AF%81">JWT 身份验证&lt;/a>。在这种情况下，客户端服务标识的组合（&lt;code>bookstore-frontend&lt;/code>）和最终用户身份（&lt;code>qualified-reviewer&lt;/code>）在授权策略中使用。&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: &amp;#34;rbac.istio.io/v1alpha1&amp;#34;
kind: ServiceRoleBinding
metadata:
name: book-reader
namespace: default
spec:
subjects:
- user: &amp;#34;cluster.local/ns/default/sa/bookstore-frontend”
properties:
request.auth.claims[group]: &amp;#34;qualified-reviewer”
roleRef:
kind: ServiceRole
name: &amp;#34;book-reader&amp;#34;
&lt;/code>&lt;/pre>
&lt;h4 id="heading-7">无身份客户&lt;/h4>
&lt;p>强烈建议在授权策略中使用经过身份验证的身份以确保安全性。但是，如果你有一个如果遗留系统不支持身份验证，您可能没有经过身份验证的身份验证。即使没有经过身份验证的身份，您仍然可以使用 Istio 授权来保护您的服务。以下示例表明您可以在授权策略中指定允许的源 IP 范围。&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: &amp;#34;rbac.istio.io/v1alpha1&amp;#34;
kind: ServiceRoleBinding
metadata:
name: book-reader
namespace: default
spec:
subjects:
- properties:
source.ip: 10.20.0.0/9
roleRef:
kind: ServiceRole
name: &amp;#34;book-reader&amp;#34;
&lt;/code>&lt;/pre>
&lt;h2 id="heading-8">概要&lt;/h2>
&lt;p>Istio 在命名空间级别，服务级别和方法级别粒度上提供授权功能。它采用“ RBAC + 条件”模型，使其成为易于使用和理解的 RBAC 系统，同时提供 ABAC 系统级别的灵活性。在 Envoy 本地上会执行 Istio 授权。虽然它通过与一起提供最好的安全性 &lt;a href="/v1.1/zh/docs/concepts/security/#%E8%AE%A4%E8%AF%81">Istio 认证功能&lt;/a>,也可以使用 Istio 授权为没有身份验证的旧系统提供访问控制。&lt;/p></description><pubDate>Fri, 20 Jul 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/istio-authorization/</link><author>Limin Wang</author><guid isPermaLink="true">/v1.1/zh/blog/2018/istio-authorization/</guid><category>authorization</category><category>rbac</category><category>security</category></item><item><title>通过 Stackdriver 将日志导出到 BigQuery、GCS、Pub/Sub</title><description>&lt;p>这篇文章展示了如何将 Istio 日志指向 &lt;a href="https://cloud.google.com/stackdriver/">&lt;code>Stackdriver&lt;/code>&lt;/a> 并将这些日志导出到各种配置的接收器，例如 &lt;a href="https://cloud.google.com/bigquery/">&lt;code>BigQuery&lt;/code>&lt;/a>、&lt;a href="https://cloud.google.com/storage/">&lt;code>Google Cloud Storage(GCS)&lt;/code>&lt;/a> 或 &lt;a href="https://cloud.google.com/pubsub/">&lt;code>Cloud Pub/Sub&lt;/code>&lt;/a>。在这篇文章的最后，可以从喜欢的地方（如 BigQuery、GCS 或 Cloud Pub/Sub）对 Istio 数据进行分析。&lt;/p>
&lt;p>&lt;a href="/v1.1/zh/docs/examples/bookinfo/">&lt;code>Bookinfo&lt;/code>&lt;/a> 示例应用程序在整个任务中用作示例应用程序。&lt;/p>
&lt;h2 id="heading">开始前&lt;/h2>
&lt;p>在集群中&lt;a href="/v1.1/zh/docs/setup/">&lt;code>安装 Istio&lt;/code>&lt;/a> 并部署应用程序。&lt;/p>
&lt;h2 id="-istio-">配置 Istio 导出日志&lt;/h2>
&lt;p>Istio 使用 &lt;code>logentry&lt;/code> &lt;a href="/v1.1/docs/reference/config/policy-and-telemetry/templates/logentry">&lt;code>模板&lt;/code>&lt;/a>导出日志。这里指定了可用于分析的所有变量。它包含源服务、目标服务、&lt;code>auth&lt;/code> 指标（即将实现&amp;hellip;&amp;hellip;）等信息。以下是示意图：&lt;/p>
&lt;figure style="width:75%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:75%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/export-logs-through-stackdriver/istio-analytics-using-stackdriver.png" title="导出日志到 Stackdriver 进行分析的图释">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/export-logs-through-stackdriver/istio-analytics-using-stackdriver.png" alt="导出日志到 Stackdriver 进行分析的图释" />
&lt;/a>
&lt;/div>
&lt;figcaption>导出日志到 Stackdriver 进行分析的图释&lt;/figcaption>
&lt;/figure>
&lt;p>Istio 支持将日志导出到 Stackdriver，而 Stackdriver 又可以配置为将日志导出到喜欢的接收器，如 BigQuery、Pub/Sub 或 GCS。请按照以下步骤设置喜欢的接收器，首先导出日志，然后在 Istio 中使用 Stackdriver。&lt;/p>
&lt;h3 id="heading-1">设置各种日志接收器&lt;/h3>
&lt;p>所有接收器的通用设置：&lt;/p>
&lt;ol>
&lt;li>为项目启动 &lt;a href="https://cloud.google.com/monitoring/api/enable-api">&lt;code>Stackdriver Monitoring API&lt;/code>&lt;/a> 。&lt;/li>
&lt;li>确保配置的接收器的 &lt;code>principalEmail&lt;/code> 具有对项目写入权限和日志管理员角色的权限。&lt;/li>
&lt;li>确保已设置 &lt;code>GOOGLE_APPLICATION_CREDENTIALS&lt;/code> 环境变量。请按照&lt;a href="https://cloud.google.com/docs/authentication/getting-started">&lt;code>此处&lt;/code>&lt;/a>的说明进行设置。&lt;/li>
&lt;/ol>
&lt;h4 id="bigquery">BigQuery&lt;/h4>
&lt;ol>
&lt;li>&lt;a href="https://cloud.google.com/bigquery/docs/datasets">&lt;code>创建 BigQuery 数据集&lt;/code>&lt;/a>作为日志导出的目标。&lt;/li>
&lt;li>记录数据集的 ID。 这里需要设置 Stackdriver 处理程序。它的格式为 &lt;code>bigquery.googleapis.com/projects/[PROJECT_ID]/datasets/[DATASET_ID]&lt;/code>&lt;/li>
&lt;li>给&lt;a href="https://cloud.google.com/logging/docs/api/tasks/exporting-logs#writing_to_the_destination">&lt;code>接收器授权&lt;/code>&lt;/a>：cloud-logs@system.gserviceaccount.com。它具有 IAM 中的 BigQuery Data Editor 的角色。&lt;/li>
&lt;li>如果使用 &lt;a href="/v1.1/zh/docs/setup/kubernetes/prepare/platform-setup/gke/">&lt;code>Google Kubernetes Engine&lt;/code>&lt;/a>，请确保在集群上启用了 &lt;code>bigquery&lt;/code> &lt;a href="https://cloud.google.com/sdk/gcloud/reference/container/clusters/create">&lt;code>Scope&lt;/code>&lt;/a>。&lt;/li>
&lt;/ol>
&lt;h4 id="google-cloud-storage-gcs">Google Cloud Storage (GCS)&lt;/h4>
&lt;ol>
&lt;li>&lt;a href="https://cloud.google.com/storage/docs/creating-buckets">&lt;code>创建 GCS 存储桶&lt;/code>&lt;/a>，希望导出日志到 GCS 中。&lt;/li>
&lt;li>记录存储桶的 ID。这里需要配置 Stackdriver。它的形式为 &lt;code>storage.googleapis.com/[BUCKET_ID]&lt;/code>。&lt;/li>
&lt;li>给&lt;a href="https://cloud.google.com/logging/docs/api/tasks/exporting-logs#writing_to_the_destination">&lt;code>接收器授权&lt;/code>&lt;/a>：&lt;code>cloud-logs @ system.gserviceaccount.com&lt;/code>。它具有 IAM 中的 Storage Object Creator 的角色。&lt;/li>
&lt;/ol>
&lt;h4 id="google-cloud-pubsub">Google Cloud Pub/Sub&lt;/h4>
&lt;ol>
&lt;li>&lt;a href="https://cloud.google.com/pubsub/docs/admin">&lt;code>创建主题&lt;/code>&lt;/a>，希望导出日志到Google Cloud Pub/Sub 中。&lt;/li>
&lt;li>记录主题的 ID。这里需要配置 Stackdriver。它的形式为&lt;code>pubsub.googleapis.com/projects/[PROJECT_ID]/topics/[TOPIC_ID]&lt;/code>。&lt;/li>
&lt;li>给&lt;a href="https://cloud.google.com/logging/docs/api/tasks/exporting-logs#writing_to_the_destination">&lt;code>接收器授权&lt;/code>&lt;/a>：&lt;code>cloud-logs @ system.gserviceaccount.com&lt;/code>。它具有 IAM 中的 Pub/Sub Publisher 角色。&lt;/li>
&lt;li>如果使用 &lt;a href="/v1.1/zh/docs/setup/kubernetes/prepare/platform-setup/gke/">&lt;code>Google Kubernetes Engine&lt;/code>&lt;/a>，请确保在集群中启动了 &lt;code>pubsub&lt;/code> &lt;a href="https://cloud.google.com/sdk/gcloud/reference/container/clusters/create">&lt;code>Scope&lt;/code>&lt;/a>。&lt;/li>
&lt;/ol>
&lt;h3 id="-stackdriver">设置 Stackdriver&lt;/h3>
&lt;p>必须创建 Stackdriver 处理程序，将数据导出到 Stackdriver。Stackdriver 处理程序的配置在&lt;a href="/v1.1/docs/reference/config/policy-and-telemetry/adapters/stackdriver/">&lt;code>此处&lt;/code>&lt;/a>描述。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>保存如下的yaml文件为 &lt;code>stackdriver.yaml&lt;/code> 。并替换 &lt;code>&amp;lt;project_id&amp;gt;, &amp;lt;sink_id&amp;gt;, &amp;lt;sink_destination&amp;gt;, &amp;lt;log_filter&amp;gt;&lt;/code> 为相应的值。&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: stackdriver
metadata:
name: handler
namespace: istio-system
spec:
# 设置 pushInterval 值。默认值是每分钟一次，不设置使用默认值。
# pushInterval: 1m
# 必须设置 Stacldriver 适配器 project_id 的值。
project_id: &amp;#34;&amp;lt;project_id&amp;gt;&amp;#34;
# apiCredentials 和 apiKey 必须设置之一; 首选方法是`appCredentials`，它对应于 Google 应用程序默认凭据。
# 如果没有提供，我们使用默认应用凭据。
# appCredentials:
# apiKey:
# serviceAccountPath:
# 描述如何将 Istio 日志映射到 Stackdriver。
logInfo:
accesslog.logentry.istio-system:
payloadTemplate: &amp;#39;{{or (.sourceIp) &amp;#34;-&amp;#34;}} - {{or (.sourceUser) &amp;#34;-&amp;#34;}} [{{or (.timestamp.Format &amp;#34;02/Jan/2006:15:04:05 -0700&amp;#34;) &amp;#34;-&amp;#34;}}] &amp;#34;{{or (.method) &amp;#34;-&amp;#34;}} {{or (.url) &amp;#34;-&amp;#34;}} {{or (.protocol) &amp;#34;-&amp;#34;}}&amp;#34; {{or (.responseCode) &amp;#34;-&amp;#34;}} {{or (.responseSize) &amp;#34;-&amp;#34;}}&amp;#39;
httpMapping:
url: url
status: responseCode
requestSize: requestSize
responseSize: responseSize
latency: latency
localIp: sourceIp
remoteIp: destinationIp
method: method
userAgent: userAgent
referer: referer
labelNames:
- sourceIp
- destinationIp
- sourceService
- sourceUser
- sourceNamespace
- destinationIp
- destinationService
- destinationNamespace
- apiClaims
- apiKey
- protocol
- method
- url
- responseCode
- responseSize
- requestSize
- latency
- connectionMtls
- userAgent
- responseTimestamp
- receivedBytes
- sentBytes
- referer
sinkInfo:
id: &amp;#39;&amp;lt;sink_id&amp;gt;&amp;#39;
destination: &amp;#39;&amp;lt;sink_destination&amp;gt;&amp;#39;
filter: &amp;#39;&amp;lt;log_filter&amp;gt;&amp;#39;
---
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: rule
metadata:
name: stackdriver
namespace: istio-system
spec:
match: &amp;#34;true&amp;#34; # 缺省 match 为 true
actions:
- handler: handler.stackdriver
instances:
- accesslog.logentry
---
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>创建配置&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f stackdriver.yaml
stackdriver &amp;#34;handler&amp;#34; created
rule &amp;#34;stackdriver&amp;#34; created
logentry &amp;#34;stackdriverglobalmr&amp;#34; created
metric &amp;#34;stackdriverrequestcount&amp;#34; created
metric &amp;#34;stackdriverrequestduration&amp;#34; created
metric &amp;#34;stackdriverrequestsize&amp;#34; created
metric &amp;#34;stackdriverresponsesize&amp;#34; created
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>访问示例应用程序。
对于 &lt;code>Bookinfo&lt;/code> 示例，请使用浏览器访问 &lt;code>http://$GATEWAY_URL/productpage&lt;/code> 或发出以下命令：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ curl http://$GATEWAY_URL/productpage
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>验证日志是否正在通过 Stackdriver 流向配置的接收器。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Stackdriver：导航到项目的 &lt;a href="https://pantheon.corp.google.com/logs/viewer">&lt;code>Stackdriver Logs Viewer&lt;/code>&lt;/a>,查看 “GKE Container” -&amp;gt; “Cluster Name” -&amp;gt; “Namespace Id” , 查看 Istio 访问日志。&lt;/li>
&lt;li>BigQuery：导航到项目的 &lt;a href="https://bigquery.cloud.google.com/">&lt;code>BigQuery Interface&lt;/code>&lt;/a>，在接收器的数据集中找到一个前缀为 &lt;code>accesslog_logentry_istio&lt;/code> 的表。&lt;/li>
&lt;li>GCS：导航到项目的 &lt;a href="https://pantheon.corp.google.com/storage/browser/">&lt;code>Storage Brower&lt;/code>&lt;/a>，在接收器的桶中找到一个名为 &lt;code>accesslog.logentry.istio-system&lt;/code> 的桶。&lt;/li>
&lt;li>Pub/Sub：导航到项目的 &lt;a href="https://pantheon.corp.google.com/cloudpubsub/topicList">&lt;code>Pub/Sub 主题列表&lt;/code>&lt;/a>, 在接收器的主题中找到 &lt;code>accesslog&lt;/code> 主题。&lt;/li>
&lt;/ul>
&lt;h2 id="heading-2">了解发生了什么&lt;/h2>
&lt;p>上面的 &lt;code>Stackdriver.yaml&lt;/code> 文件配置了 Istio 将访问日志发送到 Stackdriver，然后添加了一个接收器配置，将日志导出。具体如下：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>添加一个 &lt;code>stackdriver&lt;/code> 类型的处理程序:&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: stackdriver
metadata:
name: handler
namespace: &amp;lt;your defined namespace&amp;gt;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>在 &lt;code>spec&lt;/code> 上增加 &lt;code>logInfo&lt;/code>&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >spec:
logInfo: accesslog.logentry.istio-system:
labelNames:
- sourceIp
- destinationIp
...
...
sinkInfo:
id: &amp;#39;&amp;lt;sink_id&amp;gt;&amp;#39;
destination: &amp;#39;&amp;lt;sink_destination&amp;gt;&amp;#39;
filter: &amp;#39;&amp;lt;log_filter&amp;gt;&amp;#39;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;p>在上面的配置中，sinkInfo 包含有关日志导出到所需接收器的信息。有关如何填写不同接收器的更多信息，请参阅&lt;a href="https://cloud.google.com/logging/docs/export/#sink-terms">&lt;code>此处&lt;/code>&lt;/a>。
 &lt;/p>
&lt;ol>
&lt;li>
&lt;p>为 Stackdriver 添加规则&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: rule
metadata:
name: stackdriver
namespace: istio-system spec:
match: &amp;#34;true&amp;#34; # 缺省 match 为 true
actions:
- handler: handler.stackdriver
instances:
- accesslog.logentry
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="heading-3">清理&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>删除新的 Stackdriver 配置：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete -f stackdriver.yaml
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>如果不打算任何后续任务，请参阅 &lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E6%B8%85%E7%90%86">&lt;code>Bookinfo cleanup&lt;/code>&lt;/a> 指令关闭应用程序。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="heading-4">日志导出的可用性&lt;/h2>
&lt;p>导出到 BigQuery 只需几分钟（可以认为几乎是瞬间的），GCS 要延迟 2 至 12 小时，Pub/Sub 几乎立即的。&lt;/p></description><pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/export-logs-through-stackdriver/</link><author>Nupur Garg and Douglas Reid</author><guid isPermaLink="true">/v1.1/zh/blog/2018/export-logs-through-stackdriver/</guid></item><item><title>HTTP Egress 流量监控和访问策略</title><description>&lt;p>虽然 Istio 的主要关注点是管理服务网格内微服务之间的流量，但它也可以管理 ingress (从外部进入网格)和 egress (从网格向外)的流量。Istio 可以统一执行访问策略，并为网格内部、ingress 和 egress 流量聚合遥测数据。&lt;/p>
&lt;p>在这篇博客文章中，将向您展示如何使用 Istio 进行 HTTP Egress 流量监控和访问策略。&lt;/p>
&lt;h2 id="heading">用例&lt;/h2>
&lt;p>考虑一个运行处理 &lt;em>cnn.com&lt;/em> 内容的应用程序的组织。应用程序被解耦为部署在 Istio 服务网格中的微服务。应用程序访问 &lt;em>cnn.com&lt;/em> 的各种话题页面：&lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a>， &lt;a href="https://edition.cnn.com/sport">edition.cnn.com/sport&lt;/a> 和 &lt;a href="https://edition.cnn.com/health">edition.cnn.com/health&lt;/a>。该组织&lt;a href="/v1.1/docs/examples/advanced-gateways/egress-gateway-tls-origination/">配置了访问 edition.cnn.com 的权限&lt;/a>，一切都正常运行。然而，在某一时刻，本组织决定移除政治话题。实际上，这意味着禁止访问 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a> ，只允许访问 &lt;a href="https://edition.cnn.com/sport">edition.cnn.com/sport&lt;/a>和&lt;a href="https://edition.cnn.com/health">edition.cnn.com/health&lt;/a> 。该组织将根据具体情况，向个别应用程序和特定用户授予访问 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a> 的权限。&lt;/p>
&lt;p>为了实现这一目标，组织的运维人员监控对外部服务的访问，并分析 Istio 日志，以验证没有向 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a> 发送未经授权的请求。他们还配置了 Istio 来防止自动访问 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a> 。&lt;/p>
&lt;p>本组织决心防止对新策略的任何篡改，决定设置一些机制以防止恶意应用程序访问禁止的话题。&lt;/p>
&lt;h2 id="heading-1">相关工作和示例&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="/v1.1/zh/docs/tasks/traffic-management/egress/">Control Egress 流量&lt;/a>任务演示了网格内的应用程序如何访问外部(Kubernetes 集群之外) HTTP 和 HTTPS 服务。&lt;/li>
&lt;li>&lt;a href="/v1.1/zh/docs/examples/advanced-gateways/egress-gateway/">配置 Egress 网关&lt;/a>示例描述了如何配置 Istio 来通过一个称为 &lt;em>出口网关&lt;/em> 的专用网关服务来引导出口流量。&lt;/li>
&lt;li>&lt;a href="/v1.1/docs/examples/advanced-gateways/egress-gateway-tls-origination/">带 TLS 发起的 Egress 网关&lt;/a> 示例演示了如何允许应用程序向需要 HTTPS 的外部服务器发送 HTTP 请求，同时通过 Egress Gateway 引导流量。&lt;/li>
&lt;li>&lt;a href="/v1.1/docs/tasks/telemetry/metrics/collecting-metrics/">收集指标&lt;/a>任务描述如何为网格中的服务配置指标。&lt;/li>
&lt;li>&lt;a href="/v1.1/zh/docs/tasks/telemetry/metrics/using-istio-dashboard/">Grafana 的可视化指标&lt;/a>描述了用于监控网格流量的 Istio 仪表板。&lt;/li>
&lt;li>&lt;a href="/v1.1/zh/docs/tasks/policy-enforcement/denial-and-list/">基本访问控制&lt;/a>任务显示如何控制对网格内服务的访问。&lt;/li>
&lt;li>&lt;a href="/v1.1/zh/docs/tasks/policy-enforcement/denial-and-list/">拒绝和白/黑名单&lt;/a>任务显示如何使用黑名单或白名单检查器配置访问策略。&lt;/li>
&lt;/ul>
&lt;p>与上面的遥测和安全任务相反，这篇博客文章描述了 Istio 的监控和访问策略，专门应用于 egress 流量。&lt;/p>
&lt;h2 id="heading-2">开始之前&lt;/h2>
&lt;p>按照&lt;a href="/v1.1/docs/examples/advanced-gateways/egress-gateway-tls-origination/">带 TLS 发起的 Egress 网关&lt;/a>中的步骤，&lt;strong>启用了双向 TLS 身份验证&lt;/strong>，而不需要&lt;a href="/v1.1/docs/examples/advanced-gateways/egress-gateway-tls-origination//#cleanup">清除&lt;/a>步骤。完成该示例后，您可以从安装了 &lt;code>curl&lt;/code> 的网格中容器访问 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a>。本文假设 &lt;code>SOURCE_POD&lt;/code> 环境变量包含源 pod 的名称，容器的名称为 &lt;code>sleep&lt;/code>。&lt;/p>
&lt;h2 id="heading-3">配置监控和访问策略&lt;/h2>
&lt;p>由于您希望以 &lt;em>安全方式&lt;/em> 完成您的任务，您应该通过 &lt;em>egress 网关&lt;/em> 引导流量，正如&lt;a href="/v1.1/docs/examples/advanced-gateways/egress-gateway-tls-origination/">带 TLS 发起的 Egress 网关&lt;/a>任务中所描述的那样。这里的 &lt;em>安全方式&lt;/em> 意味着您希望防止恶意应用程序绕过 Istio 监控和策略强制。&lt;/p>
&lt;p>根据我们的场景，组织执行了&lt;a href="#%E5%BC%80%E5%A7%8B%E4%B9%8B%E5%89%8D">开始之前&lt;/a>部分中的命令，启用 HTTP 流量到 &lt;em>edition.cnn.com&lt;/em> ，并将该流量配置为通过 egress 网关。egress 网关执行 TLS 发起到 &lt;em>edition.cnn.com&lt;/em> ，因此流量在网格中被加密。此时，组织已经准备好配置 Istio 来监控和应用 &lt;em>edition.cnn.com&lt;/em> 流量的访问策略。&lt;/p>
&lt;h3 id="heading-4">日志&lt;/h3>
&lt;p>配置 Istio 以记录对 &lt;em>*.cnn.com&lt;/em> 的访问。创建一个 &lt;code>logentry&lt;/code> 和两个 &lt;a href="/v1.1/docs/reference/config/policy-and-telemetry/adapters/stdio/">stdio&lt;/a> &lt;code>handlers&lt;/code>，一个用于记录禁止访问(&lt;em>error&lt;/em> 日志级别)，另一个用于记录对 &lt;em>*.cnn.com&lt;/em> 的所有访问(&lt;em>info&lt;/em> 日志级别)。然后创建规则将 &lt;code>logentry&lt;/code> 实例定向到 &lt;code>handlers&lt;/code>。一个规则指导访问 &lt;em>*.cnn.com/politics&lt;/em> 为日志禁止访问处理程序,另一个规则指导日志条目的处理程序，输出每个访问 &lt;em>*.cnn.com&lt;/em> 作为 &lt;em>info&lt;/em> 的日志级别。要了解 Istio &lt;code>logentries&lt;/code>、&lt;code>rules&lt;/code> 和 &lt;code>handlers&lt;/code>，请参见 &lt;a href="/v1.1/zh/blog/2017/adapter-model/">Istio 适配器模型&lt;/a>。下图显示了涉及的实体和它们之间的依赖关系：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:46.46700562636976%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-monitoring-access-control/egress-adapters-monitoring.svg" title="用于 egress 监视和访问策略的实例、规则和处理程序">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-monitoring-access-control/egress-adapters-monitoring.svg" alt="用于 egress 监视和访问策略的实例、规则和处理程序" />
&lt;/a>
&lt;/div>
&lt;figcaption>用于 egress 监视和访问策略的实例、规则和处理程序&lt;/figcaption>
&lt;/figure>
&lt;ol>
&lt;li>
&lt;p>创建 &lt;code>logentry&lt;/code>、 &lt;code>rules&lt;/code> 和 &lt;code>handlers&lt;/code>。 注意您指定了 &lt;code>context.reporter.uid&lt;/code> 作为
&lt;code>kubernetes://istio-egressgateway&lt;/code> 在规则中只能从 egress 网关获取日志信息。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
# Log entry for egress access
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: logentry
metadata:
name: egress-access
namespace: istio-system
spec:
severity: &amp;#39;&amp;#34;info&amp;#34;&amp;#39;
timestamp: request.time
variables:
destination: request.host | &amp;#34;unknown&amp;#34;
path: request.path | &amp;#34;unknown&amp;#34;
responseCode: response.code | 0
responseSize: response.size | 0
reporterUID: context.reporter.uid | &amp;#34;unknown&amp;#34;
sourcePrincipal: source.principal | &amp;#34;unknown&amp;#34;
monitored_resource_type: &amp;#39;&amp;#34;UNSPECIFIED&amp;#34;&amp;#39;
---
# Handler for error egress access entries
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: stdio
metadata:
name: egress-error-logger
namespace: istio-system
spec:
severity_levels:
info: 2 # output log level as error
outputAsJson: true
---
# Rule to handle access to *.cnn.com/politics
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: rule
metadata:
name: handle-politics
namespace: istio-system
spec:
match: request.host.endsWith(&amp;#34;cnn.com&amp;#34;) &amp;amp;&amp;amp; request.path.startsWith(&amp;#34;/politics&amp;#34;) &amp;amp;&amp;amp; context.reporter.uid.startsWith(&amp;#34;kubernetes://istio-egressgateway&amp;#34;)
actions:
- handler: egress-error-logger.stdio
instances:
- egress-access.logentry
---
# Handler for info egress access entries
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: stdio
metadata:
name: egress-access-logger
namespace: istio-system
spec:
severity_levels:
info: 0 # output log level as info
outputAsJson: true
---
# Rule to handle access to *.cnn.com
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: rule
metadata:
name: handle-cnn-access
namespace: istio-system
spec:
match: request.host.endsWith(&amp;#34;.cnn.com&amp;#34;) &amp;amp;&amp;amp; context.reporter.uid.startsWith(&amp;#34;kubernetes://istio-egressgateway&amp;#34;)
actions:
- handler: egress-access-logger.stdio
instances:
- egress-access.logentry
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>发送三个 HTTP 请求到 &lt;em>cnn.com&lt;/em> 、 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a>、 &lt;a href="https://edition.cnn.com/sport">edition.cnn.com/sport&lt;/a> 和 &lt;a href="https://edition.cnn.com/health">edition.cnn.com/health&lt;/a>。
三个请求都应该返回 &lt;em>200 OK&lt;/em> 。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl exec -it $SOURCE_POD -c sleep -- sh -c &amp;#39;curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/politics; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/sport; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/health&amp;#39;
200
200
200
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>查询 Mixer 日志，查看请求信息出现在日志中:&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl -n istio-system logs -l istio-mixer-type=telemetry -c mixer | grep egress-access | grep cnn | tail -4
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T07:43:24.611462Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/politics&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:200,&amp;#34;responseSize&amp;#34;:1883355,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/sleep&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T07:43:24.886316Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/sport&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:200,&amp;#34;responseSize&amp;#34;:2094561,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/sleep&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T07:43:25.369663Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/health&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:200,&amp;#34;responseSize&amp;#34;:2157009,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/sleep&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;error&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T07:43:24.611462Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/politics&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:200,&amp;#34;responseSize&amp;#34;:1883355,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/sleep&amp;#34;}
&lt;/code>&lt;/pre>
&lt;p>您将看到与您的三个请求相关的四个日志条目。三个关于访问 &lt;em>edition.cnn.com&lt;/em> 的 &lt;em>info&lt;/em> 信息和一个关于访问 &lt;em>edition.cnn.com/politics&lt;/em> 的 &lt;em>error&lt;/em> 信息。服务网格 operators 可以查看所有访问实例，还可以搜索日志中表示禁止访问的 &lt;em>error&lt;/em> 日志。这是在自动地阻塞禁止访问之前可以应用的第一个安全措施，即将所有禁止访问实例记录为错误。在某些设置中，这可能是一个足够的安全措施。&lt;/p>
&lt;p>注意以下属性：&lt;/p>
&lt;ul>
&lt;li>&lt;code>destination&lt;/code>、 &lt;code>path&lt;/code>、 &lt;code>responseCode&lt;/code> 和 &lt;code>responseSize&lt;/code> 与请求的 HTTP 参数相关&lt;/li>
&lt;li>&lt;code>sourcePrincipal&lt;/code>:&lt;code>cluster.local/ns/default/sa/sleep&lt;/code> —— 表示 &lt;code>default&lt;/code> 命名空间中的 &lt;code>sleep&lt;/code> 服务帐户的字符串&lt;/li>
&lt;li>&lt;code>reporterUID&lt;/code>: &lt;code>kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&lt;/code> —— 报告 pod 的 UID，在本例中为 &lt;code>istio-egressgateway-747b6764b8-44rrh&lt;/code>，位于 &lt;code>istio-system&lt;/code> 命名空间中&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="heading-5">路由访问控制&lt;/h3>
&lt;p>启用对 &lt;em>edition.cnn.com&lt;/em> 的访问进行日志记录之后，自动执行访问策略，即只允许访问 &lt;em>/health&lt;/em> 和 &lt;em>/sport&lt;/em> URL 路径。这样一个简单的策略控制可以通过 Istio 路由实现。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>为 &lt;em>edition.cnn.com&lt;/em> 重定义 &lt;code>VirtualService&lt;/code> ：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: direct-cnn-through-egress-gateway
spec:
hosts:
- edition.cnn.com
gateways:
- istio-egressgateway
- mesh
http:
- match:
- gateways:
- mesh
port: 80
route:
- destination:
host: istio-egressgateway.istio-system.svc.cluster.local
subset: cnn
port:
number: 443
weight: 100
- match:
- gateways:
- istio-egressgateway
port: 443
uri:
regex: &amp;#34;/health|/sport&amp;#34;
route:
- destination:
host: edition.cnn.com
port:
number: 443
weight: 100
EOF
&lt;/code>&lt;/pre>
&lt;p>注意，您通过 &lt;code>url&lt;/code> 添加添加了一个 &lt;code>match&lt;/code>，该条件检查 URL 路径是 &lt;em>/health&lt;/em> 还是 &lt;em>/sport&lt;/em> 。还要注意，此条件已添加到 &lt;code>VirtualService&lt;/code> 的 &lt;code>istio-egressgateway&lt;/code> 部分，因为就安全性而言，egress 网关是一个经过加固的组件（请参阅 &lt;a href="/v1.1/zh/docs/examples/advanced-gateways/egress-gateway/#%E9%A2%9D%E5%A4%96%E7%9A%84%E5%AE%89%E5%85%A8%E8%80%83%E9%87%8F">egress 网关安全性注意事项&lt;/a>）。您一定不希望您的任何策略被篡改。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>发送之前的三个 HTTP 请求到 &lt;em>cnn.com&lt;/em> ：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl exec -it $SOURCE_POD -c sleep -- sh -c &amp;#39;curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/politics; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/sport; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/health&amp;#39;
404
200
200
&lt;/code>&lt;/pre>
&lt;p>向 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a> 发送请求会返回 &lt;em>404 Not Found&lt;/em> ， 然而向
&lt;a href="https://edition.cnn.com/sport">edition.cnn.com/sport&lt;/a> 和
&lt;a href="https://edition.cnn.com/health">edition.cnn.com/health&lt;/a> 发送请求，会像我们预想的那样返回 &lt;em>200 OK&lt;/em> 。&lt;/p>
&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">您可能需要等待几秒钟，等待 &lt;code>VirtualService&lt;/code> 的更新传播到 egress 网关。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;/li>
&lt;li>
&lt;p>查询 Mixer 日志，可以看到关于请求的信息再次出现在日志中：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl -n istio-system logs -l istio-mixer-type=telemetry -c mixer | grep egress-access | grep cnn | tail -4
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T07:55:59.686082Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/politics&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:404,&amp;#34;responseSize&amp;#34;:0,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/sleep&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T07:55:59.697565Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/sport&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:200,&amp;#34;responseSize&amp;#34;:2094561,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/sleep&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T07:56:00.264498Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/health&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:200,&amp;#34;responseSize&amp;#34;:2157009,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/sleep&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;error&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T07:55:59.686082Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/politics&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:404,&amp;#34;responseSize&amp;#34;:0,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/sleep&amp;#34;}
&lt;/code>&lt;/pre>
&lt;p>你依然会得到关于访问&lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a>的信息和错误消息，然而这次 &lt;code>responseCode&lt;/code> 会像我们预想的那样返回 &lt;code>404&lt;/code> 。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>虽然在这个简单的例子中使用 Istio 路由实现访问控制是可行的，但是在更复杂的例子中就不够了。例如，组织可能希望在某些条件下允许访问&lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a>，因此需要比仅通过 URL 路径过滤更复杂的策略逻辑。您可能想要应用 Istio Mixer 适配器，例如允许/禁止 URL 路径的&lt;a href="/v1.1/docs/tasks/policy-enforcement/denial-and-list/#attribute-based-whitelists-or-blacklists">白名单或黑名单&lt;/a>。策略规则允许指定复杂的条件，用丰富的表达式语言指定，其中包括与和或逻辑运算符。这些规则可用于日志记录和策略检查。更高级的用户可能希望应用基于 &lt;a href="/v1.1/docs/concepts/security/#authorization">Istio 角色访问控制&lt;/a>。&lt;/p>
&lt;p>另一方面是与远程访问策略系统的集成。如果在我们的用例中组织操作一些&lt;a href="https://en.wikipedia.org/wiki/Identity_management">标识和访问管理&lt;/a>系统，您可能希望配置 Istio 来使用来自这样一个系统的访问策略信息。您可以通过应用 &lt;a href="/v1.1/blog/2017/adapter-model/">Istio Mixer 适配器&lt;/a>来实现这种集成。&lt;/p>
&lt;p>现在您移除在本节中使用的路由取消访问控制，在下一节将向您演示通过 Mixer 策略检查实现访问控制。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>用之前&lt;a href="/v1.1/docs/examples/advanced-gateways/egress-gateway-tls-origination/#perform-tls-origination-with-an-egress-gateway">配置 Egress 网关&lt;/a>示例中的版本替换 &lt;em>edition.cnn.com&lt;/em> 的 &lt;code>VirtualService&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: direct-cnn-through-egress-gateway
spec:
hosts:
- edition.cnn.com
gateways:
- istio-egressgateway
- mesh
http:
- match:
- gateways:
- mesh
port: 80
route:
- destination:
host: istio-egressgateway.istio-system.svc.cluster.local
subset: cnn
port:
number: 443
weight: 100
- match:
- gateways:
- istio-egressgateway
port: 443
route:
- destination:
host: edition.cnn.com
port:
number: 443
weight: 100
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>发送之前的三个 HTTP 请求到 &lt;em>cnn.com&lt;/em> ， 这一次您应该会收到三个 &lt;em>200 OK&lt;/em> 的响应：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl exec -it $SOURCE_POD -c sleep -- sh -c &amp;#39;curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/politics; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/sport; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/health&amp;#39;
200
200
200
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">您可能需要等待几秒钟，等待 &lt;code>VirtualService&lt;/code> 的更新传播到 egress 网关。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;h3 id="mixer-">Mixer 策略检查访问控制&lt;/h3>
&lt;p>在该步骤中，您使用 Mixer &lt;a href="/v1.1/docs/reference/config/policy-and-telemetry/adapters/list/">&lt;code>Listchecker&lt;/code> 适配器&lt;/a>，它是一种白名单。您可以使用请求的 URL 路径定义一个 &lt;code>listentry&lt;/code>，并使用一个 &lt;code>listchecker&lt;/code> 由 &lt;code>overrides&lt;/code> 字段指定的允许 URL 路径的静态列表检查 &lt;code>listentry&lt;/code>。对于&lt;a href="https://en.wikipedia.org/wiki/Identity_management">外部标识和访问管理&lt;/a>系统，请使用 &lt;code>providerurl&lt;/code> 字段。实例、规则和处理程序的更新图如下所示。注意，您重用相同的策略规则 &lt;code>handle-cn-access&lt;/code> 来进行日志记录和访问策略检查。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:52.79420593027812%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-monitoring-access-control/egress-adapters-monitoring-policy.svg" title="用于 egress 监视和访问策略的实例、规则和处理程序">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-monitoring-access-control/egress-adapters-monitoring-policy.svg" alt="用于 egress 监视和访问策略的实例、规则和处理程序" />
&lt;/a>
&lt;/div>
&lt;figcaption>用于 egress 监视和访问策略的实例、规则和处理程序&lt;/figcaption>
&lt;/figure>
&lt;ol>
&lt;li>
&lt;p>定义 &lt;code>path-checker&lt;/code> 和 &lt;code>request-path&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | kubectl create -f -
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: listchecker
metadata:
name: path-checker
namespace: istio-system
spec:
overrides: [&amp;#34;/health&amp;#34;, &amp;#34;/sport&amp;#34;] # overrides provide a static list
blacklist: false
---
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: listentry
metadata:
name: request-path
namespace: istio-system
spec:
value: request.path
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>修改 &lt;code>handle-cnn-access&lt;/code> 策略规则并发送 &lt;code>request-path&lt;/code> 实例到 &lt;code>path-checker&lt;/code>：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
# Rule handle egress access to cnn.com
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: rule
metadata:
name: handle-cnn-access
namespace: istio-system
spec:
match: request.host.endsWith(&amp;#34;.cnn.com&amp;#34;) &amp;amp;&amp;amp; context.reporter.uid.startsWith(&amp;#34;kubernetes://istio-egressgateway&amp;#34;)
actions:
- handler: egress-access-logger.stdio
instances:
- egress-access.logentry
- handler: path-checker.listchecker
instances:
- request-path.listentry
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>执行常规测试，将 HTTP 请求发送到&lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a>， &lt;a href="https://edition.cnn.com/sport">edition.cnn.com/sport&lt;/a> 和 &lt;a href="https://edition.cnn.com/health">edition.cnn.com/health&lt;/a>。正如所料，对 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a> 的请求返回 &lt;em>403&lt;/em> （禁止）。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl exec -it $SOURCE_POD -c sleep -- sh -c &amp;#39;curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/politics; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/sport; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/health&amp;#39;
403
200
200
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h3 id="mixer--1">Mixer 策略检查访问控制，第二部分&lt;/h3>
&lt;p>在我们用例中的组织设法配置日志和访问控制之后，它决定扩展它的访问策略，允许具有特殊&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">服务帐户&lt;/a>的应用程序访问 &lt;em>cnn.com&lt;/em> 的任何主题，而不受监控。您将看到如何在 Istio 中配置此需求。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>使用 &lt;code>politics&lt;/code> 服务账户开启&lt;a href="https://github.com/istio/istio/tree/release-1.1/samples/sleep">sleep&lt;/a> 示例程序。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ sed &amp;#39;s/: sleep/: politics/g&amp;#39; samples/sleep/sleep.yaml | kubectl create -f -
serviceaccount &amp;#34;politics&amp;#34; created
service &amp;#34;politics&amp;#34; created
deployment &amp;#34;politics&amp;#34; created
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>定义 &lt;code>SOURCE_POD_POLITICS&lt;/code> shell 变量来保存带有 &lt;code>politics&lt;/code> 服务帐户的源 pod 的名称，以便向外部服务发送请求。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export SOURCE_POD_POLITICS=$(kubectl get pod -l app=politics -o jsonpath={.items..metadata.name})
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>执行常规测试，这次从 &lt;code>SOURCE_POD_POLITICS&lt;/code> 发送三个 HTTP 请求。对 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a> 的请求返回 &lt;em>403&lt;/em> ，因为您没有为 &lt;em>politics&lt;/em> 命名空间配置异常。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl exec -it $SOURCE_POD_POLITICS -c politics -- sh -c &amp;#39;curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/politics; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/sport; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/health&amp;#39;
403
200
200
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>查询 Mixer 日志，可以看到来自 &lt;em>politics&lt;/em> 命名空间的请求信息出现在日志中：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl -n istio-system logs -l istio-mixer-type=telemetry -c mixer | grep egress-access | grep cnn | tail -4
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T08:04:42.559812Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/politics&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:403,&amp;#34;responseSize&amp;#34;:84,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/politics&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T08:04:42.568424Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/sport&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:200,&amp;#34;responseSize&amp;#34;:2094561,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/politics&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;error&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T08:04:42.559812Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/politics&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:403,&amp;#34;responseSize&amp;#34;:84,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/politics&amp;#34;}
{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2019-01-29T08:04:42.615641Z&amp;#34;,&amp;#34;instance&amp;#34;:&amp;#34;egress-access.logentry.istio-system&amp;#34;,&amp;#34;destination&amp;#34;:&amp;#34;edition.cnn.com&amp;#34;,&amp;#34;path&amp;#34;:&amp;#34;/health&amp;#34;,&amp;#34;reporterUID&amp;#34;:&amp;#34;kubernetes://istio-egressgateway-747b6764b8-44rrh.istio-system&amp;#34;,&amp;#34;responseCode&amp;#34;:200,&amp;#34;responseSize&amp;#34;:2157009,&amp;#34;sourcePrincipal&amp;#34;:&amp;#34;cluster.local/ns/default/sa/politics&amp;#34;}
&lt;/code>&lt;/pre>
&lt;p>注意 &lt;code>sourcePrincipal&lt;/code> 是 &lt;code>cluster.local/ns/default/sa/politics&lt;/code>，表示 &lt;code>default&lt;/code> 命名空间中的 &lt;code>politics&lt;/code> 服务帐户。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>重新定义 &lt;code>handle-cn-access&lt;/code> 和 &lt;code>handl-politics&lt;/code> 策略规则，使 &lt;em>politics&lt;/em> 命名空间中的应用程序免受监控和策略强制。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat &amp;lt;&amp;lt;EOF | kubectl apply -f -
# Rule to handle access to *.cnn.com/politics
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: rule
metadata:
name: handle-politics
namespace: istio-system
spec:
match: request.host.endsWith(&amp;#34;cnn.com&amp;#34;) &amp;amp;&amp;amp; context.reporter.uid.startsWith(&amp;#34;kubernetes://istio-egressgateway&amp;#34;) &amp;amp;&amp;amp; request.path.startsWith(&amp;#34;/politics&amp;#34;) &amp;amp;&amp;amp; source.principal != &amp;#34;cluster.local/ns/default/sa/politics&amp;#34;
actions:
- handler: egress-error-logger.stdio
instances:
- egress-access.logentry
---
# Rule handle egress access to cnn.com
apiVersion: &amp;#34;config.istio.io/v1alpha2&amp;#34;
kind: rule
metadata:
name: handle-cnn-access
namespace: istio-system
spec:
match: request.host.endsWith(&amp;#34;.cnn.com&amp;#34;) &amp;amp;&amp;amp; context.reporter.uid.startsWith(&amp;#34;kubernetes://istio-egressgateway&amp;#34;) &amp;amp;&amp;amp; source.principal != &amp;#34;cluster.local/ns/default/sa/politics&amp;#34;
actions:
- handler: egress-access-logger.stdio
instances:
- egress-access.logentry
- handler: path-checker.listchecker
instances:
- request-path.listentry
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>从 &lt;code>SOURCE_POD&lt;/code> 中执行常规测试：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl exec -it $SOURCE_POD -c sleep -- sh -c &amp;#39;curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/politics; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/sport; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/health&amp;#39;
403
200
200
&lt;/code>&lt;/pre>
&lt;p>由于 &lt;code>SOURCE_POD&lt;/code> 没有 &lt;code>politics&lt;/code> 服务帐户，所以像以前一样访问&lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a> 会被禁止。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>从 &lt;code>SOURCE_POD_POLITICS&lt;/code> 中执行之前的测试：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl exec -it $SOURCE_POD_POLITICS -c politics -- sh -c &amp;#39;curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/politics; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/sport; curl -sL -o /dev/null -w &amp;#34;%{http_code}\n&amp;#34; http://edition.cnn.com/health&amp;#39;
200
200
200
&lt;/code>&lt;/pre>
&lt;p>访问 &lt;em>edition.cnn.com&lt;/em> 的所有话题都是被允许的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>检查 Mixer 日志，查看是否有更多使用 &lt;code>sourcePrincipal&lt;/code> 请求，能够匹配 &lt;code>cluster.local/ns/default/sa/politics&lt;/code> 的内容出现在日志中。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl -n istio-system logs -l istio-mixer-type=telemetry -c mixer | grep egress-access | grep cnn | tail -4
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="-https-egress-">与 HTTPS egress 流量控制进行比较&lt;/h2>
&lt;p>在这个用例中，应用程序使用 HTTP 和 Istio Egress 网关为它们执行 TLS 初始化。或者，应用程序可以通过向 &lt;em>edition.cnn.com&lt;/em> 发出 HTTPS 请求来发起 TLS 本身。在本节中，我们将描述这两种方法及其优缺点。&lt;/p>
&lt;p>在 HTTP 方法中，请求在本地主机上不加密地发送，由 Istio sidecar 代理拦截并转发到 egress 网关。由于您将 Istio 配置为在 sidecar 代理和 egress 网关之间使用相互的 TLS，因此流量会使 pod 加密。egress 网关解密流量，检查 URL 路径、 HTTP 方法和报头，报告遥测数据并执行策略检查。如果请求没有被某些策略检查阻止，那么 egress 网关将执行 TLS 发起到外部目的地（在我们的示例中是 &lt;em>cnn.com&lt;/em> ），因此请求将再次加密并发送到外部目的地。下图演示了这种方法的流程。网关内的 HTTP 协议根据解密后网关看到的协议来指定协议。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:64.81718469808756%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-monitoring-access-control/http-to-gateway.svg" title="HTTP egress 流量通过 egress 网关">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-monitoring-access-control/http-to-gateway.svg" alt="HTTP egress 流量通过 egress 网关" />
&lt;/a>
&lt;/div>
&lt;figcaption>HTTP egress 流量通过 egress 网关&lt;/figcaption>
&lt;/figure>
&lt;p>这种方法的缺点是请求在 pod 中发送时没有加密，这可能违反某些组织的安全策略。此外，一些 SDK 具有硬编码的外部服务 URL，包括协议，因此不可能发送 HTTP 请求。这种方法的优点是能够检查 HTTP 方法、头和 URL 路径，并基于它们应用策略。&lt;/p>
&lt;p>在 HTTPS 方法中，从应用程序到外部目的地的请求是端到端加密的。下图演示了这种方法的流程。网关中的 HTTPS 协议指定网关所看到的协议。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:64.81718469808756%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-monitoring-access-control/https-to-gateway.svg" title="HTTPS egress 流量通过 egress 网关">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-monitoring-access-control/https-to-gateway.svg" alt="HTTPS egress 流量通过 egress 网关" />
&lt;/a>
&lt;/div>
&lt;figcaption>HTTPS egress 流量通过 egress 网关&lt;/figcaption>
&lt;/figure>
&lt;p>从安全的角度来看，端到端 HTTPS 被认为是一种更好的方法。然而，由于流量是加密的，Istio 代理和出口网关只能看到源和目标 IP 以及目标的 &lt;a href="https://en.wikipedia.org/wiki/Server_Name_Indication">SNI&lt;/a>。由于您将 Istio 配置为在 sidecar 代理和 egress 网关之间使用相互的 TLS ，所以&lt;a href="/v1.1/zh/docs/concepts/security/#Istio-%E8%BA%AB%E4%BB%BD">源标识&lt;/a>也是已知的。网关无法检查 URL 路径、HTTP 方法和请求的头，因此无法基于 HTTP 信息进行监控和策略。在我们的用例中，组织将能够允许访问 &lt;em>edition.cnn.com&lt;/em> 并指定允许哪些应用程序访问 &lt;em>edition.cnn.com&lt;/em> 。但是，将不可能允许或阻止对 &lt;em>edition.cnn.com&lt;/em> 的特定URL路径的访问。使用HTTPS方法既不能阻止对 &lt;a href="https://edition.cnn.com/politics">edition.cnn.com/politics&lt;/a> 的访问，也不能监控此类访问。&lt;/p>
&lt;p>我们认为，每个组织都应充分考虑这两种方法的优缺点，并选择最适合其需要的方法。&lt;/p>
&lt;h2 id="heading-6">总结&lt;/h2>
&lt;p>在这篇博客文章中，我们展示了如何将 Istio 的不同监控和策略机制应用于 HTTP egress 流量。可以通过配置日志适配器来实现监控。访问策略可以通过配置 &lt;code>VirtualServices&lt;/code> 或配置各种策略检查适配器来实现。向您演示了一个只允许特定 URL 路径的简单策略。还向您展示了一个更复杂的策略，通过对具有特定服务帐户的应用程序进行豁免，扩展了简单策略。最后，比较了 HTTP-with-TLS-origination egress 流量与 HTTPS egress 流量，以及通过 Istio 进行控制的可能性。&lt;/p>
&lt;h2 id="heading-7">清理&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>执行&lt;a href="/v1.1/zh/docs/examples/advanced-gateways/egress-gateway//">配置 Egress 网关&lt;/a>示例的&lt;a href="/v1.1/zh/docs/examples/advanced-gateways/egress-gateway//#%E6%B8%85%E7%90%86">清理&lt;/a>部分中的说明。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>删除日志和策略检查配置：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete logentry egress-access -n istio-system
$ kubectl delete stdio egress-error-logger -n istio-system
$ kubectl delete stdio egress-access-logger -n istio-system
$ kubectl delete rule handle-politics -n istio-system
$ kubectl delete rule handle-cnn-access -n istio-system
$ kubectl delete -n istio-system listchecker path-checker
$ kubectl delete -n istio-system listentry request-path
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>删除 &lt;em>politics&lt;/em> 源 pod：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ sed &amp;#39;s/: sleep/: politics/g&amp;#39; samples/sleep/sleep.yaml | kubectl delete -f -
serviceaccount &amp;#34;politics&amp;#34; deleted
service &amp;#34;politics&amp;#34; deleted
deployment &amp;#34;politics&amp;#34; deleted
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol></description><pubDate>Fri, 22 Jun 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/egress-monitoring-access-control/</link><author>Vadim Eisenberg and Ronen Schaffer (IBM)</author><guid isPermaLink="true">/v1.1/zh/blog/2018/egress-monitoring-access-control/</guid><category>egress</category><category>traffic-management</category><category>access-control</category><category>monitoring</category></item><item><title>Istio v1aplha3 路由 API 介绍</title><description>&lt;p>到目前为止，Istio 提供了一个简单的API来进行流量管理，该API包括了四种资源：&lt;code>RouteRule&lt;/code>，&lt;code>DestinationPolicy&lt;/code>，&lt;code>EgressRule&lt;/code> 和 （Kubernetes 的）&lt;code>Ingress&lt;/code>。借助此 API，用户可以轻松管理 Istio 服务网格中的流量。该 API 允许用户将请求路由到特定版本的服务，为弹性测试注入延迟和失败，添加超时和断路器等，所有这些功能都不必更改应用程序本身的代码。&lt;/p>
&lt;p>虽然目前 API 的功能已被证明是 Istio 非常引人注目的一部分，但用户的反馈也表明，这个 API 确实有一些缺点，尤其是在使用它来管理包含数千个服务的非常大的应用程序，以及使用 HTTP 以外的协议时。 此外，使用 Kubernetes Ingress 资源来配置外部流量的方式已被证明不能满足需求。&lt;/p>
&lt;p>为了解决上述缺陷和其他的一些问题，Istio 引入了新的流量管理 API v1alpha3，新版本的 API 将完全取代之前的 API。 尽管 v1alpha3 和之前的模型在本质上是基本相同的，但它并不向后兼容的，基于旧API的模型需要进行手动转换。&lt;/p>
&lt;p>为了证明该非兼容升级的必要性，v1alpha3 API 经历了漫长而艰苦的社区评估过程，以希望新的API能够大幅改进，并经得起时间考验。 在本文中，我们将介绍新的配置模型，并试图解释影响这次变化的一些动机和设计原则。&lt;/p>
&lt;h2 id="heading">设计原则&lt;/h2>
&lt;p>路由模型的重构过程中遵循了一些关键的设计原则：&lt;/p>
&lt;ul>
&lt;li>除支持声明式（意图）配置外，也支持显式指定模型依赖的基础设施。例如，除了配置入口网关（的功能特性）之外，负责实现 入口网关功能的组件（Controller）也可以在模型指定。&lt;/li>
&lt;li>编写模型时应该&amp;quot;生产者导向”和&amp;quot;以 Host 为中心”，而不是通过组合多个规则来编写模型。 例如，所有与特定 Host 关联的规则被配置在一起，而不是单独配置。&lt;/li>
&lt;li>将路由与路由后行为清晰分开。&lt;/li>
&lt;/ul>
&lt;h2 id="v1alpha3-">v1alpha3 中的配置资源&lt;/h2>
&lt;p>在一个典型的网格中，通常有一个或多个用于终结外部 TLS 链接，将流量引入网格的负载均衡器（我们称之为 gateway）。 然后流量通过边车网关（sidecar gateway）流经内部服务。 应用程序使用外部服务的情况也很常见（例如访问 Google Maps API），一些情况下，这些外部服务可能被直接调用；但在某些部署中，网格中所有访问外部服务的流量可能被要求强制通过专用的出口网关（Egress gateway）。 下图描绘了网关在网格中的使用情况。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:35.204472660409245%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/v1alpha3-routing/gateways.svg" title="Istio服务网格中的网关">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/v1alpha3-routing/gateways.svg" alt="Role of gateways in the mesh" />
&lt;/a>
&lt;/div>
&lt;figcaption>Istio服务网格中的网关&lt;/figcaption>
&lt;/figure>
&lt;p>考虑到上述因素，&lt;code>v1alpha3&lt;/code>引入了以下这些新的配置资源来控制进入网格，网格内部和离开网格的流量路由。&lt;/p>
&lt;ol>
&lt;li>&lt;code>Gateway&lt;/code>&lt;/li>
&lt;li>&lt;code>VirtualService&lt;/code>&lt;/li>
&lt;li>&lt;code>DestinationRule&lt;/code>&lt;/li>
&lt;li>&lt;code>ServiceEntry&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>&lt;code>VirtualService&lt;/code>，&lt;code>DestinationRule&lt;/code> 和 &lt;code>ServiceEntry&lt;/code> 分别替换了原 API 中的 &lt;code>RouteRule&lt;/code>，&lt;code>DestinationPolicy&lt;/code> 和 &lt;code>EgressRule&lt;/code>。 &lt;code>Gateway&lt;/code> 是一个独立于平台的抽象，用于对流入专用中间设备的流量进行建模。&lt;/p>
&lt;p>下图描述了跨多个配置资源的控制流程。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:41.164966727369595%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/v1alpha3-routing/virtualservices-destrules.svg" title="不同v1alpha3元素之间的关系">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/v1alpha3-routing/virtualservices-destrules.svg" alt="不同v1alpha3元素之间的关系" />
&lt;/a>
&lt;/div>
&lt;figcaption>不同v1alpha3元素之间的关系&lt;/figcaption>
&lt;/figure>
&lt;h3 id="gateway">&lt;code>Gateway&lt;/code>&lt;/h3>
&lt;p>&lt;a href="/v1.1/zh/docs/reference/config/istio.networking.v1alpha3/#Gateway">&lt;code>Gateway&lt;/code>&lt;/a> 用于为 HTTP / TCP 流量配置负载均衡器，并不管该负载均衡器将在哪里运行。 网格中可以存在任意数量的 Gateway，并且多个不同的 Gateway 实现可以共存。 实际上，通过在配置中指定一组工作负载（Pod）标签，可以将 Gateway 配置绑定到特定的工作负载，从而允许用户通过编写简单的 Gateway Controller 来重用现成的网络设备。&lt;/p>
&lt;p>对于入口流量管理，您可能会问： &lt;em>为什么不直接使用 Kubernetes Ingress API&lt;/em> ？ 原因是 Ingress API 无法表达 Istio 的路由需求。 Ingress 试图在不同的 HTTP 代理之间取一个公共的交集，因此只能支持最基本的 HTTP 路由，最终导致需要将代理的其他高级功能放入到注解（annotation）中，而注解的方式在多个代理之间是不兼容的，无法移植。&lt;/p>
&lt;p>Istio &lt;code>Gateway&lt;/code> 通过将 L4-L6 配置与L7配置分离的方式克服了 &lt;code>Ingress&lt;/code> 的这些缺点。 &lt;code>Gateway&lt;/code> 只用于配置 L4-L6 功能（例如，对外公开的端口，TLS 配置），所有主流的L7代理均以统一的方式实现了这些功能。 然后，通过在 &lt;code>Gateway&lt;/code> 上绑定 &lt;code>VirtualService&lt;/code> 的方式，可以使用标准的 Istio 规则来控制进入 &lt;code>Gateway&lt;/code> 的 HTTP 和 TCP 流量。&lt;/p>
&lt;p>例如，下面这个简单的 &lt;code>Gateway&lt;/code> 配置了一个 Load Balancer，以允许访问 host &lt;code>bookinfo.com&lt;/code> 的 https 外部流量进入网格中：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
name: bookinfo-gateway
spec:
servers:
- port:
number: 443
name: https
protocol: HTTPS
hosts:
- bookinfo.com
tls:
mode: SIMPLE
serverCertificate: /tmp/tls.crt
privateKey: /tmp/tls.key
&lt;/code>&lt;/pre>
&lt;p>要为进入上面的 Gateway 的流量配置相应的路由，必须为同一个 host 定义一个 &lt;code>VirtualService&lt;/code>（在下一节中描述），并使用配置中的 &lt;code>gateways&lt;/code> 字段绑定到前面定义的 &lt;code>Gateway&lt;/code> 上：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: bookinfo
spec:
hosts:
- bookinfo.com
gateways:
- bookinfo-gateway # &amp;lt;---- bind to gateway
http:
- match:
- uri:
prefix: /reviews
route:
...
&lt;/code>&lt;/pre>
&lt;p>&lt;code>Gateway&lt;/code> 可以用于建模边缘代理或纯粹的内部代理，如第一张图所示。 无论在哪个位置，所有网关都可以用相同的方式进行配置和控制。&lt;/p>
&lt;h3 id="virtualservice">&lt;code>VirtualService&lt;/code>&lt;/h3>
&lt;p>用一种叫做 &amp;ldquo;Virtual services” 的东西代替路由规则可能看起来有点奇怪，但对于它配置的内容而言，这事实上是一个更好的名称，特别是在重新设计 API 以解决先前模型的可扩展性问题之后。&lt;/p>
&lt;p>实际上，发生的变化是：在之前的模型中，需要用一组相互独立的配置规则来为特定的目的服务设置路由规则，并通过 precedence 字段来控制这些规则的顺序；在新的 API 中，则直接对（虚拟）服务进行配置，该虚拟服务的所有规则以一个有序列表的方式配置在对应的 &lt;a href="/v1.1/zh/docs/reference/config/istio.networking.v1alpha3/#virtualservice">&lt;code>VirtualService&lt;/code>&lt;/a> 资源中。&lt;/p>
&lt;p>例如，之前在 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Bookinfo&lt;/a> 应用程序的 &lt;code>reviews&lt;/code> 服务中有两个 &lt;code>RouteRule&lt;/code> 资源，如下所示：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: config.istio.io/v1alpha2
kind: RouteRule
metadata:
name: reviews-default
spec:
destination:
name: reviews
precedence: 1
route:
- labels:
version: v1
---
apiVersion: config.istio.io/v1alpha2
kind: RouteRule
metadata:
name: reviews-test-v2
spec:
destination:
name: reviews
precedence: 2
match:
request:
headers:
cookie:
regex: &amp;#34;^(.*?;)?(user=jason)(;.*)?$&amp;#34;
route:
- labels:
version: v2
&lt;/code>&lt;/pre>
&lt;p>在 &lt;code>v1alpha3&lt;/code>，可以在单个 &lt;code>VirtualService&lt;/code> 资源中提供相同的配置：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: reviews
spec:
hosts:
- reviews
http:
- match:
- headers:
cookie:
regex: &amp;#34;^(.*?;)?(user=jason)(;.*)?$&amp;#34;
route:
- destination:
host: reviews
subset: v2
- route:
- destination:
host: reviews
subset: v1
&lt;/code>&lt;/pre>
&lt;p>正如你所看到的， 和 &lt;code>reviews&lt;/code> 服务相关的两个规则集中写在了一个地方。这个改变乍一看可能觉得并没有什么特别的优势， 然而，如果仔细观察这个新模型，会发现它和之前的 API 之间存在着根本的差异，这使得 &lt;code>v1alpha3&lt;/code> 功能更加强大。&lt;/p>
&lt;p>首先，请注意 &lt;code>VirtualService&lt;/code> 的目标服务是使用 &lt;code>hosts&lt;/code> 字段（实际上是重复字段）指定的，然后再在每个路由的 &lt;code>destination&lt;/code> 字段中指定。 这是与以前模型的重要区别。&lt;/p>
&lt;p>&lt;code>VirtualService&lt;/code> 描述了一个或多个用户可寻址目标到网格内实际工作负载之间的映射。在上面的示例中，这两个地址是相同的，但实际上用户可寻址目标可以是任何用于定位服务的，具有可选通配符前缀或 CIDR 前缀的 DNS 名称。
这对于应用从单体架构到微服务架构的迁移过程特别有用，单体应用被拆分为多个独立的微服务后，采用 &lt;code>VirtualService&lt;/code> 可以继续把多个微服务对外暴露为同一个目标地址，而不需要服务消费者进行修改以适应该变化。&lt;/p>
&lt;p>例如，以下规则允许服务消费者访问 Bookinfo 应用程序的 reviews 和 ratings 服务，就好像它们是 &lt;code>http://bookinfo.com/&lt;/code>（虚拟）服务的一部分：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: bookinfo
spec:
hosts:
- bookinfo.com
http:
- match:
- uri:
prefix: /reviews
route:
- destination:
host: reviews
- match:
- uri:
prefix: /ratings
route:
- destination:
host: ratings
...
&lt;/code>&lt;/pre>
&lt;p>实际上在 &lt;code>VirtualService&lt;/code> 中 hosts 部分设置只是虚拟的目的地,因此不一定是已在网格中注册的服务。这允许用户为在网格内没有可路由条目的虚拟主机的流量进行建模。 通过将 &lt;code>VirtualService&lt;/code> 绑定到同一 Host 的 &lt;code>Gateway&lt;/code> 配置（如前一节所述 ），可向网格外部暴露这些 Host。&lt;/p>
&lt;p>除了这个重大的重构之外， &lt;code>VirtualService&lt;/code> 还包括其他一些重要的改变：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>可以在 &lt;code>VirtualService&lt;/code> 配置中表示多个匹配条件，从而减少对冗余的规则设置。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>每个服务版本都有一个名称（称为服务子集）。 属于某个子集的一组 Pod/VM 在 &lt;code>DestinationRule&lt;/code> 定义，具体定义参见下节。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>通过使用带通配符前缀的 DNS 来指定 &lt;code>VirtualService&lt;/code> 的 host，可以创建单个规则以作用于所有匹配的服务。 例如，在 Kubernetes 中，在 &lt;code>VirtualService&lt;/code> 中使用 &lt;code>*.foo.svc.cluster.local&lt;/code> 作为 host ,可以对 &lt;code>foo&lt;/code> 命名空间中的所有服务应用相同的重写规则。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="destinationrule">&lt;code>DestinationRule&lt;/code>&lt;/h3>
&lt;p>&lt;a href="/v1.1/zh/docs/reference/config/istio.networking.v1alpha3/#destinationrule">&lt;code>DestinationRule&lt;/code>&lt;/a> 配置将流量转发到服务时应用的策略集。 这些策略应由服务提供者撰写，用于描述断路器，负载均衡设置，TLS 设置等。
除了下述改变外，&lt;code>DestinationRule&lt;/code> 与其前身 &lt;code>DestinationPolicy&lt;/code> 大致相同。&lt;/p>
&lt;ol>
&lt;li>&lt;code>DestinationRule&lt;/code> 的 &lt;code>host&lt;/code> 可以包含通配符前缀，以允许单个规则应用于多个服务。&lt;/li>
&lt;li>&lt;code>DestinationRule&lt;/code> 定义了目的 host 的子集 &lt;code>subsets&lt;/code> （例如：命名版本）。 这些 subset 用于 &lt;code>VirtualService&lt;/code> 的路由规则设置中，可以将流量导向服务的某些特定版本。 通过这种方式为版本命名后，可以在不同的 virtual service 中明确地引用这些命名版本的 subset，简化 Istio 代理发出的统计数据，并可以将 subset 编码到 SNI 头中。
为 reviews 服务配置策略和 subsets 的 &lt;code>DestinationRule&lt;/code> 可能如下所示：&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: reviews
spec:
host: reviews
trafficPolicy:
loadBalancer:
simple: RANDOM
subsets:
- name: v1
labels:
version: v1
- name: v2
labels:
version: v2
trafficPolicy:
loadBalancer:
simple: ROUND_ROBIN
- name: v3
labels:
version: v3
&lt;/code>&lt;/pre>
&lt;p>注意，与 &lt;code>DestinationPolicy&lt;/code> 不同的是，可在单个 &lt;code>DestinationRule&lt;/code> 中指定多个策略（例如上面实例中的缺省策略和 v2 版本特定的策略）。&lt;/p>
&lt;h3 id="serviceentry">&lt;code>ServiceEntry&lt;/code>&lt;/h3>
&lt;p>&lt;a href="/v1.1/zh/docs/reference/config/istio.networking.v1alpha3/#serviceentry">&lt;code>ServiceEntry&lt;/code>&lt;/a> 用于将附加条目添加到 Istio 内部维护的服务注册表中。
它最常用于对访问网格外部依赖的流量进行建模，例如访问 Web 上的 API 或遗留基础设施中的服务。&lt;/p>
&lt;p>所有以前使用 &lt;code>EgressRule&lt;/code> 进行配置的内容都可以通过 &lt;code>ServiceEntry&lt;/code> 轻松完成。 例如，可以使用类似这样的配置来允许从网格内部访问一个简单的外部服务：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: foo-ext
spec:
hosts:
- foo.com
ports:
- number: 80
name: http
protocol: HTTP
&lt;/code>&lt;/pre>
&lt;p>也就是说，&lt;code>ServiceEntry&lt;/code> 比它的前身具有更多的功能。首先，&lt;code>ServiceEntry&lt;/code> 不限于外部服务配置，它可以有两种类型：网格内部或网格外部。网格内部条目只是用于向网格显式添加服务，添加的服务与其他内部服务一样。采用网格内部条目，可以把原本未被网格管理的基础设施也纳入到网格中（例如，把虚机中的服务添加到基于 Kubernetes 的服务网格中）。网格外部条目则代表了网格外部的服务。对于这些外部服务来说，双向 TLS 身份验证是禁用的，并且策略是在客户端执行的，而不是在像内部服务请求一样在服务器端执行策略。&lt;/p>
&lt;p>由于 &lt;code>ServiceEntry&lt;/code> 配置只是将服务添加到网格内部的服务注册表中，因此它可以像注册表中的任何其他服务一样,与 &lt;code>VirtualService&lt;/code> 和/或 &lt;code>DestinationRule&lt;/code> 一起使用。例如，以下 &lt;code>DestinationRule&lt;/code> 可用于启动外部服务的 双向 TLS 连接：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: foo-ext
spec:
name: foo.com
trafficPolicy:
tls:
mode: MUTUAL
clientCertificate: /etc/certs/myclientcert.pem
privateKey: /etc/certs/client_private_key.pem
caCertificates: /etc/certs/rootcacerts.pem
&lt;/code>&lt;/pre>
&lt;p>除了扩展通用性以外，&lt;code>ServiceEntry&lt;/code> 还提供了其他一些有关 &lt;code>EgressRule&lt;/code> 改进，其中包括：&lt;/p>
&lt;ol>
&lt;li>一个 &lt;code>ServiceEntry&lt;/code> 可以配置多个服务端点，这在之前需要采用多个 &lt;code>EgressRules&lt;/code> 来实现。&lt;/li>
&lt;li>现在可以配置服务端点的解析模式（&lt;code>NONE&lt;/code>，&lt;code>STATIC&lt;/code> 或 &lt;code>DNS&lt;/code>）。&lt;/li>
&lt;li>此外，我们正在努力解决另一个难题：目前需要通过纯文本端口访问安全的外部服务（例如 &lt;code>http://google.com:443&lt;/code>）。该问题将会在未来几周内得到解决，届时将允许从应用程序直接访问 &lt;code>https://google.com&lt;/code>。请继续关注解决此限制的 Istio 补丁版本（0.8.x）。&lt;/li>
&lt;/ol>
&lt;h2 id="-v1alpha3-">创建和删除 v1alpha3 路由规则&lt;/h2>
&lt;p>由于一个特定目的地的所有路由规则现在都存储在单个 &lt;code>VirtualService&lt;/code> 资源的一个有序列表中，因此为该目的地添加新的规则不需要再创建新的 &lt;code>RouteRule&lt;/code>，而是通过更新该目的地的 &lt;code>VirtualService&lt;/code> 资源来实现。&lt;/p>
&lt;p>旧的路由规则：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f my-second-rule-for-destination-abc.yaml
&lt;/code>&lt;/pre>
&lt;p>&lt;code>v1alpha3&lt;/code> 路由规则：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f my-updated-rules-for-destination-abc.yaml
&lt;/code>&lt;/pre>
&lt;p>通过使用&lt;code>kubectl apply&lt;/code>更新现有资源，也可以删除特定目的地的最后一个路径规则。&lt;/p>
&lt;p>在添加或删除引用服务版本的路由时，需要在该服务相应的 &lt;code>DestinationRule&lt;/code> 更新 &lt;code>subsets&lt;/code> 。 正如你可能猜到的，这也是使用 &lt;code>kubectl apply&lt;/code> 完成的。&lt;/p>
&lt;h2 id="heading-1">总结&lt;/h2>
&lt;p>Istio &lt;code>v1alpha3&lt;/code> 路由 API 具有比其前身更多的功能，但不幸的是新的 API 并不向后兼容，旧的模型升级需要一次手动转换。 Istio 0.9以后将不再支持 &lt;code>RouteRule&lt;/code>，&lt;code>DesintationPolicy&lt;/code> 和 &lt;code>EgressRule&lt;/code> 这些以前的配置资源 。Kubernetes 用户可以继续使用 &lt;code>Ingress&lt;/code> 配置边缘负载均衡器来实现基本的路由。 但是，高级路由功能（例如，跨两个版本的流量分割）则需要使 &lt;code>用Gateway&lt;/code> ，这是一种功能更强大，Istio 推荐的 &lt;code>Ingress&lt;/code> 替代品。&lt;/p>
&lt;h2 id="heading-2">致谢&lt;/h2>
&lt;p>感谢以下人员为新版本的路由模型重构和实现工作做出的贡献（按字母顺序）&lt;/p>
&lt;ul>
&lt;li>Frank Budinsky (IBM)&lt;/li>
&lt;li>Zack Butcher (Google)&lt;/li>
&lt;li>Greg Hanson (IBM)&lt;/li>
&lt;li>Costin Manolache (Google)&lt;/li>
&lt;li>Martin Ostrowski (Google)&lt;/li>
&lt;li>Shriram Rajagopalan (VMware)&lt;/li>
&lt;li>Louis Ryan (Google)&lt;/li>
&lt;li>Isaiah Snell-Feikema (IBM)&lt;/li>
&lt;li>Kuat Yessenov (Google)&lt;/li>
&lt;/ul></description><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/v1alpha3-routing/</link><author>Frank Budinsky (IBM) and Shriram Rajagopalan (VMware)</author><guid isPermaLink="true">/v1.1/zh/blog/2018/v1alpha3-routing/</guid><category>traffic-management</category></item><item><title>使用AWS NLB 配置 Istio Ingress</title><description>&lt;p>本文提供了使用 &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html">AWS 网络负载均衡器&lt;/a> 配置 ingress Istio 的说明。&lt;/p>
&lt;p>可以使用网络负载均衡器 (NLB) 来代替传统的负载均衡器。 你可以查看不同的 AWS &lt;code>负载均衡器&lt;/code> 之间的 &lt;a href="https://aws.amazon.com/elasticloadbalancing/details/#Product_comparisons">比较&lt;/a>以获取更多的解释。&lt;/p>
&lt;h2 id="heading">先行条件&lt;/h2>
&lt;p>以下说明需要 Kubernetes &lt;strong>1.9.0 或更高版本&lt;/strong> 的集群。&lt;/p>
&lt;p>&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#exclamation-mark"/>&lt;/svg> AWS &lt;code>nlb&lt;/code> 在 Kubernetes 上的使用是一项 Alpha 功能 ，不建议用于生产环境的集群。&lt;/p>
&lt;h2 id="iam-">IAM 策略&lt;/h2>
&lt;p>你需要在主角色上应用策略， 以便能够配置网络负载均衡器。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>在 AWS &lt;code>iam&lt;/code> 控制台中，点击策略并单击“创建新策略”：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:52.430278884462155%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/aws-nlb/./createpolicystart.png" title="创建一个新的策略">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/aws-nlb/./createpolicystart.png" alt="创建一个新的策略" />
&lt;/a>
&lt;/div>
&lt;figcaption>创建一个新的策略&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;li>
&lt;p>选择 &lt;code>json&lt;/code>:&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:50.63492063492063%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/aws-nlb/./createpolicyjson.png" title="选择 json">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/aws-nlb/./createpolicyjson.png" alt="选择 json" />
&lt;/a>
&lt;/div>
&lt;figcaption>选择 json&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;li>
&lt;p>拷贝以下内容：&lt;/p>
&lt;pre>&lt;code class='language-json' data-expandlinks='true' >{
&amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;,
&amp;#34;Statement&amp;#34;: [
{
&amp;#34;Sid&amp;#34;: &amp;#34;kopsK8sNLBMasterPermsRestrictive&amp;#34;,
&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;,
&amp;#34;Action&amp;#34;: [
&amp;#34;ec2:DescribeVpcs&amp;#34;,
&amp;#34;elasticloadbalancing:AddTags&amp;#34;,
&amp;#34;elasticloadbalancing:CreateListener&amp;#34;,
&amp;#34;elasticloadbalancing:CreateTargetGroup&amp;#34;,
&amp;#34;elasticloadbalancing:DeleteListener&amp;#34;,
&amp;#34;elasticloadbalancing:DeleteTargetGroup&amp;#34;,
&amp;#34;elasticloadbalancing:DescribeListeners&amp;#34;,
&amp;#34;elasticloadbalancing:DescribeLoadBalancerPolicies&amp;#34;,
&amp;#34;elasticloadbalancing:DescribeTargetGroups&amp;#34;,
&amp;#34;elasticloadbalancing:DescribeTargetHealth&amp;#34;,
&amp;#34;elasticloadbalancing:ModifyListener&amp;#34;,
&amp;#34;elasticloadbalancing:ModifyTargetGroup&amp;#34;,
&amp;#34;elasticloadbalancing:RegisterTargets&amp;#34;,
&amp;#34;elasticloadbalancing:SetLoadBalancerPoliciesOfListener&amp;#34;
],
&amp;#34;Resource&amp;#34;: [
&amp;#34;*&amp;#34;
]
},
{
&amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;,
&amp;#34;Action&amp;#34;: [
&amp;#34;ec2:DescribeVpcs&amp;#34;,
&amp;#34;ec2:DescribeRegions&amp;#34;
],
&amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34;
}
]
}
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>点击审核策略，填写所有字段，接着点击创建策略：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:60.08097165991902%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/aws-nlb/./create_policy.png" title="验证策略">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/aws-nlb/./create_policy.png" alt="验证策略" />
&lt;/a>
&lt;/div>
&lt;figcaption>验证策略&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;li>
&lt;p>点击角色，选择你的主角色节点，然后点击附加策略：&lt;/p>
&lt;figure style="width:100%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:30.328324986087924%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/aws-nlb/./roles_summary.png" title="附加策略">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/aws-nlb/./roles_summary.png" alt="附加策略" />
&lt;/a>
&lt;/div>
&lt;figcaption>附加策略&lt;/figcaption>
&lt;/figure>
&lt;/li>
&lt;li>
&lt;p>现在，你的策略就已经附加到了主节点。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="-istio-ingress-">重写 Istio Ingress 服务&lt;/h2>
&lt;p>你需要使用以下内容来重写 &lt;code>istio-ingress&lt;/code> 服务：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: v1
kind: Service
metadata:
name: istio-ingress
namespace: istio-system
labels:
istio: ingress
annotations:
service.beta.kubernetes.io/aws-load-balancer-type: &amp;#34;nlb&amp;#34;
spec:
externalTrafficPolicy: Local
ports:
- port: 80
protocol: TCP
targetPort: 80
name: http
- port: 443
protocol: TCP
targetPort: 443
name: https
selector:
istio: ingress
type: LoadBalancer
&lt;/code>&lt;/pre></description><pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/aws-nlb/</link><author>Julien SENON</author><guid isPermaLink="true">/v1.1/zh/blog/2018/aws-nlb/</guid><category>ingress</category><category>traffic-management</category><category>aws</category></item><item><title>Istio 的软性多租户支持</title><description>&lt;p>多租户是一个在各种环境和各种应用中都得到了广泛应用的概念，但是不同环境中，为每租户提供的具体实现和功能性都是有差异的。&lt;a href="https://github.com/kubernetes/community/blob/master/wg-multitenancy/README.md">Kubernetes 多租户工作组&lt;/a>致力于在 Kubernetes 中定义多租户用例和功能。然而根据他们的工作进展来看，恶意容器和负载对于其他租户的 Pod 和内核资源的访问无法做到完全控制，因此只有&amp;quot;软性多租户”支持是可行的。&lt;/p>
&lt;h2 id="heading">软性多租户&lt;/h2>
&lt;p>文中提到的&amp;quot;软性多租户”的定义指的是单一 Kubernetes 控制平面和多个 Istio 控制平面以及多个服务网格相结合；每个租户都有自己的一个控制平面和一个服务网格。集群管理员对所有 Istio 控制面都有控制和监控的能力，而租户管理员仅能得到指定 Istio 的控制权。使用 Kubernetes 的命名空间和 RBAC 来完成不同租户的隔离。&lt;/p>
&lt;p>这种模式的一个用例就是企业内部共享的基础设施中，虽然预计不会发生恶意行为，但租户之间的清晰隔离仍然是很有必要的。&lt;/p>
&lt;p>本文最后会对 Istio 未来的多租户模型进行一些描述。&lt;/p>
&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">这里仅就在有限多租户环境中部署 Istio 做一些概要描述。当官方多租户支持实现之后，会在&lt;a href="/v1.1/zh/docs/">文档&lt;/a>中具体阐述。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;h2 id="heading-1">部署&lt;/h2>
&lt;h3 id="-istio-">多个 Istio 控制面&lt;/h3>
&lt;p>要部署多个 Istio 控制面，首先要在 Istio 清单文件中对所有的 &lt;code>namespace&lt;/code> 引用进行替换。以 &lt;code>istio.yaml&lt;/code> （0.8 中应该是 &lt;code>istio.yaml&lt;/code>） 为例：如果需要两个租户级的 Istio 控制面，那么第一个租户可以使用 &lt;code>istio.yaml&lt;/code> 中的缺省命名空间也就是 &lt;code>istio-system&lt;/code>；而第二个租户就要生成一个新的 Yaml 文件，并在其中使用不同的命名空间。例如使用下面的命令创建一个使用 &lt;code>istio-system1&lt;/code> 命名空间的 Yaml 文件：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ cat istio.yaml | sed s/istio-system/istio-system1/g &amp;gt; istio-system1.yaml
&lt;/code>&lt;/pre>
&lt;p>Istio Yaml 文件包含了 Istio 控制面的部署细节，包含组成控制面的 Pod（Mixer、Pilot、Ingress 以及 CA）。部署这两个控制面 Yaml 文件：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f install/kubernetes/istio.yaml
$ kubectl apply -f install/kubernetes/istio-system1.yaml
&lt;/code>&lt;/pre>
&lt;p>会在两个命名空间生成两个 Istio 控制面&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods --all-namespaces
NAMESPACE NAME READY STATUS RESTARTS AGE
istio-system istio-ca-ffbb75c6f-98w6x 1/1 Running 0 15d
istio-system istio-ingress-68d65fc5c6-dnvfl 1/1 Running 0 15d
istio-system istio-mixer-5b9f8dffb5-8875r 3/3 Running 0 15d
istio-system istio-pilot-678fc976c8-b8tv6 2/2 Running 0 15d
istio-system1 istio-ca-5f496fdbcd-lqhlk 1/1 Running 0 15d
istio-system1 istio-ingress-68d65fc5c6-2vldg 1/1 Running 0 15d
istio-system1 istio-mixer-7d4f7b9968-66z44 3/3 Running 0 15d
istio-system1 istio-pilot-5bb6b7669c-779vb 2/2 Running 0 15d
&lt;/code>&lt;/pre>
&lt;p>如果需要 Istio &lt;a href="/v1.1/zh/docs/setup/kubernetes/additional-setup/sidecar-injection/">Sidecar 注入组件&lt;/a>以及&lt;a href="/v1.1/zh/docs/tasks/telemetry/">遥测组件&lt;/a>，也需要根据租户的命名空间定义，修改所需的 Yaml 文件。&lt;/p>
&lt;p>需要由集群管理员、而不是租户自己的管理员来加载这两组 Yaml 文件。另外，要把租户管理员的操作权限限制在各自的命名空间内，还需要额外的 RBAC 配置。&lt;/p>
&lt;h3 id="heading-2">区分通用资源和命名空间资源&lt;/h3>
&lt;p>Istio 仓库中的清单文件中会创建两种资源，一种是能够被所有 Istio 控制面访问的通用资源，另一种是每个控制平面一份的专属资源。上面所说的在 Yaml 文件中替换 &lt;code>istio-system&lt;/code> 命名空间的方法自然是很简单的，更好的一种方法就是把 Yaml 文件拆分为两块，一块是所有租户共享的通用部分；另一块就是租户自有的部分。根据 &lt;a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#customresourcedefinitions">CRD 资源定义（Custom Resource Definitions）&lt;/a>中的说法，角色和角色绑定资源需要从 Istio 文件中进行剥离。另外，清单文件中提供的角色和角色绑定的定义可能不适合多租户环境，还需要进一步的细化和定制。&lt;/p>
&lt;h3 id="istio--kubernetes-rbac-">Istio 控制面的 Kubernetes RBAC 设置&lt;/h3>
&lt;p>租户管理员应该被限制在单独的 Istio 命名空间中，要完成这个限制，集群管理员需要创建一个清单，其中至少要包含一个 &lt;code>Role&lt;/code> 和 &lt;code>RoleBinding&lt;/code> 的定义，类似下面的文件所示。例子中定义了一个租户管理员，命名为 &lt;code>sales-admin&lt;/code>，他被限制在命名空间 &lt;code>istio-system&lt;/code> 之中。完整的清单中可能要在 &lt;code>Role&lt;/code> 中包含更多的 &lt;code>apiGroups&lt;/code> 条目，来定义租户管理员的资源访问能力。&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
namespace: istio-system1
name: ns-access-for-sales-admin-istio-system1
rules:
- apiGroups: [&amp;#34;&amp;#34;] # &amp;#34;&amp;#34; 代表核心 API 资源组
resources: [&amp;#34;*&amp;#34;]
verbs: [&amp;#34;*&amp;#34;]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
name: access-all-istio-system1
namespace: istio-system1
subjects:
- kind: User
name: sales-admin
apiGroup: rbac.authorization.k8s.io
roleRef:
kind: Role
name: ns-access-for-sales-admin-istio-system1
apiGroup: rbac.authorization.k8s.io
&lt;/code>&lt;/pre>
&lt;h3 id="heading-3">关注特定命名空间进行服务发现&lt;/h3>
&lt;p>除了创建 RBAC 规则来限制租户管理员只能访问指定 Istio 控制平面之外，Istio 清单还需要为 Istio Pilot 指定一个用于应用程序的命名空间，以便生成 xDS 缓存。Pilot 组件提供了命令行参数 &lt;code>--appNamespace, ns-1&lt;/code> 可以完成这一任务。&lt;code>ns-1&lt;/code> 就是租户用来部署自己应用的命名空间。&lt;code>istio-system1.yaml&lt;/code> 中包含的相关代码大致如下：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: istio-pilot
namespace: istio-system1
annotations:
sidecar.istio.io/inject: &amp;#34;false&amp;#34;
spec:
replicas: 1
template:
metadata:
labels:
istio: pilot
spec:
serviceAccountName: istio-pilot-service-account
containers:
- name: discovery
image: docker.io/&amp;lt;user ID&amp;gt;/pilot:&amp;lt;tag&amp;gt;
imagePullPolicy: IfNotPresent
args: [&amp;#34;discovery&amp;#34;, &amp;#34;-v&amp;#34;, &amp;#34;2&amp;#34;, &amp;#34;--admission-service&amp;#34;, &amp;#34;istio-pilot&amp;#34;, &amp;#34;--appNamespace&amp;#34;, &amp;#34;ns-1&amp;#34;]
ports:
- containerPort: 8080
- containerPort: 443
&lt;/code>&lt;/pre>
&lt;h3 id="heading-4">在特定命名空间中部署租户应用&lt;/h3>
&lt;p>现在集群管理员已经给租户创建了命名空间（&lt;code>istio-system1&lt;/code>），并且对 Istio Pilot 的服务发现进行了配置，要求它关注应用的命名空间（&lt;code>ns-1&lt;/code>），创建应用的 Yaml 文件，将其部署到租户的专属命名空间中：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: v1
kind: Namespace
metadata:
name: ns-1
&lt;/code>&lt;/pre>
&lt;p>然后把每个资源的命名空间都指定到 &lt;code>ns-1&lt;/code>，例如：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: v1
kind: Service
metadata:
name: details
labels:
app: details
namespace: ns-1
&lt;/code>&lt;/pre>
&lt;p>虽然没有展示出来，但是应用的命名空间也应该有 RBAC 设置，用来对特定资源进行访问控制。集群管理员和租户管理员都有权完成这种 RBAC 限制。&lt;/p>
&lt;h3 id="-istioctl">在多租户环境中使用 &lt;code>istioctl&lt;/code>&lt;/h3>
&lt;p>定义&lt;a href="https://archive.istio.io/v0.7/docs/reference/config/istio.routing.v1alpha1/#RouteRule">路由规则&lt;/a>或者&lt;a href="https://archive.istio.io/v0.7/docs/reference/config/istio.routing.v1alpha1/#DestinationPolicy">目标策略&lt;/a>时，要确认 &lt;code>istioctl&lt;/code> 命令是针对专有的 Istio 控制面所在的命名空间运行的。另外规则自身的定义也要限制在租户的命名空间里，这样才能保证规则在租户自己的网格中生效。&lt;code>-i&lt;/code> 选项用来在 Istio 控制面所属的命名空间中创建（get 和 describe 也一样）规则。&lt;code>-n&lt;/code> 参数会限制规则的所在范围是租户的网格，取值就是租户应用所在的命名空间。如果 Yaml 文件中的资源已经指定了范围，&lt;code>-n&lt;/code> 参数会被跳过。&lt;/p>
&lt;p>例如下面的命令会创建到 &lt;code>istio-system1&lt;/code> 命名空间的路由规则：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ istioctl –i istio-system1 create -n ns-1 -f route_rule_v2.yaml
&lt;/code>&lt;/pre>
&lt;p>用下面的命令可以查看：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ istioctl -i istio-system1 -n ns-1 get routerule
NAME KIND NAMESPACE
details-Default RouteRule.v1alpha2.config.istio.io ns-1
productpage-default RouteRule.v1alpha2.config.istio.io ns-1
ratings-default RouteRule.v1alpha2.config.istio.io ns-1
reviews-default RouteRule.v1alpha2.config.istio.io ns-1
&lt;/code>&lt;/pre>
&lt;p>&lt;a href="/v1.1/zh/blog/2018/soft-multitenancy/#%E5%A4%9A%E4%B8%AA-istio-%E6%8E%A7%E5%88%B6%E9%9D%A2">Multiple Istio control planes&lt;/a> 中讲述了更多多租户环境下命名空间的相关问题。&lt;/p>
&lt;h3 id="heading-5">测试结果&lt;/h3>
&lt;p>根据前文的介绍，一个集群管理员能够创建一个受限于 RBAC 和命名空间的环境，租户管理员能在其中进行部署。&lt;/p>
&lt;p>完成部署后，租户管理员就可以访问指定的 Istio 控制平面的 Pod 了。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods -n istio-system
NAME READY STATUS RESTARTS AGE
grafana-78d649479f-8pqk9 1/1 Running 0 1d
istio-ca-ffbb75c6f-98w6x 1/1 Running 0 1d
istio-ingress-68d65fc5c6-dnvfl 1/1 Running 0 1d
istio-mixer-5b9f8dffb5-8875r 3/3 Running 0 1d
istio-pilot-678fc976c8-b8tv6 2/2 Running 0 1d
istio-sidecar-injector-7587bd559d-5tgk6 1/1 Running 0 1d
prometheus-cf8456855-hdcq7 1/1 Running 0 1d
&lt;/code>&lt;/pre>
&lt;p>然而无法访问全部命名空间的 Pod：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods --all-namespaces
Error from server (Forbidden): pods is forbidden: User &amp;#34;dev-admin&amp;#34; cannot list pods at the cluster scope
&lt;/code>&lt;/pre>
&lt;p>访问其他租户的命名空间也是不可以的：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods -n istio-system1
Error from server (Forbidden): pods is forbidden: User &amp;#34;dev-admin&amp;#34; cannot list pods in the namespace &amp;#34;istio-system1&amp;#34;
&lt;/code>&lt;/pre>
&lt;p>租户管理员能够在租户指定的应用命名空间中进行应用部署。例如可以修改一下 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Bookinfo&lt;/a> 的 Yaml 然后部署到租户的命名空间 &lt;code>ns-0&lt;/code> 中，然后租户管理员就可以在这一命名空间中列出 Pod 了：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods -n ns-0
NAME READY STATUS RESTARTS AGE
details-v1-64b86cd49-b7rkr 2/2 Running 0 1d
productpage-v1-84f77f8747-rf2mt 2/2 Running 0 1d
ratings-v1-5f46655b57-5b4c5 2/2 Running 0 1d
reviews-v1-ff6bdb95b-pm5lb 2/2 Running 0 1d
reviews-v2-5799558d68-b989t 2/2 Running 0 1d
reviews-v3-58ff7d665b-lw5j9 2/2 Running 0 1d
&lt;/code>&lt;/pre>
&lt;p>同样也是不能访问其他租户的应用程序命名空间：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods -n ns-1
Error from server (Forbidden): pods is forbidden: User &amp;#34;dev-admin&amp;#34; cannot list pods in the namespace &amp;#34;ns-1&amp;#34;
&lt;/code>&lt;/pre>
&lt;p>如果部署了&lt;a href="/v1.1/zh/docs/tasks/telemetry/">遥测组件&lt;/a>, 例如
&lt;a href="/v1.1/zh/docs/tasks/telemetry/metrics/querying-metrics/">Prometheus&lt;/a>（限制在 Istio 的 &lt;code>namespace&lt;/code>），其中获得的统计结果展示的也只是租户应用命名空间的私有数据。&lt;/p>
&lt;h2 id="heading-6">结语&lt;/h2>
&lt;p>上面的一些尝试表明 Istio 有足够的能力和安全性，符合少量多租户的用例需求。另外也很明显的，Istio 和 Kubernetes &lt;strong>无法&lt;/strong>提供足够的能力和安全性来满足其他的用例，尤其是在租户之间要求完全的安全性和隔离的要求的用例。只有等容器技术（例如 Kubernetes ）能够提供更好的安全模型以及隔离能力，我们才能进一步的增强这方面的支持，Istio 的支持并不是很重要。&lt;/p>
&lt;h2 id="heading-7">问题&lt;/h2>
&lt;ul>
&lt;li>一个租户的 CA(Certificate Authority) 和 Mixer 的 Pod 中产生的 Log 包含了另一个租户的控制面的 &lt;code>info&lt;/code> 信息。&lt;/li>
&lt;/ul>
&lt;h2 id="heading-8">其他多租户模型的挑战&lt;/h2>
&lt;p>还有其他值得考虑的多租户部署模型：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>一个网格中运行多个应用程序，每个租户一个应用。集群管理员能控制和监控网格范围内的所有应用，租户管理员只能控制一个特定应用。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>单独的 Istio 控制平面控制多个网格，每个租户一个网格。集群管理员控制和监控整个 Istio 控制面以及所有网格，租户管理员只能控制特定的网格。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一个云环境（集群控制），多个 Kubernetes 控制面（租户控制）&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>这些选项，有的需要改写代码才能支持，有的无法满足用户要求。&lt;/p>
&lt;p>目前的 Istio 能力不适合第一种方案，这是因为其 RBAC 能力无法覆盖这种租户操作。另外在当前的网格模型中，Istio 的配置信息需要传递给 Envoy 代理服务器，多个租户在同一网格内共存的做法非常不安全。&lt;/p>
&lt;p>再看看第二个方式，目前的 Istio 假设每个 Istio 控制面对应一个网格。要支持这种模型需要大量改写。这种情况需要更好的对资源的范围限制进行调整，同时根据命名空间进行安全限制，此外还需要调整 Istio 的 RBAC 模型。这种模式未来可能会支持，但目前来说是不可能的。&lt;/p>
&lt;p>第三个方式对多数案例都是不合适的，毕竟多数集群管理员倾向于将同一个 Kubernetes 控制面作为 &lt;a href="https://en.wikipedia.org/wiki/Platform_as_a_service">PaaS&lt;/a> 提供给他们的租户。&lt;/p>
&lt;h2 id="heading-9">未来&lt;/h2>
&lt;p>很明显，单一 Istio 控制面控制多个网格可能是下一个功能。还有可能就是在同一个网格中支持多个租户，并提供某种程度的隔离和安全保障。要完成这样的能力，就需要像 Kubernetes 中对命名空间的的操作那样，在一个单独的控制平面中进行分区，社区中发出了&lt;a href="https://docs.google.com/document/d/14Hb07gSrfVt5KX9qNi7FzzGwB_6WBpAnDpPG6QEEd9Q">这篇文档&lt;/a>来定义其他的用例，以及要支持这些用例所需要的 Istio 功能。&lt;/p>
&lt;h2 id="heading-10">参考&lt;/h2>
&lt;ul>
&lt;li>视频：&lt;a href="https://www.youtube.com/watch?v=ahwCkJGItkU">用 RBAC 和命名空间支持的多租户功能及安全模型&lt;/a>, &lt;a href="https://schd.ws/hosted_files/kccncna17/21/Multi-tenancy%20Support%20%26%20Security%20Modeling%20with%20RBAC%20and%20Namespaces.pdf">幻灯片&lt;/a>.&lt;/li>
&lt;li>Kubecon 讨论，关于对”协同软性多租户&amp;quot;的支持 &lt;a href="https://www.youtube.com/watch?v=YRR-kZub0cA">Building for Trust: How to Secure Your Kubernetes&lt;/a>.&lt;/li>
&lt;li>Kubernetes &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC 文档&lt;/a> 以及 &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/">命名空间文档&lt;/a>.&lt;/li>
&lt;li>Kubecon 幻灯片 &lt;a href="https://schd.ws/hosted_files/kccncna17/a9/kubecon-multitenancy.pdf">Multi-tenancy Deep Dive&lt;/a>.&lt;/li>
&lt;li>Google 文档 &lt;a href="https://docs.google.com/document/d/15w1_fesSUZHv-vwjiYa9vN_uyc--PySRoLKTuDhimjc">Multi-tenancy models for Kubernetes&lt;/a>. (需要授权)&lt;/li>
&lt;li>Cloud Foundry 提出的文档：&lt;a href="https://docs.google.com/document/d/14Hb07gSrfVt5KX9qNi7FzzGwB_6WBpAnDpPG6QEEd9Q">Multi-cloud and Multi-tenancy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/document/d/12F183NIRAwj2hprx-a-51ByLeNqbJxK16X06vwH5OWE">Istio Auto Multi-Tenancy 101&lt;/a>&lt;/li>
&lt;/ul></description><pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/soft-multitenancy/</link><author>John Joyce 和 Rich Curran</author><guid isPermaLink="true">/v1.1/zh/blog/2018/soft-multitenancy/</guid><category>tenancy</category></item><item><title>用于在生产环境进行测试的 Istio 流量镜像功能</title><description>&lt;p>在非生产/测试环境中，尝试穷举一个服务所有可能的测试用例组合是个令人望而生畏的任务, 在某些情况下，您会发现编写这些用例的所有工作都与实际生产用例不匹配, 理想情况下，我们可以使用实时生产用例和流量来帮助说明我们可能在更人为的测试环境中错过的所测试服务的所有功能区域。&lt;/p>
&lt;p>Istio 可以在这里提供帮助, 随着&lt;a href="/v1.1/zh/about/notes/older/0.5/">Istio 0.5.0&lt;/a>的发布，Istio 可以镜像流量来帮助测试您的服务, 您可以编写类似于以下内容的路由规则来启用流量镜像：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: config.istio.io/v1alpha2
kind: RouteRule
metadata:
name: mirror-traffic-to-httbin-v2
spec:
destination:
name: httpbin
precedence: 11
route:
- labels:
version: v1
weight: 100
- labels:
version: v2
weight: 0
mirror:
name: httpbin
labels:
version: v2
&lt;/code>&lt;/pre>
&lt;p>这里有几点需要注意：&lt;/p>
&lt;ul>
&lt;li>当流量镜像到不同的服务时，会发生在请求的关键路径之外&lt;/li>
&lt;li>忽略对任何镜像流量的响应; 流量被视为&amp;quot;即发即忘”&lt;/li>
&lt;li>必须创建一个权重为 0 的路由，让 Istio 据此通知 Envoy 创建对应的集群定义; &lt;a href="https://github.com/istio/istio/issues/3270">这应该在未来的版本中解决&lt;/a>。&lt;/li>
&lt;/ul>
&lt;p>访问&lt;a href="/v1.1/zh/docs/tasks/traffic-management/mirroring/">镜像任务&lt;/a>了解有关镜像的更多信息，并查看更多信息
&lt;a href="https://blog.christianposta.com/microservices/traffic-shadowing-with-istio-reduce-the-risk-of-code-release/">在我的博客上综合处理这种情况&lt;/a>.&lt;/p></description><pubDate>Thu, 08 Feb 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/traffic-mirroring/</link><author>Christian Posta</author><guid isPermaLink="true">/v1.1/zh/blog/2018/traffic-mirroring/</guid><category>traffic-management</category><category>mirroring</category></item><item><title>使用外部 TCP 服务</title><description>&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">这篇博客在2018年7月23日有修改，修改的内容使用了新的 &lt;a href="/v1.1/zh/blog/2018/v1alpha3-routing/">v1alpha3 流量管理 API&lt;/a>。如果你想使用旧版本 API，请参考&lt;a href="https://archive.istio.io/v0.7/blog/2018/egress-tcp.html">这个文档&lt;/a>。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;p>在我之前的博客文章&lt;a href="/v1.1/zh/blog/2018/egress-https/">使用外部 Web 服务&lt;/a>中，描述了 Istio 服务网格内的应用程序是如何使用 HTTPS 消费外部服务的。在这篇文章中，则会演示通过 TCP 使用外部服务的方法。这里会用到 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Istio Bookinfo 示例应用程序&lt;/a>，这个版本的应用会把书籍评级数据保存在 MySQL 数据库中。下面的步骤里，会在集群外部署数据库，并配置 &lt;code>ratings&lt;/code> 服务使用这个数据库，并且还要会定义 &lt;a href="/v1.1/docs/reference/config/networking/v1alpha3/service-entry/">Service Entry&lt;/a> 以允许网内应用程序访问外部数据库。&lt;/p>
&lt;h2 id="bookinfo-">Bookinfo 示例应用程序与外部评级数据库&lt;/h2>
&lt;p>首先，在 Kubernetes 集群之外部署一个 MySQL 数据库实例来保存 Bookinfo 评级数据，然后修改 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Bookinfo 示例应用程序&lt;/a>以使用这个数据库。&lt;/p>
&lt;h3 id="heading">为评级数据设置数据库&lt;/h3>
&lt;p>根据任务需求，部署一个 &lt;a href="https://www.mysql.com">MySQL&lt;/a> 的实例，这里可以使用任何 MySQL 实例；例如可以使用 &lt;a href="https://www.ibm.com/cloud/compose/mysql">Compose for MySQL&lt;/a>。下面会使用 &lt;code>mysqlsh&lt;/code>（&lt;a href="https://dev.mysql.com/doc/mysql-shell/en/">MySQL Shell&lt;/a>）作为 MySQL 客户端来提供评级数据。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>设置 &lt;code>MYSQL_DB_HOST&lt;/code> 和 &lt;code>MYSQL_DB_PORT&lt;/code> 环境变量。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export MYSQL_DB_HOST=&amp;lt;你的 MySQL host&amp;gt;
$ export MYSQL_DB_PORT=&amp;lt;你的 MySQL port&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>如果你使用的是本地数据库，使用的是默认 MYSQL port，那 &lt;code>host&lt;/code> 和 &lt;code>port&lt;/code> 分别是 &lt;code>localhost&lt;/code> 和 &lt;code>3306&lt;/code>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>初始化数据库时，如果出现提示，执行下面的命令输入密码。这个命令通过 &lt;code>admin&lt;/code> 数据库用户凭证来执行。这个 &lt;code>admin&lt;/code> 用户是通过 &lt;a href="https://www.ibm.com/cloud/compose/mysql">Compose for Mysql&lt;/a> 创建数据库时默认存在的。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ curl -s https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/src/mysql/mysqldb-init.sql | mysqlsh --sql --ssl-mode=REQUIRED -u admin -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>或者&lt;/strong>&lt;/p>
&lt;p>如果使用的是 &lt;code>mysql&lt;/code> 客户端和本地 MySQL 数据库时，运行：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ curl -s https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/src/mysql/mysqldb-init.sql | mysql -u root -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>创建一个名为 &lt;code>bookinfo&lt;/code> 的用户，并在 &lt;code>test.ratings&lt;/code> 表上授予它 &lt;code>SELECT&lt;/code> 权限：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ mysqlsh --sql --ssl-mode=REQUIRED -u admin -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT -e &amp;#34;CREATE USER &amp;#39;bookinfo&amp;#39; IDENTIFIED BY &amp;#39;&amp;lt;password you choose&amp;gt;&amp;#39;; GRANT SELECT ON test.ratings to &amp;#39;bookinfo&amp;#39;;&amp;#34;
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>或者&lt;/strong>&lt;/p>
&lt;p>如果用的是 &lt;code>mysql&lt;/code> 和本地数据库，命令是：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ mysql -u root -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT -e &amp;#34;CREATE USER &amp;#39;bookinfo&amp;#39; IDENTIFIED BY &amp;#39;&amp;lt;password you choose&amp;gt;&amp;#39;; GRANT SELECT ON test.ratings to &amp;#39;bookinfo&amp;#39;;&amp;#34;
&lt;/code>&lt;/pre>
&lt;p>在这里，建议使用&lt;a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">最小特权原则&lt;/a>，这意味着不在 Bookinfo 应用程序中使用 &lt;code>admin&lt;/code> 用户；而是为应用程序 Bookinfo 创建了一个最小权限的特殊用户 &lt;code>bookinfo&lt;/code>， 在这种情况下，&lt;code>bookinfo&lt;/code> 用户只对单个表具有 &lt;code>SELECT&lt;/code> 特权。&lt;/p>
&lt;p>在运行命令创建用户之后，应该检查最后一个命令的编号并运行 &lt;code>history -d &amp;lt;创建用户的命令编号&amp;gt;&lt;/code> 来清理我的 bash 历史记录，防止新用户的密码存储在 bash 历史记录中，如果你使用了 &lt;code>mysql&lt;/code> 命令行工具，记得要删除 &lt;code>~/.mysql_history&lt;/code> 文件中的最后一个命令。在 &lt;a href="https://dev.mysql.com/doc/refman/5.5/en/create-user.html">MySQL 文档&lt;/a>中阅读有关新创建用户的密码保护的更多信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>检查创建的评级数据，看看一切都按预期工作：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ mysqlsh --sql --ssl-mode=REQUIRED -u bookinfo -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT -e &amp;#34;select * from test.ratings;&amp;#34;
Enter password:
+----------+--------+
| ReviewID | Rating |
+----------+--------+
| 1 | 5 |
| 2 | 4 |
+----------+--------+
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>或者&lt;/strong>&lt;/p>
&lt;p>对于 &lt;code>mysql&lt;/code> 和本地数据库：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ mysql -u bookinfo -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT -e &amp;#34;select * from test.ratings;&amp;#34;
Enter password:
+----------+--------+
| ReviewID | Rating |
+----------+--------+
| 1 | 5 |
| 2 | 4 |
+----------+--------+
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>暂时将评级设置为 &lt;code>1&lt;/code>，以便在 Bookinfo &lt;code>ratings&lt;/code> 服务使用我们的数据库时提供可视线索：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ mysqlsh --sql --ssl-mode=REQUIRED -u admin -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT -e &amp;#34;update test.ratings set rating=1; select * from test.ratings;&amp;#34;
Enter password:
Rows matched: 2 Changed: 2 Warnings: 0
+----------+--------+
| ReviewID | Rating |
+----------+--------+
| 1 | 1 |
| 2 | 1 |
+----------+--------+
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>或者&lt;/strong>&lt;/p>
&lt;p>对于 &lt;code>mysql&lt;/code> 和本地数据库：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ mysql -u root -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT -e &amp;#34;update test.ratings set rating=1; select * from test.ratings;&amp;#34;
Enter password:
+----------+--------+
| ReviewID | Rating |
+----------+--------+
| 1 | 1 |
| 2 | 1 |
+----------+--------+
&lt;/code>&lt;/pre>
&lt;p>因为 &lt;code>bookinfo&lt;/code> 用户在 &lt;code>test.ratings&lt;/code> 表上没有 &lt;code>UPDATE&lt;/code> 权限，所以在最后一个命令中使用了 &lt;code>admin/root&lt;/code> 用户&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>现在就已经可以去部署使用外部数据库的 Bookinfo 应用程序版本了。&lt;/p>
&lt;h3 id="bookinfo--1">Bookinfo 应用程序的初始设置&lt;/h3>
&lt;p>为了演示使用外部数据库的场景，首先要求有一个安装了 &lt;a href="/v1.1/zh/docs/setup/kubernetes/install/kubernetes/#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4">Istio&lt;/a> 的 Kubernetes 集群，然后部署 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Istio Bookinfo 示例应用程序&lt;/a>，并&lt;a href="/v1.1/docs/examples/bookinfo/#apply-default-destination-rules">创建默认的 Destination rule 对象&lt;/a>，还要对 Istio 的缺省策略进行配置，&lt;a href="/v1.1/docs/tasks/traffic-management/egress/#change-to-the-blocking-by-default-policy">拦截外发流量&lt;/a>&lt;/p>
&lt;p>此应用程序使用 &lt;code>ratings&lt;/code> 微服务来获取书籍评级，评分在 1 到 5 之间。评级显示为每个评论的星号。&lt;code>ratings&lt;/code> 微服务有几个不同版本，有的会使用 &lt;a href="https://www.mongodb.com">MongoDB&lt;/a>，有的会使用 &lt;a href="https://www.mysql.com">MySQL&lt;/a>。&lt;/p>
&lt;p>这篇博客例子里的命令是以 Istio 0.8 以上版本为基础的，不管是否启用了&lt;a href="/v1.1/zh/docs/concepts/security/#%E5%8F%8C%E5%90%91-tls-%E8%AE%A4%E8%AF%81">双向 TLS&lt;/a>，都是可用的。&lt;/p>
&lt;p>这是 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Bookinfo 示例应用程序&lt;/a>中应用程序的端到端架构图。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:59.086918235567985%">
&lt;a data-skipendnotes="true" href="/v1.1/docs/examples/bookinfo/withistio.svg" title="原始的 Bookinfo 应用程序">
&lt;img class="element-to-stretch" src="/v1.1/docs/examples/bookinfo/withistio.svg" alt="原始的 Bookinfo 应用程序" />
&lt;/a>
&lt;/div>
&lt;figcaption>原始的 Bookinfo 应用程序&lt;/figcaption>
&lt;/figure>
&lt;h3 id="-bookinfo-">将数据库用于 Bookinfo 应用程序中的评级数据&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>修改使用 MySQL 数据库的 &lt;code>ratings&lt;/code> 服务版本的 &lt;code>Deployment&lt;/code> 对象的 &lt;code>spec&lt;/code>，以使用前面搭建的数据库实例。该文件位于 Istio 发行包的 &lt;a href="https://github.com/istio/istio/blob/release-1.1/samples/bookinfo/platform/kube/bookinfo-ratings-v2-mysql.yaml">&lt;code>samples/bookinfo/platform/kube/bookinfo-ratings-v2-mysql.yaml&lt;/code>&lt;/a> 中。编辑以下几行：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >- name: MYSQL_DB_HOST
value: mysqldb
- name: MYSQL_DB_PORT
value: &amp;#34;3306&amp;#34;
- name: MYSQL_DB_USER
value: root
- name: MYSQL_DB_PASSWORD
value: password
&lt;/code>&lt;/pre>
&lt;p>替换上面代码段中的值，指定数据库主机，端口，用户和密码。请注意，在 Kubernetes 中使用容器环境变量中密码的正确方法是&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables">使用 Secret&lt;/a>，在这个示例任务中，我们直接配置了明文密码， &lt;strong>切记！不要在真实环境中这样做&lt;/strong>！我想你们应该也知道，&lt;code>&amp;quot;password&amp;quot;&lt;/code> 这个值也不应该用作密码。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>应用修改后的 &lt;code>spec&lt;/code> 来部署使用外部数据库的 &lt;code>ratings&lt;/code> 服务，&lt;code>v2-mysql&lt;/code> 的版本。&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-ratings-v2-mysql.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/bookinfo/platform/kube/bookinfo-ratings-v2-mysql.yaml@
deployment &amp;#34;ratings-v2-mysql&amp;#34; created
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;li>
&lt;p>将发往 &lt;code>reviews&lt;/code> 服务的所有流量路由到 &lt;code>v3&lt;/code> 版本，这样做是为了确保 &lt;code>reviews&lt;/code> 服务始终调用 &lt;code>ratings&lt;/code> 服务，此外，将发往 &lt;code>ratings&lt;/code> 服务的所有流量路由到使用外部数据库的 &lt;code>ratings:v2-mysql&lt;/code>。&lt;/p>
&lt;p>添加两个 &lt;a href="/v1.1/docs/reference/config/networking/v1alpha3/virtual-service/">&lt;code>VirtualService&lt;/code>&lt;/a>，为上述两个服务指定路由。这些代码保存在 Istio 发行包的 &lt;code>samples/bookinfo/networking/virtual-service-ratings-mysql.yaml&lt;/code> 中。&lt;/p>
&lt;p>&lt;strong>注意&lt;/strong>：确保你在完成了&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E5%BA%94%E7%94%A8%E7%BC%BA%E7%9C%81%E7%9B%AE%E6%A0%87%E8%A7%84%E5%88%99">添加默认目标路由&lt;/a>的步骤之后才执行下面的命令。&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/virtual-service-ratings-mysql.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/bookinfo/networking/virtual-service-ratings-mysql.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;/ol>
&lt;p>更新的架构如下所示，请注意网格内的蓝色箭头，来自我们添加的 &lt;code>VirtualService&lt;/code> 定义，将请求发送给 &lt;code>reviews v3&lt;/code> 和 &lt;code>ratings v2-mysql&lt;/code>。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:59.314858206480224%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-tcp/bookinfo-ratings-v2-mysql-external.svg" title="Bookinfo 应用程序，ratings 服务是 v2-mysql，使用外部的 MySQL 数据库">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-tcp/bookinfo-ratings-v2-mysql-external.svg" alt="Bookinfo 应用程序，ratings 服务是 v2-mysql，使用外部的 MySQL 数据库" />
&lt;/a>
&lt;/div>
&lt;figcaption>Bookinfo 应用程序，ratings 服务是 v2-mysql，使用外部的 MySQL 数据库&lt;/figcaption>
&lt;/figure>
&lt;p>请注意，MySQL 数据库位于 Istio 服务网格之外，或者更准确地说是在 Kubernetes 集群之外，服务网格的边界由虚线标记。&lt;/p>
&lt;h3 id="heading-1">访问网页&lt;/h3>
&lt;p>在&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E7%A1%AE%E5%AE%9A-ingress-%E7%9A%84-ip-%E5%92%8C%E7%AB%AF%E5%8F%A3">确定入口 IP 和端口&lt;/a>之后，访问应用程序的网页。&lt;/p>
&lt;p>你会发现问题，在每个评审信息下方都会显示 &lt;code>&amp;quot;Ratings service is currently unavailable”&lt;/code> 而不是评级星标。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:36.18705035971223%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-tcp/errorFetchingBookRating.png" title="Ratings 服务的错误信息">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-tcp/errorFetchingBookRating.png" alt="Ratings 服务的错误信息" />
&lt;/a>
&lt;/div>
&lt;figcaption>Ratings 服务的错误信息&lt;/figcaption>
&lt;/figure>
&lt;p>与&lt;a href="/v1.1/zh/blog/2018/egress-https/">使用外部 Web 服务&lt;/a>一样，你会体验到&lt;strong>优雅的服务降级&lt;/strong>，这很好，虽然 &lt;code>ratings&lt;/code> 服务中有错误，但是应用程序并没有因此而崩溃，应用程序的网页正确显示了书籍信息，详细信息和评论，只是没有评级星。&lt;/p>
&lt;p>你遇到的问题与&lt;a href="/v1.1/zh/blog/2018/egress-https/">使用外部 Web 服务&lt;/a>中的问题相同，即 Kubernetes 集群外的所有流量（TCP 和 HTTP）都被 Sidecar 代理根据默认策略阻止了，要启用这一访问，必须定义 &lt;code>ServiceEntry&lt;/code>&lt;/p>
&lt;h3 id="-mysql-">外部 MySQL 实例的网格外部服务入口&lt;/h3>
&lt;p>为网格外的服务定义 &lt;code>ServiceEntry&lt;/code>。&lt;/p>
&lt;ol>
&lt;li>
&lt;p>获取你的 MySQL 数据库实例的 IP 地址，例如可以通过 &lt;a href="https://linux.die.net/man/1/host">host&lt;/a> 命令实现：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ export MYSQL_DB_IP=$(host $MYSQL_DB_HOST | grep &amp;#34; has address &amp;#34; | cut -d&amp;#34; &amp;#34; -f4)
&lt;/code>&lt;/pre>
&lt;p>如果是本地数据库，设置 &lt;code>MYSQL_DB_IP&lt;/code> 环境变量为你的本机 IP，保证这个地址能被集群访问到。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>为网格外 TCP 服务定义 &lt;code>ServiceEntry&lt;/code>&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: mysql-external
spec:
hosts:
- $MYSQL_DB_HOST
addresses:
- $MYSQL_DB_IP/32
ports:
- name: tcp
number: $MYSQL_DB_PORT
protocol: tcp
location: MESH_EXTERNAL
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>检查你刚刚新增的 &lt;code>ServiceEntry&lt;/code>，确保它的值是正确的&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get serviceentry mysql-external -o yaml
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
...
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;p>请注意，对于 TCP &lt;code>ServiceEntry&lt;/code>，&lt;code>port&lt;/code> 的 &lt;code>protocol&lt;/code> 的值应该设置为 &lt;code>tcp&lt;/code>；另外注意，要在 &lt;code>addresses&lt;/code> 列表里面指定外部服务的 IP 地址（一个 &lt;code>32&lt;/code> 为后缀的 &lt;a href="https://tools.ietf.org/html/rfc2317">CIDR&lt;/a>）。&lt;/p>
&lt;p>&lt;a href="#tcp-%E6%B5%81%E9%87%8F%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%85%A5%E5%8F%A3">后续章节&lt;/a>中还会详细讨论 TCP &lt;code>ServiceEntry&lt;/code>。现在先来验证我们添加的出口规则是否解决了问题。访问网页看看评星是否恢复显示。&lt;/p>
&lt;p>现在访问应用程序的网页的时候，会显示评级而不会出现错误：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:36.69064748201439%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-tcp/externalMySQLRatings.png" title="Book Ratings 显示正常">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-tcp/externalMySQLRatings.png" alt="Book Ratings 显示正常" />
&lt;/a>
&lt;/div>
&lt;figcaption>Book Ratings 显示正常&lt;/figcaption>
&lt;/figure>
&lt;p>正如预期的那样，你会看到两个显示评论的一星评级。将评级更改为一颗星，为我们提供了一个视觉线索，确实使用了我们的外部数据库。&lt;/p>
&lt;p>与 HTTP/HTTPS 的服务入口一样，你可以动态地使用 &lt;code>kubectl&lt;/code> 删除和创建 TCP 的服务入口。&lt;/p>
&lt;h2 id="-tcp-">出口 TCP 流量控制的动机&lt;/h2>
&lt;p>一些网内 Istio 应用程序必须访问外部服务，例如遗留系统，在许多情况下，都不是通过 HTTP 或 HTTPS 协议，而是使用其他 TCP 协议完成和外部数据库的通信，例如 &lt;a href="https://docs.mongodb.com/manual/reference/mongodb-wire-protocol/">MongoDB wire 协议&lt;/a>和 &lt;a href="https://dev.mysql.com/doc/internals/en/client-server-protocol.html">MySQL 客户端/服务器协议&lt;/a>等特定数据库的协议。&lt;/p>
&lt;p>接下来我会再说说 TCP 流量的服务入口。&lt;/p>
&lt;h2 id="tcp-">TCP 流量的服务入口&lt;/h2>
&lt;p>启用到特定端口的 TCP 流量的服务入口必须指定 &lt;code>TCP&lt;/code> 作为端口的协议，此外，对于 &lt;a href="https://docs.mongodb.com/manual/reference/mongodb-wire-protocol/">MongoDB wire协议&lt;/a>，协议可以指定为 &lt;code>MONGO&lt;/code>，而不是 &lt;code>TCP&lt;/code>。&lt;/p>
&lt;p>对于服务入口配置的 &lt;code>addresses&lt;/code> 字段，必须使用 &lt;a href="https://tools.ietf.org/html/rfc2317">CIDR&lt;/a>表示法中的 IP 块。注意在 TCP 服务入口配置中，&lt;code>host&lt;/code> 字段会被忽略。&lt;/p>
&lt;p>要通过其主机名启用到外部服务的 TCP 流量，必须指定主机名的所有 IP，每个 IP 必须由 CIDR 块指定。&lt;/p>
&lt;p>请注意，外部服务的所有 IP 并不总是已知。要往外发送 TCP 流量，只能配置为被应用程序使用的 IP。&lt;/p>
&lt;p>另请注意，外部服务的 IP 并不总是静态的，例如 &lt;a href="https://en.wikipedia.org/wiki/Content_delivery_network">CDNs&lt;/a> 的情况。有时 IP 在大多数情况下是静态的，但可以不时地更改，例如由于基础设施的变化。在这些情况下，如果已知可能 IP 的范围，则应通过 CIDR 块指定范围。如果不知道可能的IP的范围，则不能使用 TCP 服务入口。可以绕过 Sidecar 代理，&lt;a href="/v1.1/zh/docs/tasks/traffic-management/egress/#%E7%9B%B4%E6%8E%A5%E8%B0%83%E7%94%A8%E5%A4%96%E9%83%A8%E6%9C%8D%E5%8A%A1">直接调用外部服务&lt;/a>。&lt;/p>
&lt;h2 id="heading-2">与网格扩展的关系&lt;/h2>
&lt;p>请注意，本文中描述的场景与&lt;a href="/v1.1/zh/docs/examples/integrating-vms/">集成虚拟机&lt;/a>示例中描述的网格扩展场景不同。在那种场景中，MySQL 实例在与 Istio 服务网格集成的外部（集群外）机器（物理机或虚拟机）上运行，和 Istio 服务网格进行集成。这种情况下，MySQL 服务成为网格的一等公民，具有 Istio 的所有有益功能，服务可以通过本地集群域名寻址，例如通过 &lt;code>mysqldb.vm.svc.cluster.local&lt;/code>，并且可以通过&lt;a href="/v1.1/zh/docs/concepts/security/#%E5%8F%8C%E5%90%91-tls-%E8%AE%A4%E8%AF%81">双向 TLS 身份验证&lt;/a>保护与它的通信，无需创建 &lt;code>ServiceEntry&lt;/code> 来访问此服务; 但是，该服务必须在 Istio 中进行注册，要启用此类集成，必须在计算机上安装 Istio 组件（ &lt;code>Envoy proxy&lt;/code> 、&lt;code>node-agent&lt;/code>、&lt;code>istio-agent&lt;/code> ），并且必须可以从中访问 Istio 控制平面（&lt;code>Pilot&lt;/code> ，&lt;code>Mixer&lt;/code> ，&lt;code>Citadel&lt;/code>）。有关详细信息，请参阅 &lt;a href="/v1.1/zh/docs/setup/kubernetes/additional-setup/mesh-expansion/">Istio 网格扩展&lt;/a>的相关说明。&lt;/p>
&lt;p>在我们的示例中，MySQL 实例可以在任何计算机上运行，也可以由云提供商作为服务进行配置。无需把服务器集成进 Istio。无需访问 Istio 控制平面。在 MySQL 作为服务的情况下，可能无法直接访问 MySQL 所在的服务器，也无法在服务器上安装所需的组件。在我们的例子中，MySQL 实例可以通过其全局域名进行寻址，如果消费应用程序希望使用该域名，尤其是在消费应用程序的部署配置中无法更改时，这就更重要了。&lt;/p>
&lt;h2 id="heading-3">清理&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>删除 &lt;code>test&lt;/code> 数据库和 &lt;code>bookinfo&lt;/code> 用户：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ mysqlsh --sql --ssl-mode=REQUIRED -u admin -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT -e &amp;#34;drop database test; drop user bookinfo;&amp;#34;
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>或者&lt;/strong>&lt;/p>
&lt;p>对于&lt;code>mysql&lt;/code>和本地数据库：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ mysql -u root -p --host $MYSQL_DB_HOST --port $MYSQL_DB_PORT -e &amp;#34;drop database test; drop user bookinfo;&amp;#34;
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>删除虚拟服务：&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/virtual-service-ratings-mysql.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete -f @samples/bookinfo/networking/virtual-service-ratings-mysql.yaml@
Deleted config: virtual-service/default/reviews
Deleted config: virtual-service/default/ratings
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;li>
&lt;p>取消部署 &lt;code>ratings v2-mysql&lt;/code> ：&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-ratings-v2-mysql.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete -f @samples/bookinfo/platform/kube/bookinfo-ratings-v2-mysql.yaml@
deployment &amp;#34;ratings-v2-mysql&amp;#34; deleted
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;li>
&lt;p>删除服务入口：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete serviceentry mysql-external -n default
Deleted config: serviceentry mysql-external
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;h2 id="heading-4">结论&lt;/h2>
&lt;p>本文演示了 Istio 服务网格中的微服务通过 TCP 使用外部服务的方法，默认情况下，Istio 会阻止所有（TCP 和 HTTP）到集群外的主机的流量，要为网格中的服务启用对外的 TCP 流量，必须创建 TCP &lt;code>ServiceEntry&lt;/code>。&lt;/p></description><pubDate>Tue, 06 Feb 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/egress-tcp/</link><author>Vadim Eisenberg</author><guid isPermaLink="true">/v1.1/zh/blog/2018/egress-tcp/</guid><category>traffic-management</category><category>egress</category><category>tcp</category></item><item><title>使用外部 Web 服务</title><description>&lt;p>在许多情况下，在 &lt;em>service mesh&lt;/em> 中的微服务序并不是应用程序的全部，有时，
网格内部的微服务需要使用在服务网格外部的遗留系统提供的功能，虽然我们希望逐步将这些系统迁移到服务网格中。
但是在迁移这些系统之前，必须让服务网格内的应用程序能访问它们。还有其他情况，
应用程序使用外部组织提供的 Web 服务，通常是通过万维网提供的服务。&lt;/p>
&lt;p>在这篇博客文章中，我修改了&lt;a href="/v1.1/zh/docs/examples/bookinfo/">Istio Bookinfo 示例应用程序&lt;/a>让它可以
从外部 Web 服务（&lt;a href="https://developers.google.com/books/docs/v1/getting_started">Google Books APIs&lt;/a> ）获取图书详细信息。
我将展示如何使用 &lt;em>mesh-external service entries&lt;/em> 在 Istio 中启用外部 HTTPS 流量。最后，
我解释了当前与 Istio 出口流量控制相关的问题。&lt;/p>
&lt;h2 id="heading">初始设定&lt;/h2>
&lt;p>为了演示使用外部 Web 服务的场景，我首先使用安装了 &lt;a href="/v1.1/zh/docs/setup/kubernetes/install/kubernetes/#%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4">Istio&lt;/a> 的
Kubernetes 集群, 然后我部署 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Istio Bookinfo 示例应用程序&lt;/a>,
此应用程序使用 &lt;em>details&lt;/em> 微服务来获取书籍详细信息，例如页数和发布者, 原始 &lt;em>details&lt;/em> 微服务提供书籍
详细信息，无需咨询任何外部服务。&lt;/p>
&lt;p>此博客文章中的示例命令适用于 Istio 1.0+，无论启用或不启用&lt;a href="/v1.1/zh/docs/concepts/security/#%E5%8F%8C%E5%90%91-tls-%E8%AE%A4%E8%AF%81">双向 TLS&lt;/a>。
Bookinfo 配置文件位于 Istio 发行存档的 &lt;code>samples/bookinfo&lt;/code> 目录中。&lt;/p>
&lt;p>以下是原始 &lt;a href="/v1.1/zh/docs/examples/bookinfo/">Bookinfo 示例应用程序&lt;/a>中应用程序端到端体系结构的副本。&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:59.086918235567985%">
&lt;a data-skipendnotes="true" href="/v1.1/docs/examples/bookinfo/withistio.svg" title="原 Bookinfo 应用程序">
&lt;img class="element-to-stretch" src="/v1.1/docs/examples/bookinfo/withistio.svg" alt="原 Bookinfo 应用程序" />
&lt;/a>
&lt;/div>
&lt;figcaption>原 Bookinfo 应用程序&lt;/figcaption>
&lt;/figure>
&lt;p>执行&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8">部署应用程序&lt;/a>、&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E7%A1%AE%E8%AE%A4%E5%BA%94%E7%94%A8%E5%9C%A8%E8%BF%90%E8%A1%8C%E4%B8%AD">确认应用正在运行&lt;/a>，以及
&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E5%BA%94%E7%94%A8%E7%BC%BA%E7%9C%81%E7%9B%AE%E6%A0%87%E8%A7%84%E5%88%99">应用默认目标规则&lt;/a>中的步骤部分，和&lt;a href="/v1.1/docs/tasks/traffic-management/egress/#change-to-the-blocking-by-default-policy">Istio 更改为 blocking-egress-by-default 策略&lt;/a>。&lt;/p>
&lt;h3 id="bookinfo--https--google-">Bookinfo 使用 HTTPS 访问 Google 图书网络服务&lt;/h3>
&lt;p>让我们添加一个新版本的 &lt;em>details&lt;/em> 微服务，&lt;em>v2&lt;/em>，从&lt;a href="https://developers.google.com/books/docs/v1/getting_started">Google Books APIs&lt;/a>中获取图书详细信息。
它设定了服务容器的 &lt;code>DO_NOT_ENCRYPT&lt;/code> 环境变量为 &lt;code>false&lt;/code>。此设置将指示已部署服务使用 HTTPS（而不是 HTTP ）来访问外部服务。&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-details-v2.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/bookinfo/platform/kube/bookinfo-details-v2.yaml@ --dry-run -o yaml | kubectl set env --local -f - &amp;#39;DO_NOT_ENCRYPT=false&amp;#39; -o yaml | kubectl apply -f -
&lt;/code>&lt;/pre>&lt;/div>
&lt;p>现在，应用程序的更新架构如下所示：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:65.1654485092242%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-https/bookinfo-details-v2.svg" title="Bookinfo 的 details V2 应用程序">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-https/bookinfo-details-v2.svg" alt="Bookinfo 的 details V2 应用程序" />
&lt;/a>
&lt;/div>
&lt;figcaption>Bookinfo 的 details V2 应用程序&lt;/figcaption>
&lt;/figure>
&lt;p>请注意，Google Book 服务位于 Istio 服务网格之外，其边界由虚线标记。&lt;/p>
&lt;p>现在让我们将指向 &lt;em>details&lt;/em> 微服务的所有流量定向到 &lt;em>details v2&lt;/em>：&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/virtual-service-details-v2.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/bookinfo/networking/virtual-service-details-v2.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;p>请注意，&lt;code>VirtualService&lt;/code> 依赖于您在&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E5%BA%94%E7%94%A8%E7%BC%BA%E7%9C%81%E7%9B%AE%E6%A0%87%E8%A7%84%E5%88%99">应用默认目标规则&lt;/a>部分中创建的目标规则。&lt;/p>
&lt;p>在&lt;a href="/v1.1/zh/docs/examples/bookinfo/#%E7%A1%AE%E5%AE%9A-ingress-%E7%9A%84-ip-%E5%92%8C%E7%AB%AF%E5%8F%A3">确定 ingress 的 IP 和端口&lt;/a>之后，
让我们访问应用程序的网页。&lt;/p>
&lt;p>糟糕&amp;hellip;页面显示 &lt;em>Error fetching product details&lt;/em>，而不是书籍详细信息：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:36.18649965205289%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-https/errorFetchingBookDetails.png" title="获取产品详细信息的错误消息">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-https/errorFetchingBookDetails.png" alt="获取产品详细信息的错误消息" />
&lt;/a>
&lt;/div>
&lt;figcaption>获取产品详细信息的错误消息&lt;/figcaption>
&lt;/figure>
&lt;p>好消息是我们的应用程序没有崩溃, 通过良好的微服务设计，我们没有让&lt;strong>故障扩散&lt;/strong>。在我们的例子中，
失败的 &lt;em>details&lt;/em> 微服务不会导致 &lt;code>productpage&lt;/code> 微服务失败, 尽管 &lt;em>details&lt;/em> 微服务失败，
仍然提供了应用程序的大多数功能, 我们有&lt;strong>优雅的服务降级&lt;/strong>：正如您所看到的，评论和评级正确显示，
应用程序仍然有用。&lt;/p>
&lt;p>那可能出了什么问题？ 啊&amp;hellip;&amp;hellip;答案是我忘了启用从网格内部到外部服务的流量，在本例中是 Google Book Web 服务。
默认情况下，Istio sidecar 代理（&lt;a href="https://www.envoyproxy.io">Envoy proxies&lt;/a>）
&lt;strong>阻止到集群外目的地的所有流量&lt;/strong>, 要启用此类流量，我们必须定义&lt;a href="/v1.1/zh/docs/reference/config/istio.networking.v1alpha3/#serviceentry">mesh-external service entry&lt;/a>。&lt;/p>
&lt;h3 id="-google-books--https-">启用对 Google Books 网络服务的 HTTPS 访问&lt;/h3>
&lt;p>不用担心，让我们定义&lt;strong>网格外部 &lt;code>ServiceEntry&lt;/code>&lt;/strong> 并修复我们的应用程序。您还必须定义 &lt;em>virtual
service&lt;/em> 使用 &lt;a href="https://en.wikipedia.org/wiki/Server_Name_Indication">SNI&lt;/a>对外部服务执行路由。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: googleapis
spec:
hosts:
- www.googleapis.com
ports:
- number: 443
name: https
protocol: HTTPS
location: MESH_EXTERNAL
resolution: DNS
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: googleapis
spec:
hosts:
- www.googleapis.com
tls:
- match:
- port: 443
sni_hosts:
- www.googleapis.com
route:
- destination:
host: www.googleapis.com
port:
number: 443
weight: 100
EOF
&lt;/code>&lt;/pre>
&lt;p>现在访问应用程序的网页会显示书籍详细信息而不会出现错误：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:34.82831114225648%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-https/externalBookDetails.png" title="正确显示书籍详细信息">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-https/externalBookDetails.png" alt="正确显示书籍详细信息" />
&lt;/a>
&lt;/div>
&lt;figcaption>正确显示书籍详细信息&lt;/figcaption>
&lt;/figure>
&lt;p>您可以查询您的 &lt;code>ServiceEntry&lt;/code> ：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get serviceentries
NAME AGE
googleapis 8m
&lt;/code>&lt;/pre>
&lt;p>您可以删除您的 &lt;code>ServiceEntry&lt;/code> ：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete serviceentry googleapis
serviceentry &amp;#34;googleapis&amp;#34; deleted
&lt;/code>&lt;/pre>
&lt;p>并在输出中看到删除了 &lt;code>ServiceEntry&lt;/code>。&lt;/p>
&lt;p>删除 &lt;code>ServiceEntry&lt;/code> 后访问网页会产生我们之前遇到的相同错误，即 &lt;em>Error fetching product details&lt;/em>,
正如我们所看到的，，与许多其他 Istio 配置一样，&lt;code>ServiceEntry&lt;/code> 是&lt;strong>动态定义&lt;/strong>的 , Istio 运算符可以动态决定
它们允许微服务访问哪些域, 他们可以动态启用和禁用外部域的流量，而无需重新部署微服务。&lt;/p>
&lt;h3 id="-google--https-">清除对 Google 图书网络服务的 HTTPS 访问权限&lt;/h3>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-details-v2.yaml'>Zip&lt;/a>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/virtual-service-details-v2.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete serviceentry googleapis
$ kubectl delete virtualservice googleapis
$ kubectl delete -f @samples/bookinfo/networking/virtual-service-details-v2.yaml@
$ kubectl delete -f @samples/bookinfo/platform/kube/bookinfo-details-v2.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;h2 id="-istio--tls">由 Istio 发起的 TLS&lt;/h2>
&lt;p>这个故事有一个警告。假设您要监视您的微服务使用 &lt;a href="https://developers.google.com/apis-explorer/">Google API&lt;/a> 的哪个特定集
（&lt;a href="https://developers.google.com/books/docs/v1/getting_started">书籍&lt;/a>，&lt;a href="https://developers.google.com/calendar/">日历&lt;/a>，&lt;a href="https://developers.google.com/tasks/">任务&lt;/a>等）
假设您要强制执行仅允许使用&lt;a href="https://developers.google.com/books/docs/v1/getting_started">图书 API&lt;/a>的策略。
假设您要监控您的微服务访问的标识符。对于这些监视和策略任务，您需要知道 URL 路径。
考虑例如 URL &lt;a href="https://www.googleapis.com/books/v1/volumes?q=isbn:0486424618">&lt;code>www.googleapis.com/books/v1/volumes?q=isbn:0486424618&lt;/code>&lt;/a>。
在该网址中，路径段指定了&lt;a href="https://developers.google.com/books/docs/v1/getting_started">图书 API&lt;/a>
&lt;code>/books&lt;/code> 和路径段的 &lt;a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number">ISBN&lt;/a> 代码
&lt;code>/volumes?q=isbn:0486424618&lt;/code>。但是，在 HTTPS 中，所有 HTTP 详细信息（主机名，路径，标头等）都是加密的
sidecar 代理的这种监督和策略执行是无法实现的。Istio 只能通过 &lt;a href="https://tools.ietf.org/html/rfc3546#section-3.1">SNI&lt;/a>（&lt;em>Server Name Indication&lt;/em>）得知加密请求中的主机名称，在这里就是 &lt;code>www.googleapis.com&lt;/code>。&lt;/p>
&lt;p>为了允许 Istio 基于域执行出口请求的过滤，微服务必须发出 HTTP 请求, 然后，Istio 打开到目标的 HTTPS 连接（执行 TLS 发起）,
根据微服务是在 Istio 服务网格内部还是外部运行，
微服务的代码必须以不同方式编写或以不同方式配置, 这与&lt;a href="/v1.1/zh/docs/concepts/what-is-istio/#%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87">最大化透明度&lt;/a>
的 Istio 设计目标相矛盾, 有时我们需要妥协&amp;hellip;&amp;hellip;&lt;/p>
&lt;p>下图显示了如何执行外部服务的 HTTPS 流量, 在顶部，Istio 服务网格外部的微服务发送常规 HTTPS 请求，
端到端加密, 在底部，Istio 服务网格内的相同微服务必须在 pod 内发送未加密的 HTTP 请求，
这些请求被 sidecar Envoy 代理拦截 , sidecar 代理执行 TLS 发起，因此 pod 和外部服务之间的流量被加密。&lt;/p>
&lt;figure style="width:60%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:95.1355088590701%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2018/egress-https/./https_from_the_app.svg" title="对外发起 HTTPS 流量的两种方式：微服务自行发起，或由 Sidecar 代理发起">
&lt;img class="element-to-stretch" src="/v1.1/blog/2018/egress-https/./https_from_the_app.svg" alt="对外发起 HTTPS 流量的两种方式：微服务自行发起，或由 Sidecar 代理发起" />
&lt;/a>
&lt;/div>
&lt;figcaption>对外发起 HTTPS 流量的两种方式：微服务自行发起，或由 Sidecar 代理发起&lt;/figcaption>
&lt;/figure>
&lt;p>以下是我们如何在 &lt;a href="https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/src/details/details.rb">Bookinfo 的 details 微服务代码&lt;/a>
中使用 Ruby &lt;a href="https://docs.ruby-lang.org/en/2.0.0/Net/HTTP.html">net/http 模块&lt;/a>：&lt;/p>
&lt;pre>&lt;code class='language-ruby' data-expandlinks='true' >uri = URI.parse(&amp;#39;https://www.googleapis.com/books/v1/volumes?q=isbn:&amp;#39; + isbn)
http = Net::HTTP.new(uri.host, ENV[&amp;#39;DO_NOT_ENCRYPT&amp;#39;] === &amp;#39;true&amp;#39; ? 80:443)
...
unless ENV[&amp;#39;DO_NOT_ENCRYPT&amp;#39;] === &amp;#39;true&amp;#39; then
http.use_ssl = true
end
&lt;/code>&lt;/pre>
&lt;p>当定义 &lt;code>DO_NOT_ENCRYPT&lt;/code> 环境变量时，请求在没有 SSL（普通 HTTP ）的 80 端口下执行。&lt;/p>
&lt;p>我们将 &lt;code>DO_NOT_ENCRYPT&lt;/code> 环境变量设置为 &lt;em>&amp;ldquo;true&amp;rdquo;&lt;/em> &lt;a href="https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-details-v2.yaml">details 的部署配置文件&lt;/a>,
&lt;code>container&lt;/code> 部分：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >env:
- name: DO_NOT_ENCRYPT
value: &amp;#34;true&amp;#34;
&lt;/code>&lt;/pre>
&lt;p>在下一节中，您将配置 TLS 发起以访问外部 Web 服务。&lt;/p>
&lt;h2 id="-tls--bookinfo--google-books-">具有 TLS 的 Bookinfo 起源于 Google Books 网络服务&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>部署 &lt;em>details v2&lt;/em> 版本，将 HTTP 请求发送到 &lt;a href="https://developers.google.com/books/docs/v1/getting_started">Google Books API&lt;/a>。
在 &lt;a href="https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-details-v2.yaml">&lt;code>bookinfo-details-v2.yaml&lt;/code>&lt;/a> 中，
&lt;code>DO_NOT_ENCRYPT&lt;/code> 变量设置为 true。&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-details-v2.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/bookinfo/platform/kube/bookinfo-details-v2.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;li>
&lt;p>将指向 &lt;em>details&lt;/em> 微服务的流量定向到 &lt;em>details v2&lt;/em>。&lt;/p>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/virtual-service-details-v2.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f @samples/bookinfo/networking/virtual-service-details-v2.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;/li>
&lt;li>
&lt;p>为 &lt;code>www.google.apis&lt;/code> 创建网格外部 &lt;code>ServiceEntry&lt;/code>，并执行目标规则以执行 TLS 发起。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
name: googleapis
spec:
hosts:
- www.googleapis.com
ports:
- number: 80
name: http
protocol: HTTP
- number: 443
name: https
protocol: HTTPS
resolution: DNS
---
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: rewrite-port-for-googleapis
spec:
hosts:
- www.googleapis.com
http:
- match:
- port: 80
route:
- destination:
host: www.googleapis.com
port:
number: 443
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: originate-tls-for-googleapis
spec:
host: www.googleapis.com
trafficPolicy:
loadBalancer:
simple: ROUND_ROBIN
portLevelSettings:
- port:
number: 443
tls:
mode: SIMPLE # 访问 edition.cnn.com 时启动 HTTPS
EOF
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>访问应用程序的网页，并验证是否显示图书详细信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="/v1.1/zh/docs/tasks/telemetry/logs/access-log/#%E5%BC%80%E5%90%AF-Envoy-%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97">启用 Envoy’s 访问日志&lt;/a>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>检查 &lt;em>details v2&lt;/em> 的 sidecar 代理的日志，并查看 HTTP 请求。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl logs $(kubectl get pods -l app=details -l version=v2 -o jsonpath=&amp;#39;{.items[0].metadata.name}&amp;#39;) istio-proxy | grep googleapis
[2018-08-09T11:32:58.171Z] &amp;#34;GET /books/v1/volumes?q=isbn:0486424618 HTTP/1.1&amp;#34; 200 - 0 1050 264 264 &amp;#34;-&amp;#34; &amp;#34;Ruby&amp;#34; &amp;#34;b993bae7-4288-9241-81a5-4cde93b2e3a6&amp;#34; &amp;#34;www.googleapis.com:80&amp;#34; &amp;#34;172.217.20.74:80&amp;#34;
EOF
&lt;/code>&lt;/pre>
&lt;p>请注意日志中的 URL 路径，可以监视路径并根据它来应用访问策略。要了解有关 HTTP 出口流量的监控和访问策略
的更多信息，请查看&lt;a href="https://archive.istio.io/v0.8/blog/2018/egress-monitoring-access-control/#logging">归档博客之出口流量监控之日志&lt;/a>。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="-tls--google-books-">清除 TLS 原始数据到 Google Books 网络服务&lt;/h3>
&lt;div>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/platform/kube/bookinfo-details-v2.yaml'>Zip&lt;/a>&lt;a data-skipendnotes='true' style='display:none' href='https://raw.githubusercontent.com/istio/istio/release-1.1/samples/bookinfo/networking/virtual-service-details-v2.yaml'>Zip&lt;/a>&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl delete serviceentry googleapis
$ kubectl delete virtualservice rewrite-port-for-googleapis
$ kubectl delete destinationrule originate-tls-for-googleapis
$ kubectl delete -f @samples/bookinfo/networking/virtual-service-details-v2.yaml@
$ kubectl delete -f @samples/bookinfo/platform/kube/bookinfo-details-v2.yaml@
&lt;/code>&lt;/pre>&lt;/div>
&lt;h3 id="istio--tls-">Istio 双向 TLS 的关系&lt;/h3>
&lt;p>请注意，在这种情况下，TLS 的源与 Istio 应用的 &lt;a href="/v1.1/zh/docs/concepts/security/#%E5%8F%8C%E5%90%91-tls-%E8%AE%A4%E8%AF%81">双向 TLS&lt;/a> 无关,
无论 Istio 双向 TLS 是否启用，外部服务的 TLS 源都将起作用 , 保证服务网&lt;strong>内&lt;/strong>的服务到服务通信，
并为每个服务提供强大的身份认证, 在此博客文章中的 &lt;strong>外部服务&lt;/strong>的情况下，我们有&lt;strong>单向&lt;/strong> TLS，
这是用于保护 Web 浏览器和 Web 服务器之间通信的相同机制 , TLS 应用于与外部服务的通信，
以验证外部服务器的身份并加密流量。&lt;/p>
&lt;h2 id="heading-1">结论&lt;/h2>
&lt;p>在这篇博文中，我演示了 Istio 服务网格中的微服务如何通过 HTTPS 使用外部 Web 服务, 默认情况下，
Istio 会阻止集群外主机的所有流量, 要启用此类流量，请使用 mesh-external,必须为服务网格创建 &lt;code>ServiceEntry&lt;/code> ,
可以通过 HTTPS 访问外部站点，当微服务发出 HTTPS 请求时，流量是端到端加密的，但是 Istio 无法监视 HTTP 详细信息，
例如请求的 URL 路径。当微服务发出 HTTP 请求时，Istio 可以监视请求的 HTTP 详细信息并强制执行基于 HTTP 的访问策略。
但是，在这种情况下，微服务和 sidecar 代理之间的流量是未加密的。在具有非常严格的安全要求的组织中，
可以禁止未加密的部分流量。&lt;/p></description><pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2018/egress-https/</link><author>Vadim Eisenberg</author><guid isPermaLink="true">/v1.1/zh/blog/2018/egress-https/</guid><category>traffic-management</category><category>egress</category><category>https</category></item><item><title>Mixer 和 SPOF 神话</title><description>&lt;p>Mixer 出现在请求路径上，很自然的会引发一个疑问：他对系统可用性和延迟会产生什么样的影响？第一次看到 Istio 架构图时，人们最常见的问题就是：&amp;ldquo;这不就是一个单点失败的典型案例么？”&lt;/p>
&lt;p>本文中我们会深入挖掘和阐述 Mixer 的设计原则，在这些设计原则的支持下 Mixer 能够令人惊奇的提高网格内的系统可用性，降低平均请求延时。&lt;/p>
&lt;p>Istio 的 Mixer 对系统总体可用性和延迟有两个主要的好处：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>提高 SLO&lt;/strong>：Mixer 把 Proxy 和服务从基础设施后端的故障中隔离出来，提供了高级、高效的网格可用性保障。作为一个整体来说，在同基础设施后端的交互中，有了 Mixer 的帮助，会有更低的故障率。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>降低延迟&lt;/strong>：通过对各个层次的分片缓存的积极使用和共享，Mixer 能够降低平均延迟。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>接下来会对上面的内容进行一下解释。&lt;/p>
&lt;h2 id="istio-">Istio 是怎么来的&lt;/h2>
&lt;p>Google 在多年中都在使用一个内部的 API 和服务管理系统，用于处理 Google 提供的众多 API。这一系统支持了最大的服务群（Google Maps、YouTube 以及 Gmail 等），承受上百万 QPS 峰值的冲击。这套系统运行的虽然很好，但是仍然无法跟上 Google 快速增长的脚步，很显然，要有新的架构来降低飞涨的运维成本。&lt;/p>
&lt;p>2014 年，我们开始了一个草案，准备替换这一系统，进行更好的伸缩。这一决定最后证明是非常正确的，在 Google 进行整体部署之后，每月降低了上百万美元的运维成本。&lt;/p>
&lt;p>过去，流量在进入具体的服务之前，首先会进入一个较重的代理，旧系统就是以这个代理为中心构建的。新的架构摒弃了共享代理的设计，用轻量高效的 Sidecar 代理取而代之，这一代理和服务实例并行，共享一个控制平面。&lt;/p>
&lt;figure style="width:75%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:74.79295535770372%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2017/mixer-spof-myth/./mixer-spof-myth-1.svg" title="Google 系统拓扑">
&lt;img class="element-to-stretch" src="/v1.1/blog/2017/mixer-spof-myth/./mixer-spof-myth-1.svg" alt="Google 系统拓扑" />
&lt;/a>
&lt;/div>
&lt;figcaption>Google 的 API 和 服务管理系统&lt;/figcaption>
&lt;/figure>
&lt;p>看起来很面熟吧？是的，跟 Istio 很像。Istio 就是作为这一分布式代理架构的继任者进行构思的。我们从内部系统中获取了核心的灵感，在同合作伙伴的协同工作中产生了很多概念，这些导致了 Istio 的诞生。&lt;/p>
&lt;h2 id="heading">架构总结&lt;/h2>
&lt;p>下图中，Mixer 在 Mesh 和基础设施之间：&lt;/p>
&lt;figure style="width:75%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:65.89948886170049%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2017/mixer-spof-myth/./mixer-spof-myth-2.svg" title="Istio 拓扑">
&lt;img class="element-to-stretch" src="/v1.1/blog/2017/mixer-spof-myth/./mixer-spof-myth-2.svg" alt="Istio 拓扑" />
&lt;/a>
&lt;/div>
&lt;figcaption>Istio 拓扑&lt;/figcaption>
&lt;/figure>
&lt;p>逻辑上，Envoy Sidecar 会在每次请求之前调用 Mixer，进行前置检查，每次请求之后又要进行指标报告。Sidecar 中包含本地缓存，一大部分的前置检查可以通过缓存来进行。另外，Sidecar 会把待发送的指标数据进行缓冲，这样可能在几千次请求之后才调用一次 Mixer。前置检查和请求处理是同步的，指标数据上送是使用 fire-and-forget 模式异步完成的。&lt;/p>
&lt;p>抽象一点说，Mixer 提供：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>后端抽象&lt;/strong>：Mixer 把 Istio 组件和网格中的服务从基础设施细节中隔离开来。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>中间人&lt;/strong>：Mixer 让运维人员能够对所有网格和基础设施后端之间的交互进行控制。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>除了这些纯功能方面，Mixer 还有一些其他特点，为系统提供更多益处。&lt;/p>
&lt;h3 id="mixerslo-">Mixer：SLO 助推器&lt;/h3>
&lt;p>有人说 Mixer 是一个 SPOF，会导致 Mesh 的崩溃，而我们认为 Mixer 增加了 Mesh 的可用性。这是如何做到的？下面是三个理由：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>无状态&lt;/strong>：Mixer 没有状态，他不管理任何自己的持久存储。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>稳固&lt;/strong>：Mixer 是一个高可靠性的组件，设计要求所有 Mixer 实例都要有超过 99.999% 的可靠性。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>缓存和缓冲&lt;/strong>：Mixer 能够积累大量的短期状态数据。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Sidecar 代理伴随每个服务实例而运行，必须节约使用内存，这样就限制了本地缓存和缓冲的数量。但是 Mixer 是独立运行的，能使用更大的缓存和缓冲。因此 Mixer 为 Sidecar 提供了高伸缩性高可用的二级缓存服务。&lt;/p>
&lt;p>Mixer 的预期可用性明显高于多数后端（多数是 99.9%）。他的本地缓存和缓冲区能够在后端无法响应的时候继续运行，因此有助于对基础设施故障的屏蔽，降低影响。&lt;/p>
&lt;h3 id="mixer">Mixer：延迟削减器&lt;/h3>
&lt;p>上面我们解释过，Istio Sidecar 具备有效的一级缓存，在为流量服务的时候多数时间都可以使用缓存来完成。Mixer 提供了更大的共享池作为二级缓存，这也帮助了 Mixer 降低平均请求的延迟。&lt;/p>
&lt;p>不只是降低延迟，Mixer 还降低了 Mesh 到底层的请求数量，这样就能显著降低到基础设施后端的 QPS，如果你要付款给这些后端，那么这一优点就会节省更多成本。&lt;/p>
&lt;h2 id="heading-1">下一步&lt;/h2>
&lt;p>我们还有机会对系统做出更多改进。&lt;/p>
&lt;h3 id="heading-2">以金丝雀部署的方式进行配置发布&lt;/h3>
&lt;p>Mixer 具备高度的伸缩性，所以他通常不会故障。然而如果部署了错误的配置，还是会引发 Mixer 进程的崩溃。为了防止这种情况的出现，可以用金丝雀部署的方式来发布配置，首先为一小部分 Mixer 进行部署，然后扩大部署范围。&lt;/p>
&lt;p>目前的 Mixer 并未具备这样的能力，我们期待这一功能成为 Istio 可靠性配置工作的一部分最终得以发布。&lt;/p>
&lt;h3 id="heading-3">缓存调优&lt;/h3>
&lt;p>我们的 Sidecar 和 Mixer 缓存还需要更好的调整，这部分的工作会着眼于资源消耗的降低和性能的提高。&lt;/p>
&lt;h3 id="heading-4">缓存共享&lt;/h3>
&lt;p>现在 Mixer 的实例之间是各自独立的。一个请求在被某个 Mixer 实例处理之后，并不会把过程中产生的缓存传递给其他 Mixer 实例。我们最终会试验使用 Memcached 或者 Redis 这样的分布式缓存，以期提供一个网格范围内的共享缓存，更好的降低对后端基础设施的调用频率。&lt;/p>
&lt;h3 id="heading-5">分片&lt;/h3>
&lt;p>在大规模的网格中，Mixer 的负载可能很重。我们可以使用大量的 Mixer 实例，每个实例都为各自承担的流量维护各自的缓存。我们希望引入智能分片能力，这样 Mixer 实例就能针对特定的数据流提供特定的服务，从而提高缓存命中率；换句话说，分片可以利用把相似的流量分配给同一个 Mixer 实例的方式来提高缓存效率，而不是把请求交给随机选择出来的 Mixer 实例进行处理。&lt;/p>
&lt;h2 id="heading-6">结语&lt;/h2>
&lt;p>Google 的实际经验展示了轻代理、大缓存控制平面结合的好处：提供更好的可用性和延迟。过去的经验帮助 Istio 构建了更精确更有效的缓存、预抓取以及缓冲策略等功能。我们还优化了通讯协议，用于降低缓存无法命中的时候，对性能产生的影响。&lt;/p>
&lt;p>Mixer 还很年轻。在 Istio 0.3 中，Mixer 并没有性能方面的重要改进。这意味着如果一个请求没有被 Sidecar 缓存命中，Mixer 就会花费更多时间。未来的几个月中我们会做很多工作来优化同步的前置检查过程中的这种情况。&lt;/p>
&lt;p>我们希望本文能够让读者能够意识到 Mixer 对 Istio 的益处。&lt;/p>
&lt;p>如果有说明或者问题，无需犹豫，&lt;/p></description><pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2017/mixer-spof-myth/</link><author>Martin Taillefer</author><guid isPermaLink="true">/v1.1/zh/blog/2017/mixer-spof-myth/</guid></item><item><title>Mixer 适配器模型</title><description>&lt;p>Istio 0.2 引入了一种新的 Mixer 适配器模型，这种模型使接入后端基础设施具有更多的灵活性 。本文将解释这种模型是如何工作的。&lt;/p>
&lt;h2 id="heading">为什么是适配器模型?&lt;/h2>
&lt;p>后端基础设施提供了支持服务构建的功能。他们包括访问控制、遥测、配额控制、计费系统等等。传统服务会直接与这些后端系统集成，并与后端紧密耦合，并集成到其中的个性化语义和操作。&lt;/p>
&lt;p>Mixer 服务作为 Istio 和一套开放式基础设施之间的抽象层。Istio 组件和运行在 Service Mesh 中的服务，通过 Mixer 就可以在不直接访问后端接口的情况下和这些后端进行交互。&lt;/p>
&lt;p>除了作为应用层与基础设施隔离外，Mixer 提供了一种中介模型，这种模型允许注入和控制应用和后端的策略。操作人员可以控制哪些数据汇报给哪个后端，哪个后端提供授权等等。&lt;/p>
&lt;p>考虑到每个基础服务都有不同的接口和操作模型，Mixer 需要用户通过代码来解决这些差异，我们可以称这些用户自己封装的代码为&lt;a href="https://github.com/istio/istio/wiki/Mixer-Compiled-In-Adapter-Dev-Guide">&lt;em>适配器&lt;/em>&lt;/a>。&lt;/p>
&lt;p>适配器以 Go 包的形式直接链接到 Mixer 二进制中。如果默认的适配器不能满足特定的使用需求，自定义适配器也是很简单的。&lt;/p>
&lt;h2 id="heading-1">设计哲学&lt;/h2>
&lt;p>Mixer 本质上就是一个处理属性和路由的机器。代理将&lt;a href="/v1.1/zh/docs/concepts/policies-and-telemetry/#%E5%B1%9E%E6%80%A7">属性&lt;/a>作为预检和遥测报告的一部分发送出来，并且转换为一系列对适配器的调用。运维人员提供了用于描述如何将传入的属性映射为适配器的配置。&lt;/p>
&lt;figure style="width:60%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:42.60859894197215%">
&lt;a data-skipendnotes="true" href="/v1.1/docs/concepts/policies-and-telemetry/machine.svg" title="Attribute Machine">
&lt;img class="element-to-stretch" src="/v1.1/docs/concepts/policies-and-telemetry/machine.svg" alt="Attribute Machine" />
&lt;/a>
&lt;/div>
&lt;figcaption>Attribute Machine&lt;/figcaption>
&lt;/figure>
&lt;p>配置是一个复杂的任务。有证据表明绝大多数服务中断是由配置错误造成的。为了帮助解决这一问题，Mixer 的配置模型通过做限制来避免错误。例如，在配置中使用强类型，以此来确保在上下文环境中使用了有意义的属性或者属性表达式。&lt;/p>
&lt;h2 id="handlers-">Handlers: 适配器的配置&lt;/h2>
&lt;p>Mixer 使用的每个适配器都需要一些配置才能运行。一般来说，适配器需要一些信息。例如，到后端的 URL 、证书、缓存选项等等。每个适配器使用一个 &lt;a href="https://developers.google.com/protocol-buffers/">protobuf&lt;/a> 消息来定义所需要的配置数据。&lt;/p>
&lt;p>你可以通过创建 &lt;a href="/v1.1/zh/docs/concepts/policies-and-telemetry/#%E5%A4%84%E7%90%86%E5%99%A8-handler">&lt;em>handler&lt;/em>&lt;/a> 为适配器提供配置。Handler 就是一套能让一个适配器就绪的完整配置。对同一个适配器可以有任意数量的 Handler，这样就可以在不同的场景下复用了。&lt;/p>
&lt;h2 id="templates-">Templates: 适配输入结构&lt;/h2>
&lt;p>通常对于进入到 Mesh 服务中的请求，Mixer 会发生两次调用，一次是预检，一次是遥测报告。每一次调用，Mixer 都会调用一个或更多的适配器。不同的适配器需要不同的数据作为输入来处理。例如，日志适配器需要日志输入，metric 适配器需要 metric 数据作为输入，认证的适配器需要证书等等。Mixer &lt;a href="/v1.1/zh/docs/reference/config/policy-and-telemetry/templates/">&lt;em>templates&lt;/em>&lt;/a> 用来描述每次请求适配器消费的数据。&lt;/p>
&lt;p>每个 Template 被指定为 &lt;a href="https://developers.google.com/protocol-buffers/">protobuf&lt;/a> 消息。一个模板描述了一组数据，这些数据在运行时被传递给一个或多个适配器。一个适配器可以支持任意数量的模板，开发者还可以设计支持特定模板的是适配器。&lt;/p>
&lt;p>&lt;a href="/v1.1/zh/docs/reference/config/policy-and-telemetry/templates/metric/">&lt;code>metric&lt;/code>&lt;/a> 和 &lt;a href="/v1.1/zh/docs/reference/config/policy-and-telemetry/templates/logentry/">&lt;code>logentry&lt;/code>&lt;/a> 是两个最重要的模板，分别表示负载的单一指标，和到适当后端的单一日志条目。&lt;/p>
&lt;h2 id="instances-">Instances: 属性映射&lt;/h2>
&lt;p>你可以通过创建 &lt;a href="/v1.1/zh/docs/concepts/policies-and-telemetry/#%E5%AE%9E%E4%BE%8B-instance">&lt;em>instances&lt;/em>&lt;/a> 来决定哪些数据被传递给特定的适配器。Instances 决定了 Mixer 如何通过 &lt;a href="/v1.1/zh/docs/concepts/policies-and-telemetry/#%E5%B1%9E%E6%80%A7">attributes&lt;/a> 把来自代理的属性拆分为各种数据然后分发给不同的适配器。&lt;/p>
&lt;p>创建实例通常需要使用 &lt;a href="/v1.1/zh/docs/concepts/policies-and-telemetry/#%E5%B1%9E%E6%80%A7%E8%A1%A8%E8%BE%BE%E5%BC%8F">attribute expressions&lt;/a> 。这些表达式的功能是使用属性和常量来生成结果数据，用于给instance字段进行赋值。&lt;/p>
&lt;p>在模板中定义的每个 instance 字段、每个属性、每个表达式都有一个 &lt;a href="https://github.com/istio/api/blob/master/policy/v1beta1/value_type.proto">type&lt;/a>，只有兼容的数据类型才能进行赋值。例如不能把整型的表达式赋值给字符串类型。强类型设计的目的就是为了降低配置出错引发的风险。&lt;/p>
&lt;h2 id="rules-">Rules: 将数据交付给适配器&lt;/h2>
&lt;p>最后一个问题就是告诉 Mixer 哪个 instance 在什么时候发送给哪个 handler。这个通过创建 &lt;a href="/v1.1/zh/docs/concepts/policies-and-telemetry/#%E8%A7%84%E5%88%99-rule">&lt;em>rules&lt;/em>&lt;/a> 实现。每个规则都会指定一个特定的处理程序和要发送给该处理程序的示例。当 Mixer 处理一个调用时，它会调用指定的处理程序，并给他一组特定的处理实例。&lt;/p>
&lt;p>Rule 中包含有匹配断言，这个断言是一个返回布尔值的属性表达式。只有属性表达式断言成功的 Rule 才会生效，否则这条规则就形同虚设，当然其中的 Handler 也不会被调用。&lt;/p>
&lt;h2 id="heading-2">未来的工作&lt;/h2>
&lt;p>我们正在努力改进和提升适配器的使用及开发。例如，计划中包含很多新特性使用户更加方便地使用 Templates。另外，表达式语言也正在不断的发展和成熟。&lt;/p>
&lt;p>长远来看，我们正在寻找不直接将适配器直接连接到 Mixer 二进制的方法。这将简化部署和开发使用。&lt;/p>
&lt;h2 id="heading-3">结论&lt;/h2>
&lt;p>新的 Mixer 适配器模型的设计是为了提供一个灵活的框架用来支持一个开放基础设施。&lt;/p>
&lt;p>Handler 为各个适配器提供了配置数据，Template 用于在运行时确定不同的适配器所需的数据类型，Instance 让运维人员准备这些数据，Rule 将这些数据提交给一个或多个 Handler 进行处理。&lt;/p>
&lt;p>更多信息可以关注&lt;a href="/v1.1/zh/docs/concepts/policies-and-telemetry/">这里&lt;/a>。更多关于 templates, handlers,和 rules 的内容可以关注&lt;a href="/v1.1/zh/docs/reference/config/policy-and-telemetry/">这里&lt;/a>。你也可以在&lt;a href="https://github.com/istio/istio/tree/release-1.1/samples/bookinfo">这里&lt;/a>找到对应的示例。&lt;/p></description><pubDate>Fri, 03 Nov 2017 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2017/adapter-model/</link><author>Martin Taillefer</author><guid isPermaLink="true">/v1.1/zh/blog/2017/adapter-model/</guid><category>adapters</category><category>mixer</category><category>policies</category><category>telemetry</category></item><item><title>宣布 Istio 0.2</title><description>&lt;p>我在2017年5月24日发布了 Istio ，它是一个用于连接、管理、监控和保护微服务的开放平台。看着饱含浓厚兴趣的开发者、运营商、合作伙伴和不断发展的社区，我们感到十分的欣慰。我们 0.1 版本的重点是展示 Istio 在 Kubernetes 中的所有概念。&lt;/p>
&lt;p>今天我们十分高兴地宣布推出 0.2 版本，它提高了稳定性和性能、允许在 Kubernetes 集群中广泛部署并自动注入 sidecar 、为 TCP 服务添加策略和身份验证、同时保证扩展网格收录那些部署在虚拟机中的服务。此外，Istio 可以利用 Consul/Nomad 或 Eureka 在 Kubernetes 外部运行。 除了核心功能，Istio 的扩展已经准备由第三方公司和开发人员编写。&lt;/p>
&lt;h2 id="02">0.2版本的亮点&lt;/h2>
&lt;h3 id="heading">可用性改进&lt;/h3>
&lt;ul>
&lt;li>&lt;em>支持多命名空间&lt;/em>: Istio 现在可以跨多个名称空间在集群范围内工作，这也是来自 0.1 版本中社区最强烈的要求之一。&lt;/li>
&lt;li>&lt;em>TCP 服务的策略与安全&lt;/em>: 除了 HTTP ，我们还为 TCP 服务增加了透明双向 TLS 认证和策略实施。这将让拥有像遥测，策略和安全等 Istio 功能的同时，保护更多 Kubernetes deployment 。&lt;/li>
&lt;li>&lt;em>自动注入 sidecar&lt;/em>: 通过利用 Kubernetes 1.7 提供的 alpha &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">初始化程序&lt;/a> ，当您的集群启用了该程序时，envoy sidecar 就可以自动注入到应用的 deployment 里。 这使得你可以使用 &lt;code>kubectl&lt;/code> 命令部署微服务， 这与您通常在没有 Istio 的情况下部署微服务的命令完全相同。&lt;/li>
&lt;li>&lt;em>扩展 Istio&lt;/em> : 改进的 Mixer 设计，可以允许供应商编写 Mixer 适配器以实现对其自身系统的支持，例如应用管理或策略实施。该 &lt;a href="https://github.com/istio/istio/wiki/Mixer-Compiled-In-Adapter-Dev-Guide">Mixer 适配器开发指南&lt;/a> 可以轻松的帮你将 Istio 集成于你的解决方案。&lt;/li>
&lt;li>&lt;em>使用您自己的 CA 证书&lt;/em>: 允许用户提供自己的密钥和证书给 Istio CA 和永久 CA 密钥/证书存储，允许在持久化存储中提供签名密钥/证书，以便于 CA 重启。&lt;/li>
&lt;li>&lt;em>改进路由和指标&lt;/em>: 支持 WebSocket 、MongoDB 和 Redis 协议。 您可以将弹性功能（如熔断器）应用于第三方服务。除了 Mixer 的指标外，数以百计 Envoy 指标现在已经在 Prometheus 中可见，它们用于监控 Istio 网格中的流量吞吐。&lt;/li>
&lt;/ul>
&lt;h3 id="heading-1">跨环境支持&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;em>网格扩展&lt;/em>: Istio 网格现在可以在 Kubernetes 之外跨服务 —— 就像那些运行在虚拟机中的服务一样，他们同时享受诸如自动双向 TLS认证、流量管理、遥测和跨网格策略实施带来的好处。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>运行在 Kubernetes 外部&lt;/em>: 我们知道许多客户使用其他的服务注册中心和 orchestration 解决方案（如 &lt;a href="/v1.1/docs/setup/consul/quick-start/">Consul/Nomad&lt;/a> 和 Eureka）， Istio Pilot 可以在 Kubernetes 外部单独运行，同时从这些系统中获取信息，并在虚拟机或容器中管理 Envoy fleet 。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="-istio-">加入到塑造 Istio 未来的队伍中&lt;/h2>
&lt;p>呈现在我们面前的是一幅不断延伸的&lt;a href="/v1.1/about/feature-stages/">蓝图&lt;/a> ，它充满着强大的潜能。我们将在下个版本致力于 Istio 的稳定性，可靠性，第三方工具集成和多集群用例。&lt;/p>
&lt;p>想要了解如何参与并为 Istio 的未来做出贡献，请查看我们在 GitHub 的&lt;a href="https://github.com/istio/community">社区&lt;/a>项目，它将会向您介绍我们的工作组，邮件列表，各种社区会议，常规流程和指南。&lt;/p>
&lt;p>我们要感谢为我们测试新版本、提交错误报告、贡献代码、帮助其他成员以及通过参与无数次富有成效的讨论塑造 Istio 的出色社区，这让我们的项目自启动以来在GitHub上累积了3000颗星，并且在 Istio 邮件列表上有着数百名活跃的社区成员。&lt;/p>
&lt;p>谢谢&lt;/p></description><pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2017/0.2-announcement/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2017/0.2-announcement/</guid></item><item><title>Istio 使用网络策略</title><description>&lt;p>使用网络策略去保护运行在 Kubernetes 上的应用程序现在是一种广泛接受的行业最佳实践。 鉴于 Istio 也支持策略，我们希望花一些时间来解释 Istio 策略和 Kubernetes 网络策略的相互作用和互相支持提供应用程序的安全。&lt;/p>
&lt;p>让我们从基础开始：为什么你想要同时使用 Istio 和 Kubernetes 网络策略？ 简短的回答是它们处理不同的事情。 表格列出 Istio 和网络策略之间的主要区别（我们将描述“典型”实现，例如：Calico，但具体实现细节可能因不同的网络提供商而异）：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>Istio 策略&lt;/th>
&lt;th>网络策略&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;strong>层级&lt;/strong>&lt;/td>
&lt;td>&amp;quot;服务&amp;rdquo; &amp;mdash; L7&lt;/td>
&lt;td>&amp;quot;网络&amp;rdquo; &amp;mdash; L3-4&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>实现&lt;/strong>&lt;/td>
&lt;td>用户空间&lt;/td>
&lt;td>内核&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;strong>执行点&lt;/strong>&lt;/td>
&lt;td>Pod&lt;/td>
&lt;td>节点&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="heading">层级&lt;/h2>
&lt;p>从 OSI 模型的角度来看7层（应用程序），Istio 策略运行在网络应用程序的“服务”层。但事实上云原生应用程序模型是7层实际上至少包含两层：服务层和内容层。服务层通常是 HTTP ，它封装了实际的应用程序数据（内容层）。Istio 的 Envoy 代理运行的 HTTP 服务层。相比之下，网络策略在 OSI 模型中的第3层（网络）和第4层（传输）运行。&lt;/p>
&lt;p>运行在服务层为 Envoy 代理提供了一组丰富的属性，以便基础协议进行策略决策，其中包括 HTTP/1.1 和 HTTP/2（ gRPC 运行在 HTTP/2 上）。因此，您可以基于虚拟主机、URL或其他 HTTP 头部应用策略。在未来，Istio 将支持广泛的7层协议、以及通用的 TCP 和 UDP 传输。&lt;/p>
&lt;p>相比之下，Istio 策略运行在网络层具有通用的优势，因为所有网络应用程序都使用IP。无论7层协议如何，您都可以在网络层应用策略：DNS 、SQL 数据库、实时流以及许多不使用 HTTP 的其他服务都可以得到保护。网络策略不仅限于经典防火墙的 IP 地址、 协议和端口三元组， Istio 和网络策略都可以使用丰富的 Kubernetes 标签来描述 pod 端点。&lt;/p>
&lt;h2 id="heading-1">实现&lt;/h2>
&lt;p>Istio 的代理基于 &lt;a href="https://envoyproxy.github.io/envoy/">&lt;code>Envoy&lt;/code>&lt;/a>，它作为数据平面的用户空间守护进程实现的，使用标准套接字与网络层交互。这使它在处理方面具有很大的灵活性，并允许它在容器中分发（和升级！）。&lt;/p>
&lt;p>网络策略数据平面通常在内核空间中实现（例如：使用 iptables 、eBPF 过滤器、或甚至自定义内核模块）。在内核空间使它们性能很好，但不像 Envoy 代理那样灵活。&lt;/p>
&lt;h2 id="heading-2">执行点&lt;/h2>
&lt;p>Envoy 代理的策略执行是在 pod 中，作为同一网络命名空间中的 sidecar 容器。这使得部署模型简单。某些容器赋予权限可以重新配置其 pod 中的网络（CAP_NET_ADMIN）。如果此类服务实例绕过代理受到损害或行为不当（如：在恶意租户中）。&lt;/p>
&lt;p>虽然这不会让攻击者访问其他启用了 Istio 的 pod ，但通过配置，会打开几种攻击：&lt;/p>
&lt;ul>
&lt;li>攻击未受保护的 pods&lt;/li>
&lt;li>尝试通过发送大量流量为受保护的 pods 造成访问拒绝&lt;/li>
&lt;li>在 pod 中收集的漏出数据&lt;/li>
&lt;li>攻击集群基础设施（ 服务器或 Kubernetes 服务）&lt;/li>
&lt;li>攻击网格外的服务，如数据库，存储阵列或遗留系统。&lt;/li>
&lt;/ul>
&lt;p>网络策略通常在客户机的网络命名空间之外的主机节点处执行。 这意味着必须避免受损或行为不当的 pod 进入根命名空间的执行。 通过在 Kubernetes 1.8 中添加 egress 策略，这是网络策略成为保护基础设施免受工作负载受损的关键部分。&lt;/p>
&lt;h2 id="heading-3">举例&lt;/h2>
&lt;p>让我们来看一些Istio应用程序使用 Kubernetes 网络策略的示例。 下面我们以 Bookinfo 应用程序为例，介绍网络策略功能的用例：&lt;/p>
&lt;ul>
&lt;li>减少应用程序入口的攻击面&lt;/li>
&lt;li>在应用程序中实现细粒度隔离&lt;/li>
&lt;/ul>
&lt;h3 id="heading-4">减少应用程序入口的攻击面&lt;/h3>
&lt;p>应用程序的 ingress 控制器是外部世界进入我们应用程序的主要入口。 快速查看 &lt;code>istio.yaml&lt;/code> （用于安装 Istio ）定义了 Istio-ingress，如下所示：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: v1
kind: Service
metadata:
name: istio-ingress
labels:
istio: ingress
spec:
type: LoadBalancer
ports:
- port: 80
name: http
- port: 443
name: https
selector:
istio: ingress
&lt;/code>&lt;/pre>
&lt;p>&lt;code>istio-ingress&lt;/code> 暴露端口 80 和 443 . 我们需要将流入流量限制在这两个端口上。 Envoy 有&lt;a href="https://www.envoyproxy.io/docs/envoy/latest/operations/admin.html#operations-admin-interface">&lt;code>内置管理接口&lt;/code>&lt;/a>，我们不希望错误配置 &lt;code>istio-ingress&lt;/code> 镜像而导致意外地将我们的管理接口暴露给外界。这里深度防御的示例：正确配置的镜像应该暴露接口，正确配置的网络策略将阻止任何人连接到它，要么失败，要么配置错误，受到保护。&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
name: istio-ingress-lockdown
namespace: default
spec:
podSelector:
matchLabels:
istio: ingress
ingress:
- ports:
- protocol: TCP
port: 80
- protocol: TCP
port: 443
&lt;/code>&lt;/pre>
&lt;h3 id="heading-5">在应用程序中实现细粒度隔离&lt;/h3>
&lt;p>如下是 Bookinfo 应用程序的服务示意图：&lt;/p>
&lt;figure style="width:80%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:59.086918235567985%">
&lt;a data-skipendnotes="true" href="/v1.1/docs/examples/bookinfo/withistio.svg" title="Bookinfo Service Graph">
&lt;img class="element-to-stretch" src="/v1.1/docs/examples/bookinfo/withistio.svg" alt="Bookinfo Service Graph" />
&lt;/a>
&lt;/div>
&lt;figcaption>Bookinfo Service Graph&lt;/figcaption>
&lt;/figure>
&lt;p>此图显示了一个正确功能的应用程序应该允许的每个连接。 所有其他连接，例如从 Istio Ingress 直接到 Rating 服务，不是应用程序的一部分。 让我们排除那些无关的连接，它们不能被攻击者所用。 例如：想象一下，Ingress pod 受到攻击者的攻击，允许攻击者运行任意代码。 如果我们使用网络策略只允许连接到 &lt;code>productpage&lt;/code>（&lt;code>http://$GATEWAY_URL/productpage&lt;/code>）的 Pod ，则攻击者不再获得对我的应用程序后端的访问权限，尽管它们已经破坏了服务网格的成员。&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
name: product-page-ingress
namespace: default
spec:
podSelector:
matchLabels:
app: productpage
ingress:
- ports:
- protocol: TCP
port: 9080
from:
- podSelector:
matchLabels:
istio: ingress
&lt;/code>&lt;/pre>
&lt;p>推荐你可以而且应该为每个服务编写类似的策略，允许其他 pod 访问执行。&lt;/p>
&lt;h2 id="heading-6">总结&lt;/h2>
&lt;p>我们认为 Istio 和网络策略在应用策略方面有不同的优势。 Istio 具有应用协议感知和高度灵活性，非常适合应用策略来支持运营目标，如：服务路由、重试、熔断等，以及在应用层开启的安全性，例如：令牌验证。 网络策略是通用的、高效的、与 pod 隔离，使其成为应用策略以支持网络安全目标的理想选择。 此外，拥有在网络堆栈的不同层运行的策略是一件非常好的事情，因为它为每个层提供特定的上下文而不会混合状态并允许责任分离。&lt;/p>
&lt;p>这篇文章是基于 Spike Curtis 的三部分博客系列，他是 Tigera 的 Istio 团队成员之一。 完整系列可以在这里找到：&lt;a href="https://www.projectcalico.org/using-network-policy-in-concert-with-istio/">https://www.projectcalico.org/using-network-policy-in-concert-with-istio/&lt;/a>&lt;/p></description><pubDate>Thu, 10 Aug 2017 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2017/0.1-using-network-policy/</link><author>Spike Curtis</author><guid isPermaLink="true">/v1.1/zh/blog/2017/0.1-using-network-policy/</guid></item><item><title>使用 Istio 进行金丝雀部署</title><description>&lt;div>
&lt;aside class="callout tip">
&lt;div class="type">&lt;svg class="large-icon">&lt;use xlink:href="/v1.1/img/icons.svg#callout-tip"/>&lt;/svg>&lt;/div>
&lt;div class="content">本篇博客最后更新时间 2018 年 5 月 16 号，采用了最新版本的流量管理模型。&lt;/div>
&lt;/aside>
&lt;/div>
&lt;p>采用 &lt;a href="/v1.1/zh/">Istio&lt;/a> 项目的一大好处就是为服务金丝雀方式部署提供了控制便利。金丝雀部署（或上线）背后的想法是通过让一小部分用户流量引入的新版本进行测试，如果一切顺利，则可以增加（可能逐渐增加）百分比，逐步替换旧版本。如在过程中出现任何问题，则可以中止并回滚到旧版本。最简单的方式，是随机选择百分比请求到金丝雀版本，但在更复杂的方案下，则可以基于请求的区域，用户或其他属性。&lt;/p>
&lt;p>基于领域的专业水平，您可能想知道为什么需要 Istio 来支持金丝雀部署，因为像 Kubernetes 这样的平台已经提供了进行&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">版本上线&lt;/a>和&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments">金丝雀部署&lt;/a>的方法。问题解决了吗 ？不完全是。虽然以这种方式进行部署可以在简单的情况下工作，但功能非常有限，特别是在大规模自动缩放的云环境中大流量的情况下。&lt;/p>
&lt;h2 id="kubernetes-">Kubernetes 中的金丝雀部署&lt;/h2>
&lt;p>假设我们有一个已部署的 &lt;strong>helloworld&lt;/strong> 服务 &lt;strong>v1&lt;/strong> 版本，我们想要测试（或简单上线）新版本 &lt;strong>v2&lt;/strong>。使用 Kubernetes，您可以通过简单地更新服务的 &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment&lt;/a> 中的镜像并自动进行部署来&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#updating-a-deployment">上线&lt;/a>新版本的 &lt;strong>helloworld&lt;/strong> 服务。如果我们特能够小心保证在启动并且在仅启动一个或两个 v2 副本&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pausing-and-resuming-a-deployment">暂停&lt;/a>上线时有足够的 &lt;strong>v1&lt;/strong> 副本运行，则能够保持金丝雀发布对系统的影响非常小。后续我们可以观察效果，或在必要时进行&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-back-a-deployment">回滚&lt;/a>。最好，我们也能够对 Deployment 设置 &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling-a-deployment">HPA&lt;/a>，在上线过程中减少或增加副本以处理流量负载时，也能够保持副本比例一致。&lt;/p>
&lt;p>尽管这种机制能够很好工作，但这种方式只适用于部署的经过适当测试的版本，也就是说，更多的是蓝/绿发布，又称红/黑发布，而不是 “蜻蜓点水“ 式的金丝雀部署。实际上，对于后者（例如，并没有完全准备好或者无意对外暴露的版本），Kubernetes 中的金丝雀部署将使用具有&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively">公共 pod 标签&lt;/a>的两个 Deployment 来完成。在这种情况下，我们不能再使用自动缩放器，因为是有由两个独立的自动缩放器来进行控制，不同负载情况下，副本比例（百分比）可能与所需的比例不同。&lt;/p>
&lt;p>无论我们使用一个或者两个部署，使用 Docker，Mesos/Marathon 或 Kubernetes 等容器编排平台进行的金丝雀发布管理都存在一个根本问题：使用实例扩容来管理流量；版本流量分发和副本部署在上述平台中并独立。所有 pod 副本，无论版本如何，在 &lt;code>kube-proxy&lt;/code> 循环池中都被一视同仁地对待，因此管理特定版本接收的流量的唯一方法是控制副本比例。以小百分比维持金丝雀流量需要许多副本（例如，1％ 将需要至少 100 个副本）。即使我们可以忽略这个问题，部署方式功能仍然非常有限，因为它只支持简单（随机百分比）金丝雀部署。如果我们想根据某些特定规则将请求路由到金丝雀版本上，我们仍然需要另一种解决方案。&lt;/p>
&lt;h2 id="-istio">使用 Istio&lt;/h2>
&lt;p>使用 Istio，流量路由和副本部署是两个完全独立的功能。服务的 pod 数量可以根据流量负载灵活伸缩，与版本流量路由的控制完全正交。这在自动缩放的情况下能够更加简单地管理金丝雀版本。事实上，自动缩放管理器仍然独立运行，其在响应因流量路由导致的负载变化与其他原因导致负载变化的行为上没有区别。&lt;/p>
&lt;p>Istio 的&lt;a href="/v1.1/zh/docs/concepts/traffic-management/#%E8%A7%84%E5%88%99%E9%85%8D%E7%BD%AE">路由规则&lt;/a>也带来了其他的便利；你可以轻松实现细粒度控制流量百分比（例如，路由 1％ 的流量而不需要 100 个 pod），当然也可以使用其他规则来控制流量（例如，将特定用户的流量路由到金丝雀版本）。作为展示，让我们看一下采用这种方式部署 &lt;strong>helloworld&lt;/strong> 服务的简单便捷。&lt;/p>
&lt;p>首先我们定义 &lt;strong>helloworld&lt;/strong> 服务，和普通 &lt;strong>Kubernetes&lt;/strong> 服务一样，如下所示：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >apiVersion: v1
kind: Service
metadata:
name: helloworld
labels:
app: helloworld
spec:
selector:
app: helloworld
...
&lt;/code>&lt;/pre>
&lt;p>然后我们添加 2 个 Deployment，分别为版本 &lt;strong>v1&lt;/strong> 和 &lt;strong>v2&lt;/strong>，这两个版本都包含服务选择标签 &lt;code>app：helloworld&lt;/code> ：&lt;/p>
&lt;pre>&lt;code class='language-yaml' data-expandlinks='true' >kind: Deployment
metadata:
name: helloworld-v1
spec:
replicas: 1
template:
metadata:
labels:
app: helloworld
version: v1
spec:
containers:
- image: helloworld-v1
...
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
name: helloworld-v2
spec:
replicas: 1
template:
metadata:
labels:
app: helloworld
version: v2
spec:
containers:
- image: helloworld-v2
...
&lt;/code>&lt;/pre>
&lt;p>需要注意的是，这与使用普通 Kubernetes 进行&lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments">金丝雀部署&lt;/a>的方式完全相同，但是在 Kubernetes 方式下控制流量分配需要调整每个 Deployment 的副本数目。例如，将 10％ 的流量发送到金丝雀版本（v2），v1 和 v2 的副本可以分别设置为 9 和 1。&lt;/p>
&lt;p>但是在&lt;a href="/v1.1/zh/docs/setup/">启用 Istio&lt;/a> 的集群中，我们可以通过设置路由规则来控制流量分配。如将 10％ 的流量发送到金丝雀版本本，我们可以使用 &lt;code>kubectl&lt;/code> 来设置以下的路由规则：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: helloworld
spec:
hosts:
- helloworld
http:
- route:
- destination:
host: helloworld
subset: v1
weight: 90
- destination:
host: helloworld
subset: v2
weight: 10
---
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
name: helloworld
spec:
host: helloworld
subsets:
- name: v1
labels:
version: v1
- name: v2
labels:
version: v2
EOF
&lt;/code>&lt;/pre>
&lt;p>当规则设置生效后，Istio 将确保只有 10% 的请求发送到金丝雀版本，无论每个版本的运行副本数量是多少。&lt;/p>
&lt;h2 id="heading">部署中的自动缩放&lt;/h2>
&lt;p>由于我们不再需要保持副本比例，所以我们可以安全地设置 Kubernetes &lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA&lt;/a> 来管理两个版本 Deployment 的副本：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl autoscale deployment helloworld-v1 --cpu-percent=50 --min=1 --max=10
deployment &amp;#34;helloworld-v1&amp;#34; autoscaled
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl autoscale deployment helloworld-v2 --cpu-percent=50 --min=1 --max=10
deployment &amp;#34;helloworld-v2&amp;#34; autoscaled
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get hpa
NAME REFERENCE TARGET CURRENT MINPODS MAXPODS AGE
Helloworld-v1 Deployment/helloworld-v1 50% 47% 1 10 17s
Helloworld-v2 Deployment/helloworld-v2 50% 40% 1 10 15s
&lt;/code>&lt;/pre>
&lt;p>如果现在对 &lt;strong>helloworld&lt;/strong> 服务上产生一些负载，我们会注意到，当扩容开始时，&lt;strong>v1&lt;/strong> 扩容副本数目远高于 &lt;strong>v2&lt;/strong> ，因为 &lt;strong>v1&lt;/strong> pod 正在处理 90％ 的负载。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods | grep helloworld
helloworld-v1-3523621687-3q5wh 0/2 Pending 0 15m
helloworld-v1-3523621687-73642 2/2 Running 0 11m
helloworld-v1-3523621687-7hs31 2/2 Running 0 19m
helloworld-v1-3523621687-dt7n7 2/2 Running 0 50m
helloworld-v1-3523621687-gdhq9 2/2 Running 0 11m
helloworld-v1-3523621687-jxs4t 0/2 Pending 0 15m
helloworld-v1-3523621687-l8rjn 2/2 Running 0 19m
helloworld-v1-3523621687-wwddw 2/2 Running 0 15m
helloworld-v1-3523621687-xlt26 0/2 Pending 0 19m
helloworld-v2-4095161145-963wt 2/2 Running 0 50m
&lt;/code>&lt;/pre>
&lt;p>如果更改路由规则将 50％ 的流量发送到 &lt;strong>v2&lt;/strong>，我们则可以在短暂的延迟后注意到 &lt;strong>v1&lt;/strong> 副本数的减少，而 &lt;strong>v2&lt;/strong> 副本数相应地增加。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods | grep helloworld
helloworld-v1-3523621687-73642 2/2 Running 0 35m
helloworld-v1-3523621687-7hs31 2/2 Running 0 43m
helloworld-v1-3523621687-dt7n7 2/2 Running 0 1h
helloworld-v1-3523621687-gdhq9 2/2 Running 0 35m
helloworld-v1-3523621687-l8rjn 2/2 Running 0 43m
helloworld-v2-4095161145-57537 0/2 Pending 0 21m
helloworld-v2-4095161145-9322m 2/2 Running 0 21m
helloworld-v2-4095161145-963wt 2/2 Running 0 1h
helloworld-v2-4095161145-c3dpj 0/2 Pending 0 21m
helloworld-v2-4095161145-t2ccm 0/2 Pending 0 17m
helloworld-v2-4095161145-v3v9n 0/2 Pending 0 13m
&lt;/code>&lt;/pre>
&lt;p>最终结果与 Kubernetes Deployment 上线非常相似，只是整个流程并不是集中地进行编排和管理。相反，我们看到几个组件独立完成工作，虽然它们有因果关系。&lt;/p>
&lt;p>有一点不同的是，当我们停止负载时，无论设置路由规则如何，两个版本的副本数最终都会缩小到最小值（1）。&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl get pods | grep helloworld
helloworld-v1-3523621687-dt7n7 2/2 Running 0 1h
helloworld-v2-4095161145-963wt 2/2 Running 0 1h
&lt;/code>&lt;/pre>
&lt;h2 id="heading-1">聚焦金丝雀测试&lt;/h2>
&lt;p>如上所述，Istio 路由规则可用于根据特定规则准进行流量路由，从而能够提供更复杂的金丝雀部署方案。例如，与简单通过将金丝雀版本暴露给任意百分比的用户方式不同，我们希望在内部用户上尝试，甚至可能只是内部用户的一部分。&lt;/p>
&lt;p>以下命令可将特定网站上 50％ 的用户流量路由到金丝雀版本，而其他用户则不受影响：&lt;/p>
&lt;pre>&lt;code class='language-bash' data-expandlinks='true' >$ kubectl apply -f - &amp;lt;&amp;lt;EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
name: helloworld
spec:
hosts:
- helloworld
http:
- match:
- headers:
cookie:
regex: &amp;#34;^(.*?;)?(email=[^;]*@some-company-name.com)(;.*)?$&amp;#34;
route:
- destination:
host: helloworld
subset: v1
weight: 50
- destination:
host: helloworld
subset: v2
weight: 50
- route:
- destination:
host: helloworld
subset: v1
EOF
&lt;/code>&lt;/pre>
&lt;p>和以前一样，绑定到 2 个版本 Deployment 的自动缩放器会相应地自动管理副本，但这对流量分配没有影响。&lt;/p>
&lt;h2 id="heading-2">总结&lt;/h2>
&lt;p>本文中，我们看到了 Istio 如何支持通用可扩展的金丝雀部署，以及与 Kubernetes 部署的差异。Istio 服务网格提供了管理流量分配所需的基础控制，并完全独立于部署缩放。这允许简单而强大的方式来进行金丝雀测试和上线。&lt;/p>
&lt;p>支持金丝雀部署的智能路由只是 Istio 的众多功能之一，它将使基于大型微服务的应用程序的生产部署变得更加简单。查看 &lt;a href="/v1.1/zh/">istio.io&lt;/a> 了解更多信息。&lt;/p>
&lt;p>可在&lt;a href="https://github.com/istio/istio/tree/release-1.1/samples/helloworld">此处&lt;/a>查看示例代码。&lt;/p></description><pubDate>Wed, 14 Jun 2017 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2017/0.1-canary/</link><author>Frank Budinsky</author><guid isPermaLink="true">/v1.1/zh/blog/2017/0.1-canary/</guid><category>traffic-management</category><category>canary</category></item><item><title>使用 Istio 增强端到端安全</title><description>&lt;p>传统的网络安全方式无法解决部署在动态变化环境下分布式应用的安全威胁。这里，我们将描述 &lt;code>Istio Auth&lt;/code> 如何帮助企业将其安全从边界保护转变为内部所有服务间通信保护。 使用 &lt;code>Istio Auth&lt;/code> 开发人员和运维人员可以在不改动程序的情况下，对于敏感数据进行保护，防止未经授权的内部人员访问。&lt;/p>
&lt;p>&lt;code>Istio Auth&lt;/code> 是更广泛的 &lt;a href="/v1.1/zh">Istio 平台&lt;/a>的安全组件。它结合了 Google 生产环境中保护数百万微服务安全的经验。&lt;/p>
&lt;h2 id="heading">背景知识&lt;/h2>
&lt;p>现代应用程序体系结构越来越多地基于共享服务，共享服务部署在云平台上，可被方便地进行动态部署和扩容。 传统的网络边界安全性（例如防火墙）控制力度太粗，会导致部分非预期的客户端访问。使用盗取合法客户端的认证令牌进行重放攻击，就是一种常见的安全风险。对于持有敏感数据公司而言，内部威胁是一个需要关注的主要风险。其他网络安全方法（如 IP 白名单）通过静态方式定义，难以大规模管理，不适合动态变化的生产环境。&lt;/p>
&lt;p>因此，安全管理员需要一种工具，其可以能够默认开启并且始终保护生产环境中服务间的所有通信。&lt;/p>
&lt;h2 id="heading-1">解决方案：增强的服务身份和验证&lt;/h2>
&lt;p>多年来，Google 通过研发架构和技术，帮助其生产环境中数百万个微服务抵御了外部攻击和内部威胁。 关键安全原则包括信任端而不是网络，基于服务身份和级别授权的双向强身份验证。&lt;code>Istio Auth&lt;/code> 基于相同的原则。&lt;/p>
&lt;p>&lt;code>Istio Auth&lt;/code> 服务 0.1 版本在 Kubernetes 上运行，并提供以下功能：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>服务间强身份认证&lt;/p>
&lt;/li>
&lt;li>
&lt;p>访问控制以限制可以访问服务（及其数据）的身份&lt;/p>
&lt;/li>
&lt;li>
&lt;p>传输中的数据自动加密&lt;/p>
&lt;/li>
&lt;li>
&lt;p>密钥和证书的大规模管理&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>Istio Auth&lt;/code> 基于双向 TLS 和 X.509 等行业标准。 此外，Google 还积极参与一个开放的，社区驱动的 &lt;a href="https://spiffe.io/">SPIFFE&lt;/a> 服务安全框架。 随着 &lt;a href="https://spiffe.io/">SPIFFE&lt;/a> 规范的成熟，我们打算让 &lt;code>Istio Auth&lt;/code> 参考并实现。&lt;/p>
&lt;p>下图概述了 Kubernetes 上的 &lt;code>Istio Auth&lt;/code> 服务认证体系结构。&lt;/p>
&lt;figure style="width:100%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:56.25%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2017/0.1-auth/istio_auth_overview.svg" title="`Istio Auth` 概览">
&lt;img class="element-to-stretch" src="/v1.1/blog/2017/0.1-auth/istio_auth_overview.svg" alt="`Istio Auth` 概览" />
&lt;/a>
&lt;/div>
&lt;figcaption>`Istio Auth` 概览&lt;/figcaption>
&lt;/figure>
&lt;p>上图说明了三个关键的安全功能：&lt;/p>
&lt;h3 id="heading-2">强身份认证&lt;/h3>
&lt;p>&lt;code>Istio Auth&lt;/code> 使用 &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Kubernetes 服务帐户&lt;/a> 来识别服务运行的身份。 身份用于建立信任和定义服务级别访问策略。 身份在服务部署时分配，并在 X.509 证书的 SAN（主题备用名称）字段中进行编码。 使用服务帐户作为身份具有以下优点：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>管理员可以使用 Kubernetes 1.6 中引入的 &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC&lt;/a> 功能配置谁有权访问服务帐户&lt;/p>
&lt;/li>
&lt;li>
&lt;p>灵活地识别人类用户，服务或一组服务&lt;/p>
&lt;/li>
&lt;li>
&lt;p>稳定地支持服务身份的动态配置和工作负载自动扩展&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="heading-3">通信安全&lt;/h3>
&lt;p>服务间通信基于高性能客户端和服务器端 &lt;a href="https://envoyproxy.github.io/envoy/">Envoy&lt;/a> 代理的传输隧道。 代理之间的通信使用双向 TLS 来进行保护。 使用双向 TLS 的好处是服务身份不会被替换为从源窃取或重放攻击的令牌。 &lt;code>Istio Auth&lt;/code> 还引入了安全命名的概念，以防止服务器欺骗攻击 - 客户端代理验证允许验证特定服务的授权的服务帐户。&lt;/p>
&lt;h3 id="heading-4">密钥管理和分配&lt;/h3>
&lt;p>&lt;code>Istio Auth&lt;/code> 为每个集群提供 CA（证书颁发机构），并可对密钥和证书自动管理。 这种情况下，&lt;code>Istio Auth&lt;/code> 具备以下功能 ：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>为每个服务帐户生成密钥和证书对。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>使用 &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/">Kubernetes Secrets&lt;/a> 将密钥和证书分发到相应的 pod。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>定期轮换密钥和证书。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>必要时（未来）撤销特定密钥和证书对。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>下图说明了 Kubernetes 上的端到端 &lt;code>Istio Auth&lt;/code> 身份验证工作流程：&lt;/p>
&lt;figure style="width:100%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:56.25%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2017/0.1-auth/istio_auth_workflow.svg" title="`Istio Auth` 工作流程">
&lt;img class="element-to-stretch" src="/v1.1/blog/2017/0.1-auth/istio_auth_workflow.svg" alt="`Istio Auth` 工作流程" />
&lt;/a>
&lt;/div>
&lt;figcaption>`Istio Auth` 工作流程&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;code>Istio Auth&lt;/code> 是更广泛的容器安全中的一部分。 Red Hat 是 Kubernetes 开发的合作伙伴，定义了 &lt;a href="https://www.redhat.com/en/resources/container-security-openshift-cloud-devops-whitepaper">10 层&lt;/a> 容器安全。 Istio 和 &lt;code>Istio Auth&lt;/code> 解决了其中两个层：”网络隔离” 和 “API 和服务端点管理”。 随着集群联邦在 Kubernetes 和其他平台上的发展，我们的目的是让 Istio 对跨越多个联邦集群的服务间通信提供保护。&lt;/p>
&lt;h2 id="istio-auth-">&lt;code>Istio Auth&lt;/code> 优点&lt;/h2>
&lt;p>&lt;strong>深度防御&lt;/strong>：当与 Kubernetes（或基础架构）网络策略结合使用时，用户可以获得更多的安全信心，因为他们知道 Pod 或服务间的通信在网络层和应用层上都得到保护。&lt;/p>
&lt;p>&lt;strong>默认安全&lt;/strong>：当与 Istio 的代理和集中策略引擎一起使用时，可在极少或不更改应用的情况下部署并配置 &lt;code>Istio Auth&lt;/code>。 因此，管理员和操作员可以确保默认开启服务通信保护，并且可以跨协议和运行时一致地实施这些策略。&lt;/p>
&lt;p>&lt;strong>强大的服务认证&lt;/strong>：&lt;code>Istio Auth&lt;/code> 使用双向 TLS 保护服务通信，以确保服务身份不会是其他来源窃取或重放攻击的令牌。 这可确保只能从经过严格身份验证和授权的客户端才能够访问具有敏感数据的服务。&lt;/p>
&lt;h2 id="heading-5">加入我们&lt;/h2>
&lt;p>&lt;code>Istio Auth&lt;/code> 是提供完整安全功能的第一步，安全功能可以用于抵御外部攻击和内部威胁，保护服务的敏感数据。 虽然初始版本仅在 Kubernetes 上运行，但我们的目标是使其能够在不同的生产环境中保护服务通信。 我们鼓励更多的社区&lt;a href="https://github.com/istio/istio/tree/release-1.1/security">加入我们&lt;/a>，为不同的应用技术栈和运行平台上轻松地提供强大的服务安全保障。&lt;/p></description><pubDate>Thu, 25 May 2017 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2017/0.1-auth/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2017/0.1-auth/</guid></item><item><title>初次了解 Istio</title><description>&lt;p>Google、 IBM 和 Lyft 骄傲的宣布了 &lt;a href="/v1.1/zh">Istio&lt;/a> 的首个公开版本，Istio 是一个提供一致的方式用于连接、加固、管理和监控微服务的开源项目。当前版本专注于支持 &lt;a href="https://kubernetes.io/">Kubernetes&lt;/a> 环境，我们计划在接下来的几个月添加诸如虚拟机和 Cloud Foundry 等环境的支持。
Istio 为微服务添加了流量管理能力，同时为比如安全、监控、路由、连接管理和策略等附加能力打下了基础。此软件构建于来自 Lyft 的经过实战检验的 &lt;a href="https://envoyproxy.github.io/envoy/">Envoy&lt;/a> 代理之上，能在&lt;em>无需改动任何应用代码&lt;/em>的情况下赋予对应用流量的可见性和控制能力。Istio 为 CIO 们提供了一个在企业内加强安全、策略和规范的强有力的工具。&lt;/p>
&lt;h2 id="heading">背景&lt;/h2>
&lt;p>基于微服务模式编写可靠的、松耦合的、产品级的应用是有挑战的。随着巨型单体应用被分解为微服务，软件团队不得不面对将微服务集成进分布式系统的挑战：服务发现、负载均衡、故障容忍、端到端监测、动态路由，还有最重要的合规和安全。&lt;/p>
&lt;p>层出不穷的方案尝试解决这些挑战，不同编程语言的解决方案差异巨大、缺少共性，这常常会危及到安全。&lt;/p>
&lt;p>有一个解决方案是使用通用 RPC 库比如 &lt;a href="https://grpc.io">gRPC&lt;/a>，但是这在大规模适配时花销不菲，且可能在某些事实上无法更动的应用上留下棕色地带。运维人员需要一个灵活的工具来使他们的微服务变得安全、合规、可追踪和高可用，开发人员也需要这种能力来在产品环境实验不同的功能或者部署金丝雀版本而不影响系统的完整性。&lt;/p>
&lt;h2 id="heading-1">解决方案：服务网格&lt;/h2>
&lt;p>想象一下如果我们可以在服务和网络之间透明的注入一层基础设施来给予运维人员所需要的控制能力的同时又能让开发人员免除需要将解决分布式系统问题的代码糅合到业务代码的烦恼。这种一致的基础设施层与服务开发的搭配通常被称之为 &lt;strong>&lt;em>服务网格&lt;/em>&lt;/strong>。正如微服务帮助不同的功能团队之间互相解耦，服务网格可以帮助解除功能开发和发布流程之间的耦合。Istio 通过在不同的服务网络间注入代理来将不同的微服务集成进同一个服务网格。&lt;/p>
&lt;p>Google、IBM 和 Lyft 为了共同的愿景，基于为内部和企业客户构建和管理大规模微服务的经验合力创造了 Istio，以此来为微服务的开发和维护提供一个可靠的基础。Google 和 IBM 在他们自身的应用以及他们的企业客户的敏感的／受管制的环境中实施大规模微服务时积累了丰富的经验，同时 Lyft 开发了 Envoy 以解决他们内部面对的挑战。在成功的将其应用于生产环境，管理过能每秒处理两百万个请求的分布于上万个虚拟机 超过100个微服务一年后 &lt;a href="https://eng.lyft.com/announcing-envoy-c-l7-proxy-and-communication-bus-92520b6c8191">Lyft 开源 Envoy&lt;/a> 。&lt;/p>
&lt;h2 id="istio-">Istio 的好处&lt;/h2>
&lt;p>&lt;strong>集群范围的可见性&lt;/strong>：故障时有发生，运维人员需要工具来监控集群健康和微服务状态。Istio 生成有关应用和网络行为的详细监测数据，可使用 &lt;a href="https://prometheus.io/">Prometheus&lt;/a> 和 &lt;a href="https://github.com/grafana/grafana">Grafana&lt;/a> 渲染，也可以发送指标和日志到任何收集、聚合和查询的系统以轻松的扩展其功能。Istio 使用 &lt;a href="https://github.com/openzipkin/zipkin">Zipkin&lt;/a> 提供分析性能瓶颈和诊断分布式故障的功能。&lt;/p>
&lt;figure style="width:100%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:55.425531914893625%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2017/0.1-announcement/./istio_grafana_dashboard-new.png" title="Grafana Dashboard with Response Size">
&lt;img class="element-to-stretch" src="/v1.1/blog/2017/0.1-announcement/./istio_grafana_dashboard-new.png" alt="Grafana Dashboard with Response Size" />
&lt;/a>
&lt;/div>
&lt;figcaption>Grafana Dashboard with Response Size&lt;/figcaption>
&lt;/figure>
&lt;figure style="width:100%">
&lt;div class="wrapper-with-intrinsic-ratio" style="padding-bottom:29.912663755458514%">
&lt;a data-skipendnotes="true" href="/v1.1/blog/2017/0.1-announcement/./istio_zipkin_dashboard.png" title="Zipkin Dashboard">
&lt;img class="element-to-stretch" src="/v1.1/blog/2017/0.1-announcement/./istio_zipkin_dashboard.png" alt="Zipkin Dashboard" />
&lt;/a>
&lt;/div>
&lt;figcaption>Zipkin Dashboard&lt;/figcaption>
&lt;/figure>
&lt;p>&lt;strong>适应能力和效率&lt;/strong>：当开发微服务时，运维人员需要假设网络是不可靠的。运维人员可以使用重试、负载均衡、流程控制（HTTP/2）和熔断等措施来缓解不可靠网络中这些常见的故障。Istio 提供了一致的方式来配置这些功能，使其易于维护一个高适应性的服务网格。&lt;/p>
&lt;p>&lt;strong>开发者生产力&lt;/strong>：Istio 使开发者专注于使用他们喜欢的编程语言构建服务功能，这有效的提升了开发者的生产力，同时 Istio 使用统一的方式处理适应性和网络认证。开发者免于将解决分布式系统问题的代码糅合到业务代码。Istio 提供支持 A/B 测试、金丝雀部署和故障注入的通用功能进一步的提高了生产力。&lt;/p>
&lt;p>&lt;strong>策略驱动运维&lt;/strong>：Istio 赋予肩负不同职责的团队以独立操作的能力。它将集群管理员从应用部署环节中分离，这可以增强应用的安全、监测、伸缩和服务拓扑等能力而&lt;em>不需要&lt;/em>变更代码。运维人员可以精确的路由一部分生产流量用于检验一个新版本的服务。他们可以在流量中注入故障和延迟来测试服务网格的适应能力，同时可以设置请求限制来放置服务被过载。Istio 也可以被用于确保合规，在服务间定义 ACL 可以仅允许被授权的服务才能相互访问。&lt;/p>
&lt;p>&lt;strong>默认安全&lt;/strong>：一个常见的缪误是认为分布式计算的网络是安全的。Istio 使用双向 TLS 连接，使运维人员可以确保服务之间的通信是经过认证和安全的，而使开发者或运维人员无需负担繁重的认证管理任务。我们的安全框架符合 &lt;a href="https://spiffe.github.io/">SPIFFE&lt;/a> 规范，且基于在 Google 内部经过大范围测试的类似系统。&lt;/p>
&lt;p>&lt;strong>渐进式适配&lt;/strong>：我们有意使 Istio 对于运行于网格中的服务完全透明，这允许团队逐步适配 Istio 的功能。适配人员可以首先启用集群范围内的可见性，一旦他们适应了 Istio 的存在，他们可以按需开启其他功能。&lt;/p>
&lt;h2 id="heading-2">加入我们&lt;/h2>
&lt;p>Istio 是一个完全开放的开发项目。今天我们发布了能工作于 Kubernetes 集群的 0.1 版本，我们计划每三个月发布一个大版本，包括支持更多的环境。我们的目标是赋能开发者和运维人员，使他们在所有环境中都能敏捷的发布和维护微服务，拥有底层网络的完全的可见性，且获得一致的控制和安全能力。我们期待与 Istio 社区和我们的合作伙伴一起沿着 &lt;a href="/v1.1/zh/about/feature-stages/">路线图&lt;/a> 朝着这些目标前进。&lt;/p>
&lt;p>访问 &lt;a href="https://github.com/istio/istio/releases">此处&lt;/a> 获取最新发布的代码。&lt;/p>
&lt;p>查看在 GlueCon 2017 公布 Istio 时的 &lt;a href="/v1.1/talks/istio_talk_gluecon_2017.pdf">介绍&lt;/a>。&lt;/p>
&lt;h2 id="heading-3">社区&lt;/h2>
&lt;p>我们很兴奋的看到来自社区中很多公司的早期支持：
&lt;a href="https://blog.openshift.com/red-hat-istio-launch/">Red Hat&lt;/a> 的 Red Hat OpenShift 和 OpenShift Application Runtimes，
Pivotal 的 &lt;a href="https://content.pivotal.io/blog/pivotal-and-istio-advancing-the-ecosystem-for-microservices-in-the-enterprise">Pivotal Cloud Foundry&lt;/a>，
WeaveWorks 的 &lt;a href="https://www.weave.works/blog/istio-weave-cloud/">Weave Cloud&lt;/a> 和 Weave Net 2.0，
&lt;a href="https://www.projectcalico.org/welcoming-istio-to-the-kubernetes-networking-community">Tigera&lt;/a> 的 Calico Network Policy Engine 项目，还有 &lt;a href="https://www.datawire.io/istio-and-datawire-ecosystem/">Datawire&lt;/a> 的 Ambassador 项目。我们期待看到更多的公司加入我们。&lt;/p>
&lt;p>想要参与时可以通过以下任意渠道与我们联系：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="/v1.1/zh">istio.io&lt;/a> 提供文档和示例。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://discuss.istio.io">Istio discussion board&lt;/a> 综合交流区。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://stackoverflow.com/questions/tagged/istio">Stack Overflow&lt;/a> 用于问答&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/istio/istio/issues">GitHub&lt;/a> 用于提交 Issue&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Twitter &lt;a href="https://twitter.com/IstioMesh">@IstioMesh&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>欢迎登船！&lt;/p></description><pubDate>Wed, 24 May 2017 00:00:00 +0000</pubDate><link>/v1.1/zh/blog/2017/0.1-announcement/</link><author>The Istio Team</author><guid isPermaLink="true">/v1.1/zh/blog/2017/0.1-announcement/</guid></item></channel></rss>